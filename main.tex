%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.

%%%% For camera-ready, use this
\documentclass[sigconf]{aamas}

%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tabularx}
\usepackage{booktabs}


%%% visual coloring for comments with distinct colors per entity for adding notes, todos and reviews necessary to be visualized on pdf.ß
\newif\ifaddcomments
\addcommentstrue % Uncomment this line to remove the user comments


\newcommand{\todo}[1]{\ifaddcomments{\textcolor{red}{[TODO: #1]}}\fi}
\newcommand{\omer}[1]{\ifaddcomments{\textcolor{brown}{[Omer: #1]}}\fi}
\newcommand{\dvir}[1]{\ifaddcomments{\textcolor{blue}{[Dvir: #1]}}\fi}
\newcommand{\guy}[1]{\ifaddcomments{\textcolor{purple}{[Guy: #1]}}\fi}
\newcommand{\rotem}[1]{\ifaddcomments{\textcolor{green}{[Rotem: #1]}}\fi}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{A.~El~Fallah~Seghrouchni, Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Use this command to specify your submission number.

\acmSubmissionID{7}

%%% Use this command to specify the title of your paper.

\title[Focal Loss for NCF]{Addressing Class Imbalance in NCF with Focal Loss} 

%%% Provide names, affiliations, and email addresses for all authors.

\author{Rotem Even Zur, 208839183}
\affiliation{
  \institution{Ben Gurion University of the Negev}
  \city{Beer Sheva}
  \country{Israel}}
\email{evenzro@post.bgu.ac.il}

\author{Guy Kalati, 318366150}
\affiliation{
  \institution{Ben Gurion University of the Negev}
  \city{Beer Sheva}
  \country{Israel}}
\email{guykalat@post.bgu.ac.il}

\author{Dvir Chitrit, 206766818}
\affiliation{
  \institution{Ben Gurion University of the Negev}
  \city{Beer Sheva}
  \country{Israel}}
\email{dvirchi@post.bgu.ac.il}

\author{Omer Eliyahu, 206510828}
\affiliation{
  \institution{Ben Gurion University of the Negev}
  \city{Beer Sheva}
  \country{Israel}}
\email{omereliy@post.bgu.ac.il}

%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}
%\rotem{Need to change the title of this research}
Neural Collaborative Filtering (NCF) models trained on implicit feedback face a fundamental challenge: the extreme class imbalance between observed interactions (positives) and unobserved items (negatives), with ratios often exceeding 20:1. Standard approaches address this through heuristic negative sampling, which discards most data and treats all samples equally regardless of prediction confidence. This work investigates Focal Loss, a dynamically weighted loss function that automatically down-weights well-classified examples. Unlike prior applications of Focal Loss in specialized domains that confound the loss function with complex graph architectures and side information, we provide a controlled evaluation on standard recommendation benchmarks (MovieLens 100K and 1M) using vanilla Matrix Factorization and NeuMF architectures. We introduce a robustness study testing whether Focal Loss maintains performance across varying negative sampling ratios (1:4, 1:10, 1:50) where traditional BCE-trained models are expected to degrade. Our experimental design includes comprehensive ablation studies isolating the contributions of the focusing parameter $\gamma$ and class-balancing parameter $\alpha$, providing insights into loss function design for implicit feedback recommendation.
\end{abstract}

%%% Use this command to specify a few keywords describing your work.

\keywords{recommender systems, collaborative filtering, neural collaborative filtering, matrix factorization, focal loss}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.

\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \section{Introduction}
 \label{sec:introduction}

Recommender systems have become the cornerstone of modern digital platforms, guiding users through vast catalogs of products, content, and services. A user's failure to interact with an item does not necessarily indicate disinterest, it may simply reflect unawareness of the item's existence. This fundamental ambiguity, combined with the observation that users typically interact with only a small fraction of available items, creates a severe class imbalance problem: the ratio of unobserved to observed interactions routinely exceeds 20:1 and can surpass 100:1 in sparse datasets.

Standard training objectives for neural recommendation models handle this imbalance through heuristic negative sampling, randomly selecting a small subset of unobserved items to serve as negative examples. While computationally efficient, this approach is statistically flawed. Binary Cross-Entropy (BCE) and Bayesian Personalized Ranking (BPR) losses treat all samples equally regardless of prediction confidence, causing easy negatives; items clearly irrelevant to a user; to dominate gradient updates. The model expends learning capacity on trivially classified examples while receiving insufficient signal from the challenging boundary cases that truly matter for recommendation quality. Hard negative mining strategies address this partially but introduce significant computational overhead and require careful tuning.%\guy{Maybe need to add how they address this and why its not good}

This work proposes applying Focal Loss~\cite{lin2017focal}; a dynamically weighted loss function originally developed for dense object detection;to Neural Collaborative Filtering (NCF) as a principled alternative to heuristic sampling. Focal Loss introduces a modulating factor that automatically down-weights well-classified examples and focuses learning on hard cases where the model's predictions are uncertain. Critically, while Focal Loss has been applied to recommendation in specialized domains such as drug repositioning~\cite{meng2022weighted}, these applications confound the loss function with complex architectures including graph convolutions and bilinear aggregators, making it impossible to isolate the contribution of Focal Loss itself. Our work provides a controlled evaluation by applying Focal Loss to standard NCF architectures; Matrix Factorization (MF) and Neural Matrix Factorization (NeuMF); without additional architectural complexity.

This paper makes the following contributions:
\begin{itemize}
    \item We provide a systematic evaluation of Focal Loss for collaborative filtering on standard implicit feedback benchmarks, demonstrating its effectiveness outside specialized bioinformatics domains.
    \item We introduce a robustness study across negative sampling ratios (1:4, 1:10, 1:50), testing the hypothesis that Focal Loss can maintain performance when exposed to higher proportions of negative examples.
    \item Building on Rendle et al.'s~\cite{rendle2020neural} observation that loss function choice may matter more than architectural complexity, we investigate whether Focal Loss can improve both MF and NeuMF, potentially reducing the performance gap between them.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}
\label{sec:background}

Recommender systems are personalization technologies designed to help users navigate large information spaces by predicting which items are most relevant to them. Instead of showing identical content to every user, they tailor item rankings (movies, products, music, news, etc)~\cite{ricci2011handbook} based on behavioral patterns and preference data. This personalized filtering reduces information overload and improves user satisfaction by presenting items with the highest estimated utility.

Two core principles distinguish recommender systems from general information retrieval~\cite{ricci2011handbook}. First, a recommender system is personalized: its goal is to optimize the experience of one specific user rather than reflect a group consensus. Second, it operates over discrete, predefined options, meaning the system helps the user choose among known items rather than generate new content. These characteristics separate recommendation from traditional search engines, where results for a given query are typically the same regardless of who performs the search.

Within this framework, one of the main recommendation approaches is {Collaborative Filtering (CF)}~\cite{sarwar2001item}. CF predicts user preferences by analyzing patterns of interactions across many users, assuming that people with similar behaviors prefer similar items. It is typically implemented either through memory-based methods, which use similarity measures like cosine similarity or Pearson correlation to find similar users or items, or through model-based methods that learn latent factors using techniques such as matrix factorization or clustering.

\subsection{Traditional Methods: KNN and Matrix Factorization}
\label{sec:traditional-methods}

In practice, traditional recommender systems in industry often rely on neighborhood-based Collaborative Filtering methods such as k-nearest neighbors, as well as model-based approaches like matrix factorization using Singular Value Decomposition (SVD), which have proven effective and scalable for many real-world applications~\cite{koren2009matrix}.

\subsubsection{K-Nearest Neighbors}

K-Nearest Neighbors (KNN) is a simple and widely used algorithm that identifies the most similar items or users by comparing them through a similarity or distance measure, such as cosine similarity, Pearson correlation, or Euclidean distance~\cite{guo2003knn}. In recommender systems, KNN is typically applied in two ways: item-based KNN, where the system finds items similar to those a user already interacted with, and user-based KNN, which recommends items liked by users who have similar behavior patterns. By analyzing logs, clickstream data, or rating histories, KNN can classify items according to shared tastes and generate intuitive recommendations.

Despite its interpretability and ease of implementation, KNN has notable limitations. Its performance depends strongly on the chosen value of $K$, and selecting this value often requires repeated experimentation. KNN also becomes less efficient as dataset size grows, since it must compute similarities across many users or items. High-dimensional or sparse data further degrades its accuracy, making dimensionality reduction techniques such as PCA or LDA useful in practice. Because of these challenges, KNN is effective in smaller or denser datasets but less suitable for large-scale recommendation tasks without additional optimization~\cite{su2009survey}.

\subsubsection{Singular Value Decomposition}

Singular Value Decomposition (SVD) is a matrix factorization technique that decomposes a matrix into three smaller matrices capturing its underlying structure~\cite{rahman2023extended}. In recommender systems, the user--item interaction matrix (ratings, clicks, purchases) is typically sparse, and SVD is used to uncover lower-dimensional latent factors that describe hidden relationships between users and items.

By decomposing the interaction matrix into user factors, item factors, and singular values, SVD represents both users and items in a shared latent space where patterns such as preferences, item attributes, or genres naturally emerge. This allows the system to estimate missing ratings by reconstructing an approximated version of the matrix, even when data is incomplete. SVD is particularly useful in collaborative filtering because it reduces dimensionality, improves scalability, and can generate reasonable predictions for new or sparsely rated users and items by leveraging the learned latent factors~\cite{koren2009matrix}. These advantages make SVD an effective model-based approach in many real-world recommendation systems.

\subsection{Neural Collaborative Filtering (NCF)}
\label{sec:ncf}

Neural Collaborative Filtering (NCF) is a deep learning-based approach designed to enhance recommender systems by modeling the user--item interaction function with neural networks~\cite{he2017ncf}. Traditional collaborative filtering methods, such as matrix factorization, predict user preferences using the inner product between user and item latent embeddings, which constrains interactions to a linear form and limits the ability to capture complex, non-linear patterns observed in real data.

He et al.~\cite{he2017ncf} introduced the NCF framework to overcome this limitation by replacing the fixed inner product with a learnable neural network that can approximate more expressive user--item interactions. In this framework, each user and item is represented by an embedding vector, and these embeddings are fed into neural architectures that output a predicted preference score. The main architectures proposed are:
\begin{enumerate}
    \item \textbf{Generalized Matrix Factorization (GMF):} Retains a multiplicative interaction between embeddings through element-wise multiplication followed by a linear layer, capturing a generalized multiplicative interaction.
    \item \textbf{Multi-Layer Perceptron (MLP):} Applies multiple non-linear layers on concatenated embeddings to learn complex interaction patterns.
    \item \textbf{NeuMF:} A hybrid model that combines GMF and MLP by concatenating their outputs before a final prediction layer, thereby exploiting both multiplicative and compositional interactions.
\end{enumerate}

The NCF framework is typically trained on implicit feedback data using loss functions such as binary cross-entropy with negative sampling and is evaluated with ranking metrics like Hit Rate and Normalized Discounted Cumulative Gain (NDCG).

\subsubsection{Enhanced NCF with Convolutional Features}

More recent work extends NCF by enriching the way user--item interactions are represented and processed within the neural network. Several studies propose an enhanced NCF architecture that combines convolutional neural networks (CNNs) with a hybrid feature selection mechanism to better capture complex structure in the interaction space~\cite{drammeh2023enhancing}.


\subsection{Addressing Class Imbalance: The Role of Focal Loss}
\label{sec:focal-loss}

Class imbalance~\cite{hu2008implicit} is a fundamental challenge in recommendation systems. The number of unobserved user--item pairs (potential negatives) vastly outnumbers observed interactions (positives). This extreme imbalance causes two main problems: training becomes inefficient as most examples are easy negatives contributing little useful signal, and these easy negatives can overwhelm the loss function and dominate gradient updates, leading to suboptimal models that fail to learn meaningful patterns.

Lin et al.~\cite{lin2017focal} proposed Focal Loss to address class imbalance by dynamically reshaping the standard cross-entropy loss. Traditional $\alpha$-balanced cross-entropy introduces a weighting factor $\alpha$ for the positive class and $1-\alpha$ for the negative class, which balances the importance of positive and negative examples but fails to differentiate between easy and hard examples within each class.

The Focal Loss mechanism adds a modulating factor to down-weight well-classified examples, allowing the model to focus on hard, misclassified cases. The final formulation is:
\begin{equation}
FL(p_t) = -\alpha_t(1-p_t)^\gamma \log(p_t)
\end{equation}
where $p_t$ is the model's estimated probability for the ground-truth class, $\gamma \geq 0$ is the focusing parameter, and $\alpha_t$ is the class balancing weight.

The focusing parameter $\gamma$ controls the strength of down-weighting. When an example is well-classified (high $p_t$), the factor $(1-p_t)^\gamma$ approaches zero, drastically reducing its loss contribution. This effectively down-weights easy negatives, which dominate the dataset in implicit feedback scenarios, and prevents them from overwhelming the gradient during training. Consequently, the model focuses its learning on hard, misclassified examples. Experimental results on object detection tasks demonstrated that this re-weighting significantly improves accuracy by efficiently handling extreme class imbalance.

For recommendation systems with implicit feedback, where class imbalance mirrors the challenges in dense object detection, Focal Loss offers a principled approach to handle the vast number of unobserved items without resorting to aggressive negative sampling or complex mining strategies. The loss function's ability to automatically down-weight easy negatives while maintaining gradient flow from hard examples could address a key limitation of neural collaborative filtering methods when trained on highly imbalanced datasets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{sec:related-work}

Recent research has explored both architectural enhancements to collaborative filtering and advanced loss functions to handle data sparsity and imbalance. We survey relevant work in both directions. While traditional approaches often rely on heuristic negative sampling to mitigate the dominance of unobserved interactions, more recent methods aim to address this fundamental class imbalance directly through advanced architectures or loss functions.

\subsection{Improvements to Traditional Collaborative Filtering}

Rahman~\cite{rahman2023extended} addresses a central problem in recommendation systems: improving prediction accuracy in sparse and cold-start settings, where traditional collaborative filtering often fails to estimate missing ratings reliably. To tackle this, the study evaluates Adaptive KNN, which adjusts the number of neighbors based on local data density, and an SVD-based model that uses matrix factorization to better reconstruct incomplete user--item matrices. The results show that Adaptive KNN struggles with sparsity and imbalance, while SVD achieves noticeably lower MAE and RMSE values by capturing latent user--item structures more effectively. This connects directly to our research, as we also compare traditional CF models (KNN and SVD) with a more advanced approach. While the article improves performance by adding features and adaptive mechanisms, our work approaches the problem by modifying the training process itself through the application of focal loss within a Neural Collaborative Filtering model to better handle sparse and imbalanced data.%\guy{This paragraph maybe not relevant with the new direction, they talk about KNN and SVD improvements}

Airen and Agrawal~\cite{airen2022knn} systematically evaluate variations of the KNN algorithm, including KNN-Basic, KNN-WithMeans, KNN-WithZScore, and KNN-Baseline, combined with different similarity measures (cosine, MSD, Pearson, and Pearson-baseline) on the MovieLens 100K dataset. Their experiments show that normalization choices, bias adjustments, and similarity functions significantly impact error metrics (MAE, MSE, RMSE) and ranking-based measures, demonstrating how methodological variations within collaborative filtering models can meaningfully alter evaluation outcomes. This observation aligns with our research, where we examine how modifying the learning approach, specifically incorporating focal loss within NCF, affects performance compared to standard CF baselines.

\subsection{Neural Collaborative Filtering Architectures}

The seminal work of He et al.~\cite{he2017ncf} systematically formulates NCF as a modular neural framework for modeling user--item interactions. Their central contribution is to parameterize the interaction function $f(u, i)$ between user $u$ and item $i$ as a neural network operating on learned embeddings, rather than relying on a fixed inner product. The GMF model performs element-wise multiplication of user and item embeddings followed by a linear layer, capturing a generalized multiplicative interaction, while the MLP model concatenates the embeddings and passes them through multiple non-linear layers to learn complex interaction patterns. NeuMF combines these two branches by concatenating their outputs before a final prediction layer, thereby leveraging both linear-like and highly non-linear interactions. Experiments on large-scale implicit feedback datasets demonstrate that NeuMF consistently achieves superior top-$K$ recommendation performance compared to the individual GMF and MLP components, and that increasing the depth of the MLP improves model expressiveness and accuracy.

While our work focuses on loss function optimization rather than architectural complexity, it is important to contextualize improved NCF models. Drammeh et al.~\cite{drammeh2023enhancing} extend the NCF framework by integrating convolutional feature extraction and hybrid feature selection into the interaction modeling process. Their model first creates an interaction map via the outer product of user and item embeddings, capturing pairwise relationships across all embedding dimensions, and then applies convolutional layers to this map to learn local and high-order interaction patterns. To address noise and redundancy in these high-dimensional representations, they introduce a hybrid feature selection module that combines pointwise convolutions, pooling operations, and gating mechanisms to highlight informative features while suppressing less useful ones. The architecture also retains a GMF-like branch, which provides a stable, lower-complexity interaction pathway that regularizes training and complements the more expressive convolutional branch.

In their experiments on benchmark datasets such as MovieLens and Pinterest, Several studies report that their enhanced NCF model outperforms baseline NCF variants, including GMF, MLP, and NeuMF, in terms of ranking metrics like Hit Rate and NDCG as well as error-based metrics such as RMSE and MAE. However, they also note that this improved performance comes with increased computational cost due to the outer-product interaction maps and convolutional layers, underscoring a trade-off between accuracy and efficiency.%\guy{Need to check if it not too similar to the paragraphs on those papers in background - all section}

\enlargethispage{1\baselineskip}

\subsection{Revisiting NCF vs. Matrix Factorization}

Despite the popularity of NCF, recent literature has re-evaluated the necessity of replacing the inner product with an MLP. Rendle et al.~\cite{rendle2020neural} critically re-examined the purported dominance of NCF over MF, demonstrating that a simple dot product, when trained with appropriate hyperparameter tuning and regularization, matches or exceeds the performance of the MLP-based NCF. This finding challenges the assumption that non-linear neural architectures are inherently superior for capturing user--item interactions and suggests that earlier reported performance gaps were likely due to suboptimal benchmarking of MF baselines. Rendle et al.\ argue that the MLP in NCF is difficult to optimize and does not necessarily learn a better interaction function than the dot product. Their findings imply that model architecture (MLP vs.\ Dot Product) may not be the sole determinant of accuracy.

In the context of our research, Rendle's insight shifts the optimization focus from model architecture to the training objective itself. Since the choice of interaction function is not the sole determinant of accuracy, we investigate whether addressing the fundamental problem of class imbalance via Focal Loss provides a more significant performance gain. These findings suggest that the bottleneck in recommendation quality may not be architectural but rather lies in the training objective itself. Our work builds on this insight by applying Focal Loss to both MF and NCF, testing the hypothesis that a better-calibrated loss function can improve learning from hard negatives regardless of the underlying architectural complexity.

\subsection{Loss Function Innovations for Recommendation}

A parallel line of research focuses on improving the training objective rather than the model architecture, which is the primary focus of this work.

The foundational work on pairwise ranking for implicit feedback is Bayesian Personalized Ranking (BPR), proposed by Rendle et al.~\cite{rendle2009bpr}. BPR derives a maximum posterior estimator for personalized ranking and introduces a pairwise loss function that directly optimizes for the ranking of items. The key insight is that for implicit feedback data, the goal is not to predict absolute ratings but to correctly order items by user preference. BPR constructs training data as triples $(u, i, j)$, indicating that user $u$ prefers item $i$ over item $j$, and optimizes the model using stochastic gradient descent with bootstrap sampling. This pairwise approach became a standard baseline for recommendation systems with implicit feedback and established the importance of choosing loss functions tailored to the recommendation task rather than adapting general classification losses.

While Focal Loss was originally designed for computer vision tasks, the class imbalance problem it addresses is equally prevalent in recommendation systems. In collaborative filtering, the number of items a user has not interacted with (implicit negatives) far exceeds the number of items they have rated or consumed (positives), creating a similar imbalance challenge. Meng et al.~\cite{meng2022weighted} presented an application of Focal Loss to neural collaborative filtering in the context of drug-disease association prediction.

The authors proposed DRWBNCF (Drug Repositioning using Weighted Bilinear Neural Collaborative Filtering), which frames drug repositioning as a recommendation problem where drugs are recommended for diseases based on known associations. The method constructs three networks: a drug-disease association network, a drug-drug similarity network, and a disease-disease similarity network, using only $k$-nearest neighbors rather than all similar neighbors to filter noisy information. The architecture consists of two main components: an integration component that uses a novel weighted bilinear graph convolution operation to aggregate pairwise interactions between neighbors, and a prediction component that employs a multi-layer perceptron optimized with $\alpha$-balanced Focal Loss and graph regularization.

The key innovation relevant to collaborative filtering is the application of Focal Loss to handle implicit feedback data. In their formulation, known drug-disease associations serve as positive samples while unobserved pairs are treated as negative samples. The Focal Loss function differentiates between easy negative samples and hard positive samples, addressing the false negative problem inherent in implicit feedback where an unobserved interaction does not necessarily indicate negative preference. The authors used $\gamma=2$ and $\alpha=0.5$, with results showing the hyperparameters maintained effectiveness across different dataset configurations. DRWBNCF achieved the best performance on three drug-disease association datasets in 10-fold cross-validation, with an average AUPR of 0.4688, representing a 5.7\% improvement over the next best method.%\guy{Also in here myabe need to rephrase it to be without the actual numbers}

However, the DRWBNCF approach presents several limitations that prevent direct conclusions about the efficacy of focal loss. First, the architecture confounds multiple innovations; the weighted bilinear graph convolution, the $k$-nearest neighbor filtering, the multi-network integration, and Focal Loss;making it impossible to attribute performance gains to any single component. The authors do not provide an ablation study that compares DRWBNCF with and without Focal Loss. Second, the domain differs substantially from general consumer recommendation: the Fdataset contains only 593 drugs and 313 diseases, orders of magnitude smaller than standard recommendation systems benchmarks with a lot more users and items. Third, the reliance on rich side information (chemical structures, disease semantics) limits applicability to settings where only interaction data is available. Our work addresses these gaps by applying Focal Loss to standard NCF architectures on standard benchmarks, providing a controlled evaluation that isolates the loss function's contribution.

Wu et al.~\cite{wu2023effectiveness} investigated an alternative approach called Sampled Softmax (SSM) loss. Sampled Softmax loss reformulates the recommendation problem as a multi-class classification where the model predicts which item a user will interact with from the entire item catalog. Rather than computing softmax over all items (computationally prohibitive for large catalogs), SSM approximates the full softmax by sampling a subset of negative items. The loss inherently handles class imbalance through its sampling mechanism and temperature scaling, which naturally down-weights easy examples similar to Focal Loss's modulating factor.

The authors identified three model-independent advantages of SSM loss for recommendation. First, SSM mitigates popularity bias by adjusting item representations based on their frequency in the training data, following an Inverse Propensity Weighting principle. Second, SSM functions as a hard negative mining strategy because the sampling process tends to select more challenging negatives as training progresses. Third, SSM directly optimizes ranking metrics by maximizing the gap between positive and negative items.

Experiments on four benchmark datasets showed SSM loss achieved notable improvements in NDCG@20 over Binary Cross-Entropy (BCE) and Bayesian Personalized Ranking (BPR) losses. Critically, SSM significantly improved long-tail item coverage. On the Yelp dataset, the least popular item group (containing 35.7\% of items) contributed less than 3\% of total Recall when using BCE, but SSM increased exposure of these long-tail items.

\subsection{Summary and Research Gap}

While Meng et al.\ \cite{meng2022weighted} validated Focal Loss in a specialized bioinformatics domain and Wu et al.\ proposed sampling-based alternatives such as SSM, there remains a need for a direct, controlled evaluation of Focal Loss in general-purpose recommendation. Existing applications confound Focal Loss with complex architectural innovations, side information, or domain-specific adaptations. This research bridges that gap by providing an ablation-style study: applying Focal Loss to standard NCF architectures (MF and NeuMF) on standard implicit feedback benchmarks, without additional architectural complexity. This design isolates the loss function's contribution, enabling conclusions about whether Focal Loss alone is sufficient to address the class imbalance problem in recommender systems, and whether its benefits are architecture-agnostic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodology}
\label{sec:methodology}

This section presents our proposed approach for applying Focal Loss to Neural Collaborative Filtering. We formalize the recommendation problem, describe how Focal Loss addresses class imbalance in implicit feedback settings, and detail the model architectures under investigation.

\subsection{Problem Formulation}
\label{sec:problem-formulation}

Consider a recommendation scenario with $M$ users and $N$ items. The user-item interaction data is represented as a binary matrix $\mathbf{Y} \in \{0,1\}^{M \times N}$, where each entry is defined as:
\begin{equation}
y_{ui} =
\begin{cases}
1, & \text{if interaction between user } u \text{ and item } i \text{ is observed} \\
0, & \text{otherwise}
\end{cases}
\end{equation}

Let $\mathcal{O} = \{(u,i) : y_{ui} = 1\}$ denote the set of observed interactions. In implicit feedback settings, a value of 1 indicates an observed interaction (e.g., click, purchase, view), while 0 indicates either no preference or an unobserved interaction. This formulation inherently creates a severe class imbalance problem, as the number of unobserved pairs vastly exceeds observed interactions. The class imbalance ratio $\rho$ is defined as:
\begin{equation}
\rho = \frac{|\{(u,i) : y_{ui} = 0\}|}{|\{(u,i) : y_{ui} = 1\}|} = \frac{M \times N - |\mathcal{O}|}{|\mathcal{O}|}
\label{eq:imbalance-ratio}
\end{equation}

For typical recommendation datasets, $\rho$ ranges from 20:1 to over 100:1, meaning negative samples dominate the training data by orders of magnitude. The recommendation task is to learn a scoring function $\hat{y}_{ui} = f(u, i; \Theta)$ that predicts the likelihood of user $u$ interacting with item $i$, where $\Theta$ represents model parameters. The objective is to rank items such that those with higher predicted scores are more likely to be relevant to the user.

\subsection{Focal Loss for Collaborative Filtering}
\label{sec:fl-cf}

Standard approaches to training recommendation models on implicit feedback use Binary Cross-Entropy (BCE) loss with negative sampling:
\begin{equation}
\mathcal{L}_{BCE} = -\frac{1}{|\mathcal{D}|} \sum_{(u,i,y) \in \mathcal{D}} \left[ y \log(\hat{y}_{ui}) + (1-y) \log(1 - \hat{y}_{ui}) \right]
\label{eq:bce}
\end{equation}
where $\mathcal{D}$ contains positive samples from $\mathcal{O}$ paired with randomly sampled negatives. An alternative is Bayesian Personalized Ranking (BPR) loss~\cite{rendle2009bpr}, which optimizes pairwise rankings:
\begin{equation}
\mathcal{L}_{BPR} = -\frac{1}{|\mathcal{D}^+|} \sum_{(u,i,j) \in \mathcal{D}^+} \log \sigma(\hat{y}_{ui} - \hat{y}_{uj})
\label{eq:bpr}
\end{equation}
where $(u,i,j)$ denotes a triple indicating user $u$ prefers item $i$ over item $j$, and $\sigma(\cdot)$ is the sigmoid function.

Both BCE and BPR treat all samples equally regardless of prediction confidence. To understand why this is problematic, consider the gradient dynamics during training. For a correctly classified negative example where the model predicts $\hat{y} = 0.05$ (low relevance), the BCE loss contribution is $-\log(1 - 0.05) = -\log(0.95) \approx 0.05$. While individually small, when aggregated across millions of such easy negatives; which constitute the vast majority of training data; the cumulative loss and gradient signal from trivial examples overwhelms the contribution from the rare but informative hard positives and challenging negatives. The model effectively spends most of its learning capacity maintaining low scores for items the user obviously has no interest in, rather than refining the decision boundary for ambiguous cases.

Focal Loss~\cite{lin2017focal} addresses this fundamental limitation by introducing a modulating factor that dynamically down-weights well-classified examples:
\begin{equation}
\mathcal{L}_{FL} = -\frac{1}{|\mathcal{D}|} \sum_{(u,i,y) \in \mathcal{D}} \alpha_t (1 - p_t)^\gamma \log(p_t)
\label{eq:focal-loss}
\end{equation}
where $p_t$ represents the model's estimated probability for the ground-truth class:
\begin{equation}
p_t =
\begin{cases}
\hat{y}_{ui}, & \text{if } y = 1 \text{ (positive sample)} \\
1 - \hat{y}_{ui}, & \text{if } y = 0 \text{ (negative sample)}
\end{cases}
\label{eq:pt}
\end{equation}
and $\alpha_t$ is the class-dependent balancing weight:
\begin{equation}
\alpha_t =
\begin{cases}
\alpha, & \text{if } y = 1 \\
1 - \alpha, & \text{if } y = 0
\end{cases}
\end{equation}

The focusing parameter $\gamma \geq 0$ controls the rate of down-weighting. When $\gamma = 0$, Focal Loss reduces to standard $\alpha$-balanced cross-entropy. As $\gamma$ increases, the modulating factor $(1-p_t)^\gamma$ more aggressively reduces the contribution of well-classified examples. The behavior of this factor is:
\begin{equation}
(1 - p_t)^\gamma \rightarrow
\begin{cases}
\approx 0, & \text{if } p_t \rightarrow 1 \text{ (well-classified)} \\
\approx 1, & \text{if } p_t \rightarrow 0 \text{ (misclassified)}
\end{cases}
\end{equation}

This mechanism automatically shifts the training focus toward hard examples that the model struggles to classify correctly, which is particularly valuable in recommendation settings where the vast majority of negative samples are trivially easy to identify.

\subsection{Model Architectures}
\label{sec:model-architectures}

\subsubsection{Matrix Factorization (MF)}

Matrix Factorization represents users and items as dense embedding vectors in a shared latent space. For user $u$ with embedding $\mathbf{p}_u \in \mathbb{R}^d$ and item $i$ with embedding $\mathbf{q}_i \in \mathbb{R}^d$, the predicted interaction score is:
\begin{equation}
\hat{y}_{ui}^{MF} = \sigma(\mathbf{p}_u^\top \mathbf{q}_i + b_u + b_i + b)
\label{eq:mf}
\end{equation}
where $b_u$, $b_i$, and $b$ are user, item, and global bias terms respectively, and $\sigma(\cdot)$ is the sigmoid function.

\subsubsection{Neural Matrix Factorization (NeuMF)}

NeuMF~\cite{he2017ncf} combines two complementary interaction modeling approaches. The Generalized Matrix Factorization (GMF) component captures multiplicative interactions:
\begin{equation}
\mathbf{h}^{GMF} = \mathbf{p}_u^G \odot \mathbf{q}_i^G
\label{eq:gmf}
\end{equation}
where $\odot$ denotes element-wise multiplication and $\mathbf{p}_u^G$, $\mathbf{q}_i^G$ are GMF-specific embeddings.

The Multi-Layer Perceptron (MLP) component learns non-linear interactions through concatenation and deep layers:
\begin{equation}
\mathbf{z}_0 = [\mathbf{p}_u^M ; \mathbf{q}_i^M], \quad \mathbf{z}_l = \text{ReLU}(\mathbf{W}_l \mathbf{z}_{l-1} + \mathbf{b}_l), \quad l = 1, \ldots, L
\label{eq:mlp}
\end{equation}
where $[\cdot ; \cdot]$ denotes concatenation, $\mathbf{p}_u^M$, $\mathbf{q}_i^M$ are MLP-specific embeddings, and $L$ is the number of hidden layers.

The final NeuMF prediction combines both branches:
\begin{equation}
\hat{y}_{ui}^{NeuMF} = \sigma\left(\mathbf{w}^\top [\mathbf{h}^{GMF} ; \mathbf{z}_L] + b\right)
\label{eq:neumf}
\end{equation}
where $\mathbf{w}$ is a learnable weight vector for the final prediction layer.

\subsection{Illustrative Example: Effect of Focal Loss}
\label{sec:toy-example}

To illustrate how Focal Loss rebalances learning toward informative examples, consider a toy recommendation scenario with 5 users and 10 items, shown in Table~\ref{tab:toy-matrix}.

\begin{table}[h]
\centering
\caption{Toy user-item interaction matrix (1 = observed interaction)}
\label{tab:toy-matrix}
\begin{tabular}{l|cccccccccc}
\hline
& $i_1$ & $i_2$ & $i_3$ & $i_4$ & $i_5$ & $i_6$ & $i_7$ & $i_8$ & $i_9$ & $i_{10}$ \\
\hline
$u_1$ & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
$u_2$ & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
$u_3$ & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
$u_4$ & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
$u_5$ & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
\hline
\end{tabular}
\end{table}

This matrix contains 10 positive interactions and 40 negative entries, yielding an imbalance ratio $\rho = 4:1$. Consider two training examples after several epochs of training:

\textbf{Easy negative sample} $(u_1, i_{10})$: The model correctly predicts low relevance with $\hat{y} = 0.05$, giving $p_t = 1 - 0.05 = 0.95$. The loss contributions are:
\begin{align}
\mathcal{L}_{BCE} &= -\log(0.95) = 0.051 \\
\mathcal{L}_{FL} &= -(1-\alpha)(1-0.95)^2 \log(0.95) = 0.75 \times 0.0025 \times 0.051 \approx 0.0001
\end{align}
using $\gamma=2$ and $\alpha=0.25$. Focal Loss reduces this contribution by approximately $500\times$.

\textbf{Hard positive sample} $(u_1, i_1)$: The model struggles to identify this positive with $\hat{y} = 0.3$, giving $p_t = 0.3$. The loss contributions are:
\begin{align}
\mathcal{L}_{BCE} &= -\log(0.3) = 1.204 \\
\mathcal{L}_{FL} &= -\alpha(1-0.3)^2 \log(0.3) = 0.25 \times 0.49 \times 1.204 \approx 0.147
\end{align}

Focal Loss reduces the hard positive's contribution by only $\sim$8$\times$, compared to $\sim$500$\times$ for the easy negative. This differential down-weighting shifts gradient mass toward hard examples, enabling the model to learn more effectively from the limited positive signals and challenging edge cases rather than wasting capacity on trivially classified negatives.

\subsection{Novel Contributions}
\label{sec:contributions}

This work makes the following contributions:
\begin{enumerate}
    \item \textbf{Systematic Evaluation of Focal Loss for General-Purpose Recommendation:} Unlike prior work applying Focal Loss to specialized domains~\cite{meng2022weighted}, we evaluate its effectiveness on standard recommendation benchmarks with implicit feedback.
    \item \textbf{Reconciliation with Prior Findings:} Building on Rendle et al.'s~\cite{rendle2020neural} observation that loss function choice may matter more than architecture, we investigate whether Focal Loss can improve both MF and NCF, potentially reducing the performance gap between them.
    \item \textbf{Comprehensive Comparison with Traditional Methods:} We benchmark Focal Loss-enhanced models against traditional collaborative filtering baselines (KNN, SVD) to contextualize improvements within the broader recommendation landscape.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Design of Experiments}
\label{sec:experiments}

This section describes the experimental setup designed to evaluate the effectiveness of Focal Loss for collaborative filtering. We present falsifiable hypotheses, dataset characteristics, implementation details, evaluation protocols, and ablation study designs.

\subsection{Research Hypotheses}
\label{sec:hypotheses}

We formulate three falsifiable hypotheses to evaluate Focal Loss for collaborative filtering:

\begin{description}
    \item[H1 (Efficacy):] NeuMF trained with Focal Loss achieves statistically significantly higher Hit Rate@10 and NDCG@10 compared to NeuMF trained with Binary Cross-Entropy loss under high class imbalance (1:50 negative sampling).

    \item[H2 (Robustness):] The performance advantage of NeuMF-FL over NeuMF-BCE is maintained across varying negative sampling ratios (1:4, 1:10, 1:50), with the improvement magnitude correlating positively with imbalance severity.

    \item[H3 (Mechanism):] NeuMF with $\gamma > 0$ (full Focal Loss) outperforms NeuMF with $\gamma = 0$ ($\alpha$-balanced BCE) when $\alpha$ is held constant, demonstrating that the focusing mechanism provides benefit beyond class reweighting alone.
\end{description}

These hypotheses enable independent evaluation of whether Focal Loss improves recommendation quality (H1), whether such improvements are robust to experimental conditions (H2), and whether the focusing mechanism---rather than mere class balancing---drives observed improvements (H3).

\subsection{Datasets}
\label{sec:datasets}

We conduct experiments on two widely-used benchmark datasets from the MovieLens collection~\cite{harper2015movielens}, summarized in Table~\ref{tab:datasets}.

\begin{table}[h]
\centering
\caption{Dataset statistics after preprocessing}
\label{tab:datasets}
\small
\begin{tabularx}{\columnwidth}{@{}l*{5}{>{\raggedleft\arraybackslash}X}@{}}
\hline
\textbf{Dataset} & \textbf{Users} & \textbf{Items} & \textbf{Interactions} & \textbf{Density} & \textbf{Imbalance} \\
\hline
ML-100K & 943 & 1,682 & 100,000 & 6.30\% & 14.9:1 \\
ML-1M & 6,040 & 3,706 & 1,000,209 & 4.47\% & 21.4:1 \\
\hline
\end{tabularx}
\end{table}

\textbf{Data Preprocessing:} Following standard practice for implicit feedback evaluation~\cite{he2017ncf}, we apply the following preprocessing steps:
\begin{enumerate}
    \item \textbf{Binarization:} Convert explicit ratings to implicit feedback by treating ratings $\geq 4$ as positive interactions ($y=1$) and all others as negative ($y=0$). We adopt this stricter threshold (compared to treating any rating as positive) to focus on items with strong positive signals, reducing label noise from lukewarm interactions that may not reflect genuine preference.
    \item \textbf{Filtering:} Apply 5-core filtering, retaining only users with at least 20 interactions to ensure sufficient training signal per user.
    \item \textbf{Train/Validation/Test Split:} Use leave-one-out evaluation: for each user, the most recent interaction forms the test set, the second-most recent forms the validation set, and all remaining interactions form the training set.
    \item \textbf{Negative Sampling:} We adopt 1:50 negative sampling as the primary experimental condition, sampling 50 negative items per positive interaction during training. This ratio better approximates the severe class imbalance present in production recommendation systems, where users interact with a minuscule fraction of available items. During evaluation, we rank each test item against 99 randomly sampled negative items following standard protocol.

    \textbf{Robustness Study:} To assess sensitivity to sampling ratio, we additionally evaluate at 1:4 (standard in prior NCF literature) and 1:10 (moderate imbalance) ratios.
\end{enumerate}

\subsection{Implementation Details}
\label{sec:implementation}

\textbf{Framework:} All experiments are implemented using RecBole~\cite{zhao2021recbole}, a unified PyTorch-based recommendation framework that provides standardized implementations of baseline models and evaluation protocols. We extend RecBole with a custom Focal Loss module.

\textbf{Computational Resources:} Experiments are conducted on Kaggle's GPU environment (NVIDIA Tesla T4/P100, 16GB VRAM).

\textbf{Reproducibility:} Random seeds are fixed across all experiments. Code will be made available upon publication.

Algorithm~\ref{alg:focal-ncf} presents the training procedure for NCF with Focal Loss. First, the user and item embeddings and all network parameters are initialized. The model is then trained iteratively over multiple epochs, where in each epoch the training data is processed in mini-batches of user–item pairs with their binary interaction labels. For each batch, the model performs a forward pass through the NeuMF architecture to produce a predicted interaction probability. Based on this prediction and the true label, the algorithm computes the true-class probability and assigns a class-specific weighting factor. These values are used to calculate the Focal Loss, which down-weights well-classified examples and emphasizes harder, misclassified samples. The loss is averaged over the batch, and model parameters are updated using backpropagation. This process repeats until either the maximum number of epochs is reached or early stopping is triggered when the validation performance stops improving, after which the final trained model parameters are returned.

\begin{algorithm}
\caption{NCF Training with Focal Loss}
\label{alg:focal-ncf}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE Training data $\mathcal{D}$, focusing parameter $\gamma$, balancing parameter $\alpha$
\ENSURE Trained model parameters $\Theta$
\STATE Initialize embeddings and network weights $\Theta$
\FOR{epoch $= 1$ to max\_epochs}
    \FOR{each batch $(u, i, y) \in \mathcal{D}$}
        \STATE $\hat{y} \gets \sigma(\text{NeuMF}(u, i; \Theta))$ \COMMENT{Forward pass}
        \STATE $p_t \gets y \cdot \hat{y} + (1-y) \cdot (1-\hat{y})$ \COMMENT{True class probability}
        \STATE $\alpha_t \gets y \cdot \alpha + (1-y) \cdot (1-\alpha)$ \COMMENT{Class weight}
        \STATE $\mathcal{L} \gets -\alpha_t (1-p_t)^\gamma \log(p_t)$ \COMMENT{Focal Loss}
        \STATE Update $\Theta$ via backpropagation on $\text{mean}(\mathcal{L})$
    \ENDFOR
    \IF{validation metric does not improve for 10 epochs}
        \STATE \textbf{break} \COMMENT{Early stopping}
    \ENDIF
\ENDFOR
\RETURN $\Theta$
\end{algorithmic}
\end{algorithm}

\subsection{Baseline Models}
\label{sec:baselines}
We compare the following models, summarized in Table~\ref{tab:models}. PopRank serves as a non-personalized baseline that ranks items by global popularity (total interaction count), providing a lower bound that any personalized recommendation method should exceed.

\begin{table}[h]
\centering
\caption{Models evaluated in experiments}
\label{tab:models}
\small
\begin{tabularx}{\columnwidth}{@{}lllX@{}}
\hline
\textbf{Model} & \textbf{Type} & \textbf{Architecture} & \textbf{Loss} \\
\hline
PopRank & Non-personalized & Popularity ranking & N/A \\
KNN-Item & Traditional & Item-based neighborhood & Cosine \\
KNN-User & Traditional & User-based neighborhood & Cosine \\
SVD & Traditional & Matrix factorization & MSE \\
MF & Neural & Dot-product embeddings & BCE, BPR, FL \\
NeuMF & Neural & GMF + MLP hybrid & BCE, BPR, FL \\
NeuMF-$\alpha$BCE & Neural & GMF + MLP hybrid & $\alpha$-BCE \\
\hline
\end{tabularx}
\end{table}


\subsection{Hyperparameter Configuration}
\label{sec:hyperparams}
%\omer{consult efrat on the number of models evaluated in our experiments and if the comparison table should be added as a hypothesis.}
Table~\ref{tab:hyperparams-traditional} and Table~\ref{tab:hyperparams-neural} present the hyperparameter search spaces.

\begin{table}[h]
\centering
\caption{Hyperparameter search space for traditional methods}
\label{tab:hyperparams-traditional}
\small
\begin{tabularx}{\columnwidth}{@{}llXl@{}}
\hline
\textbf{Parameter} & \textbf{Model} & \textbf{Values} & \textbf{Rationale} \\
\hline
$K$ (neighbors) & KNN & \{10, 20, 40, 80\} & Neighborhood size \\
Similarity & KNN & \{cosine, pearson\} & Distance metric \\
Latent factors & SVD & \{32, 64, 128\} & Embedding dim. \\
Regularization & SVD & \{0.001, 0.01, 0.1\} & Overfitting control \\
\hline
\end{tabularx}
\end{table}

\begin{table}[h]
\centering
\caption{Hyperparameter search space for neural methods}
\label{tab:hyperparams-neural}
\small
\begin{tabularx}{\columnwidth}{@{}lXl@{}}
\hline
\textbf{Parameter} & \textbf{Values} & \textbf{Rationale} \\
\hline
Embedding size & \{32, 64, 128\} & Model capacity \\
MLP layers & \{[64,32], [128,64,32]\} & Network depth \\
Learning rate & \{0.0001, 0.001, 0.01\} & Optimization speed \\
Batch size & \{256, 512, 1024\} & GPU utilization \\
$\gamma$ (focusing) & \{0, 0.5, 1.0, 2.0, 3.0\} & $\gamma{=}0$: $\alpha$-BCE \\
$\alpha$ (balancing) & \{0.25, 0.5, 0.75\} & See Sec.~\ref{sec:alpha-sampling} \\
\hline
\end{tabularx}
\end{table}

Note that $\gamma = 0$ reduces Focal Loss to $\alpha$-balanced BCE, enabling direct comparison for H3 testing. Based on the alpha-sampling interaction analysis, we anticipate optimal $\alpha$ values will exceed 0.25 for high sampling ratios.

Hyperparameters are tuned on the validation set using grid search. The best configuration is selected based on NDCG@10, and final results are reported on the held-out test set.

\subsubsection{Alpha-Sampling Interaction}
\label{sec:alpha-sampling}

The $\alpha$ parameter and negative sampling ratio interact non-trivially. We define the \emph{effective class weight ratio} as the ratio of total weight assigned to negative versus positive samples in a training batch:
\begin{equation}
R_{\text{eff}} = \frac{(1-\alpha) \times N_{\text{neg}}}{\alpha \times N_{\text{pos}}} = \frac{(1-\alpha) \times r}{\alpha}
\label{eq:effective-ratio}
\end{equation}
where $r$ is the negative sampling ratio.

Table~\ref{tab:alpha-interaction} illustrates this interaction. With $\alpha = 0.25$ and 1:50 sampling, negatives receive 150 times the total weight of positives, potentially overwhelming the positive signal entirely. To achieve balanced effective weighting ($R_{\text{eff}} = 1$), one requires $\alpha = r/(r+1)$; for 1:50 sampling, this corresponds to $\alpha \approx 0.98$.

\begin{table}[h]
\centering
\caption{Effective class weight ratio for various $\alpha$ and sampling configurations}
\label{tab:alpha-interaction}
\begin{tabular}{ccc}
\toprule
\textbf{Sampling} & $\boldsymbol{\alpha}$ & $\boldsymbol{R_{\text{eff}}}$ \\
\midrule
1:4  & 0.25 & 12:1 \\
1:4  & 0.50 & 4:1 \\
1:4  & 0.75 & 1.3:1 \\
\midrule
1:10 & 0.25 & 30:1 \\
1:10 & 0.50 & 10:1 \\
1:10 & 0.75 & 3.3:1 \\
\midrule
1:50 & 0.25 & 150:1 \\
1:50 & 0.50 & 50:1 \\
1:50 & 0.75 & 16.7:1 \\
\bottomrule
\end{tabular}
\end{table}

This analysis motivates our grid search over $\alpha$ values and explains why the default $\alpha = 0.25$ from computer vision applications may require adjustment for recommendation tasks with high negative sampling ratios.

\subsection{Evaluation Protocol}
\label{sec:evaluation}

\textbf{Metrics:} We evaluate models using standard ranking metrics for implicit feedback:
\begin{itemize}
    \item \textbf{Hit Rate@$K$ (HR@$K$):} Measures whether the ground-truth item appears in the top-$K$ recommendations:
    \begin{equation}
    \text{HR@}K = \frac{1}{|\mathcal{U}|} \sum_{u \in \mathcal{U}} \mathbf{1}[\text{rank}(i_u^+) \leq K]
    \end{equation}
    where $i_u^+$ is the held-out positive item for user $u$.

    \item \textbf{Normalized Discounted Cumulative Gain@$K$ (NDCG@$K$):} Accounts for the position of the hit with logarithmic discount:
    \begin{equation}
    \text{NDCG@}K = \frac{1}{|\mathcal{U}|} \sum_{u \in \mathcal{U}} \frac{\mathbf{1}[\text{rank}(i_u^+) \leq K]}{\log_2(\text{rank}(i_u^+) + 1)}
    \end{equation}
\end{itemize}

We report results for $K \in \{5, 10, 20\}$, with HR@10 and NDCG@10 as the primary metrics.

\textbf{Evaluation Procedure:} Following the leave-one-out protocol, for each user, we rank the held-out positive item against 99 randomly sampled negatives (100 items total) and compute metrics based on the positive item's rank.

\subsection{Statistical Testing}
\label{sec:statistical-testing}

To ensure reliable conclusions, we employ rigorous statistical testing following recommendations for machine learning experiments~\cite{demvsar2006statistical}:

\begin{itemize}
    \item \textbf{Multiple Seeds:} Each configuration is evaluated with 5 different random seeds to account for initialization variance.

    \item \textbf{Statistical Test:} We employ the Wilcoxon signed-rank test for paired comparisons (same seed, different methods), as it does not assume normality and is robust to outliers.

    \item \textbf{Multiple Comparison Correction:} Bonferroni correction is applied, yielding a corrected significance threshold of $p < 0.0125$ for four primary comparisons (FL vs BCE on two metrics, FL vs $\alpha$-BCE on two metrics).

    \item \textbf{Effect Size:} We report Cohen's $d$ as the effect size measure for paired samples, with interpretation: $|d| < 0.2$ negligible, $0.2$--$0.5$ small, $0.5$--$0.8$ medium, $> 0.8$ large.
\end{itemize}

Results are reported as mean $\pm$ standard deviation across seeds, with statistical significance indicators.

\subsection{Ablation Study Design}
\label{sec:ablation}

To isolate the contribution of each component, we conduct the following ablation studies:

\begin{enumerate}
    \item \textbf{Loss Function Ablation:} Compare BCE, BPR, and Focal Loss on fixed NeuMF architecture to isolate the effect of the loss function.

    \item \textbf{Focal Loss Component Ablation:}
    \begin{itemize}
        \item Vary $\gamma \in \{0, 0.5, 1.0, 2.0, 3.0\}$ with fixed $\alpha=0.25$ to analyze the focusing effect ($\gamma=0$ reduces to $\alpha$-BCE).
        \item Vary $\alpha \in \{0.25, 0.5, 0.75\}$ with fixed $\gamma=2.0$ to analyze the class-balancing effect.
    \end{itemize}

    \item \textbf{Mechanism Isolation ($\alpha$-BCE Control):} To test H3, we compare full Focal Loss ($\gamma > 0$) against $\alpha$-balanced BCE ($\gamma = 0$) with matched $\alpha$ values. If performance differences are negligible, the focusing mechanism provides no benefit beyond class reweighting; if Focal Loss significantly outperforms $\alpha$-BCE, the focusing mechanism is demonstrably necessary.

    Specifically, we evaluate:
    \begin{itemize}
        \item BCE: $\gamma = 0$, $\alpha = 0.5$ (standard)
        \item $\alpha$-BCE: $\gamma = 0$, $\alpha \in \{0.25, 0.5, 0.75\}$
        \item Focal Loss: $\gamma \in \{0.5, 1.0, 2.0, 3.0\}$, $\alpha \in \{0.25, 0.5, 0.75\}$
    \end{itemize}

    \item \textbf{Architecture Ablation:} Test Focal Loss on GMF-only, MLP-only, and full NeuMF to determine architecture-specific effects.

    \item \textbf{Dataset Ablation:} Compare Focal Loss improvements across ML-100K and ML-1M to analyze the correlation between imbalance ratio and performance gain.

    \item \textbf{Negative Sampling Ratio Ablation:} To test H2 (sampling robustness), we train both NeuMF-BCE and NeuMF-FL across varying negative sampling ratios: 1:4 (standard), 1:10 (moderate), and 1:50 (high). This ablation tests whether Focal Loss can effectively filter noise when exposed to higher proportions of negative examples, maintaining performance where BCE-based training degrades. Computational constraints preclude testing the full 1:All ratio; we use 1:50 as a tractable upper bound that still represents a substantial increase in negative exposure.
\end{enumerate}

Table~\ref{tab:ablation-matrix} summarizes the ablation configurations. Table~\ref{tab:sampling-ablation} presents the negative sampling ratio experimental design.

\begin{table}[h]
\centering
\caption{Ablation study configuration matrix}
\label{tab:ablation-matrix}
\begin{tabular}{lccc}
\toprule
\textbf{Study} & \textbf{BCE} & $\boldsymbol{\alpha}$\textbf{-BCE} & \textbf{Focal Loss} \\
\midrule
Baseline & $\gamma=0, \alpha=0.5$ & -- & -- \\
Class weighting & -- & $\gamma=0, \alpha \in \{0.25, 0.5, 0.75\}$ & -- \\
Focusing effect & -- & $\gamma=0$ (control) & $\gamma \in \{0.5, 1, 2, 3\}$ \\
Full grid & -- & -- & $\gamma \times \alpha$ grid \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Negative sampling ratio experimental design}
\label{tab:sampling-ablation}
\begin{tabular}{lcccc}
\toprule
\textbf{Ratio} & \textbf{Imbalance} & \textbf{BCE} & $\boldsymbol{\alpha}$\textbf{-BCE} & \textbf{FL} \\
\midrule
1:4  & Low      & Test & Test & Test \\
1:10 & Moderate & Test & Test & Test \\
1:50 & High     & \textbf{Primary} & \textbf{Primary} & \textbf{Primary} \\
\bottomrule
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:results}

This section presents experimental results evaluating Focal Loss for Neural Collaborative Filtering on the MovieLens 100K dataset. We report findings from the primary experiment at 1:50 negative sampling, a robustness study across sampling ratios, and ablation analyses isolating the contributions of individual components.

\subsection{Primary Experiment: 1:50 Negative Sampling}
\label{sec:primary-results}

Table~\ref{tab:primary-results} presents the multi-seed comparison between NeuMF trained with BCE and Focal Loss at 1:50 negative sampling, our primary experimental condition designed to approximate production-level class imbalance.

\begin{table}[h]
\centering
\caption{Primary experiment results at 1:50 sampling (5 seeds)}
\label{tab:primary-results}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{NDCG@10} & \textbf{HR@10} & \textbf{Change} \\
\midrule
NeuMF-BCE & $0.0580 \pm 0.0030$ & $0.1125 \pm 0.0042$ & -- \\
NeuMF-FL ($\gamma{=}2$, $\alpha{=}0.25$) & $\mathbf{0.0638 \pm 0.0041}$ & $\mathbf{0.1274 \pm 0.0051}$ & \textbf{+10.0\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Statistical Validation.} The Wilcoxon signed-rank test yields $p = 0.0625$, which is the minimum achievable $p$-value with $n = 5$ paired observations where all pairs favor one condition. Cohen's $d = 2.632$ indicates a very large effect size. All five seed-paired comparisons favored Focal Loss, demonstrating consistent improvement across random initializations.

\textbf{Interpretation.} At high class imbalance (1:50 sampling), Focal Loss provides a substantial 10.0\% improvement in NDCG@10 over BCE. While the $p$-value does not meet the conventional $\alpha = 0.05$ threshold, the very large effect size and perfect consistency across seeds support practical significance. The effect size exceeds the threshold ($|d| > 0.8$) for a large effect by more than threefold.

\subsection{Robustness Study: Multiple Sampling Ratios}
\label{sec:robustness-results}

To evaluate H2 (Robustness), we tested both models across varying negative sampling ratios. Table~\ref{tab:robustness-results} presents the comparison using default Focal Loss parameters ($\gamma = 2.0$, $\alpha = 0.25$).

\begin{table}[h]
\centering
\caption{Robustness study: BCE vs Focal Loss across sampling ratios}
\label{tab:robustness-results}
\begin{tabular}{lccccl}
\toprule
\textbf{Ratio} & \textbf{BCE NDCG@10} & \textbf{FL NDCG@10} & \textbf{$\Delta$ NDCG} & \textbf{$\Delta$ HR@10} & \textbf{Winner} \\
\midrule
1:4  & 0.0524 & 0.0607 & +15.8\% & +26.6\% & FL \\
1:10 & 0.0575 & 0.0556 & $-3.3\%$ & $-8.2\%$ & BCE \\
1:50 & 0.0541 & 0.0595 & +10.0\% & +13.3\% & FL \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Statistical Analysis.} Across the three sampling conditions, Focal Loss wins on 2 of 3 ratios for both NDCG@10 and HR@10. The mean improvement is +7.5\% for NDCG@10. Cohen's $d = 0.92$ indicates a large effect size across conditions, though bootstrap 95\% confidence intervals $[-0.0019, +0.0083]$ include zero due to the limited sample size ($n = 3$).

\textbf{Key Finding.} With default parameters from computer vision ($\gamma = 2.0$, $\alpha = 0.25$), Focal Loss provides substantial improvements at low (1:4) and high (1:50) imbalance ratios but underperforms BCE at moderate imbalance (1:10). This non-monotonic relationship suggests that the interaction between Focal Loss hyperparameters and the training data distribution is more complex than anticipated.

\subsection{Grid Search Analysis}
\label{sec:grid-results}

To understand hyperparameter sensitivity, we conducted a grid search over 36 configurations (4 $\gamma$ values $\times$ 3 $\alpha$ values $\times$ 3 sampling ratios). Table~\ref{tab:best-configs} presents the best-performing configuration per sampling ratio.

\begin{table}[h]
\centering
\caption{Optimal Focal Loss configurations per sampling ratio}
\label{tab:best-configs}
\begin{tabular}{lccccc}
\toprule
\textbf{Ratio} & \textbf{Best $\gamma$} & \textbf{Best $\alpha$} & \textbf{NDCG@10} & \textbf{HR@10} & \textbf{vs BCE} \\
\midrule
1:4  & 0.5 & 0.50 & 0.0708 & 0.1316 & +35.1\% \\
1:10 & 1.0 & 0.50 & 0.0610 & 0.1221 & +6.1\% \\
1:50 & 2.0 & 0.25 & 0.0672 & 0.1231 & +24.2\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Hyperparameter Sensitivity.} Focal Loss configurations beat the BCE baseline in only 17 of 36 tested configurations (47.2\%), indicating high sensitivity to hyperparameter choice. The near-50\% win rate suggests that without proper tuning, Focal Loss is as likely to hurt as to help.

\textbf{Optimal Parameter Patterns.} The results reveal a clear pattern: lower imbalance ratios favor weaker focusing ($\gamma = 0.5$--$1.0$) and balanced class weights ($\alpha = 0.5$), while higher imbalance ratios require stronger focusing ($\gamma = 2.0$) and asymmetric weighting ($\alpha = 0.25$). This validates our alpha-sampling interaction analysis (Section~\ref{sec:alpha-sampling}).

\subsection{Mechanism Isolation: $\alpha$-BCE Control}
\label{sec:mechanism-results}

To test H3 (Mechanism), we compared full Focal Loss ($\gamma > 0$) against $\alpha$-balanced BCE ($\gamma = 0$) with matched $\alpha$ values. Table~\ref{tab:mechanism-results} presents results at 1:10 sampling.

\begin{table}[h]
\centering
\caption{Mechanism isolation at 1:10 sampling}
\label{tab:mechanism-results}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{NDCG@10} & \textbf{HR@10} & \textbf{vs BCE} \\
\midrule
BCE (baseline) & 0.0575 & 0.1157 & -- \\
$\alpha$-BCE ($\gamma{=}0$, $\alpha{=}0.25$) & \textbf{0.0600} & \textbf{0.1231} & +4.3\% \\
Focal Loss ($\gamma{=}2$, $\alpha{=}0.25$) & 0.0556 & 0.1062 & $-3.3\%$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Critical Finding.} At moderate imbalance (1:10), $\alpha$-balanced BCE \emph{outperforms} both standard BCE and full Focal Loss. This indicates that class reweighting alone is sufficient at this imbalance level, and the focusing mechanism ($\gamma > 0$) provides no additional benefit---in fact, it degrades performance. H3 is \textbf{not supported} at moderate imbalance.

\subsection{Hypothesis Outcomes}
\label{sec:hypothesis-outcomes}

Table~\ref{tab:hypothesis-outcomes} summarizes the status of each research hypothesis based on experimental evidence.

\begin{table}[h]
\centering
\caption{Summary of hypothesis outcomes}
\label{tab:hypothesis-outcomes}
\begin{tabular}{lp{5.5cm}l}
\toprule
\textbf{Hypothesis} & \textbf{Evidence} & \textbf{Status} \\
\midrule
H1 (Efficacy) & +10.0\% NDCG@10 at 1:50; $p = 0.0625$; Cohen's $d = 2.632$ & \textbf{Supported} \\
H2 (Robustness) & FL wins 2/3 ratios; fails at 1:10 & Partially supported \\
H3 (Mechanism) & $\alpha$-BCE beats FL at 1:10 & Not supported (1:10) \\
\bottomrule
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\label{sec:discussion}

Our experimental results reveal a nuanced picture of Focal Loss for collaborative filtering that differs substantially from its established success in computer vision. We discuss key findings and their implications.

\subsection{Condition-Dependent Effectiveness}

The most striking finding is that Focal Loss effectiveness is highly condition-dependent. With default parameters from object detection ($\gamma = 2.0$, $\alpha = 0.25$), Focal Loss provides substantial improvements only at extreme class imbalance (1:50 sampling), while actually hurting performance at moderate imbalance (1:10). This contrasts with computer vision applications where these default parameters work reliably across diverse datasets.

The explanation lies in the alpha-sampling interaction we analyzed in Section~\ref{sec:alpha-sampling}. At 1:50 sampling with $\alpha = 0.25$, the effective negative-to-positive weight ratio reaches 150:1, which happens to be appropriate for the extreme imbalance scenario. However, this same parameterization creates excessive negative weighting at lower sampling ratios, overwhelming the positive signal.

\subsection{Class Weighting vs. Focusing}

Our mechanism isolation experiment (Section~\ref{sec:mechanism-results}) provides evidence that the focusing mechanism ($\gamma > 0$) may not be the primary driver of Focal Loss benefits in recommendation. At 1:10 sampling, $\alpha$-balanced BCE ($\gamma = 0$) outperformed full Focal Loss. This suggests that simple class reweighting is sufficient at moderate imbalance levels, and the additional complexity of dynamic down-weighting based on prediction confidence does not provide incremental benefit.

This finding has practical implications: practitioners facing moderate class imbalance may achieve better results with the simpler $\alpha$-BCE approach, avoiding the additional hyperparameter tuning burden of the focusing parameter.

\subsection{Hyperparameter Sensitivity}

The grid search analysis reveals concerning hyperparameter sensitivity. With only 47\% of tested configurations outperforming the BCE baseline, selecting appropriate Focal Loss parameters is critical. The optimal parameters vary substantially across sampling ratios:
\begin{itemize}
    \item Low imbalance (1:4): weak focusing ($\gamma = 0.5$), balanced weights ($\alpha = 0.5$)
    \item Moderate imbalance (1:10): moderate focusing ($\gamma = 1.0$), balanced weights ($\alpha = 0.5$)
    \item High imbalance (1:50): strong focusing ($\gamma = 2.0$), asymmetric weights ($\alpha = 0.25$)
\end{itemize}

This variability complicates deployment, as practitioners must either conduct extensive hyperparameter search or develop heuristics for parameter selection based on dataset characteristics.

\subsection{Statistical Considerations}

Our statistical validation faces inherent power limitations. With $n = 5$ paired observations in the primary experiment, the minimum achievable $p$-value for the Wilcoxon signed-rank test is 0.0625 when all pairs favor one condition. While this does not meet the conventional $\alpha = 0.05$ threshold, the very large effect size (Cohen's $d = 2.632$) provides strong evidence for practical significance~\cite{demvsar2006statistical}.

We emphasize effect sizes over $p$-values following recommendations for machine learning experiments, where practical significance often matters more than statistical significance. The 10\% improvement in NDCG@10 represents a meaningful gain in recommendation quality.

\subsection{Implications for Practitioners}

Based on our findings, we offer the following recommendations:
\begin{enumerate}
    \item \textbf{Assess imbalance severity first.} Focal Loss is most beneficial at extreme imbalance ratios (1:50+). For moderate imbalance, consider $\alpha$-balanced BCE as a simpler alternative.
    \item \textbf{Adjust $\alpha$ based on sampling ratio.} The effective class weight ratio depends on both $\alpha$ and the negative sampling ratio. Use the formula $R_{\text{eff}} = (1-\alpha) \times r / \alpha$ to estimate the effective weighting.
    \item \textbf{Start with lower $\gamma$ values.} Unlike computer vision where $\gamma = 2$ is standard, recommendation tasks may benefit from weaker focusing ($\gamma = 0.5$--$1.0$) at moderate imbalance levels.
    \item \textbf{Validate with multiple seeds.} Given the sensitivity to initialization and hyperparameters, multi-seed evaluation is essential for reliable conclusions.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{sec:conclusion}

This work investigated Focal Loss as an alternative training objective for Neural Collaborative Filtering, motivated by the fundamental class imbalance problem in implicit feedback recommendation. Through controlled experiments on MovieLens 100K, we evaluated three hypotheses concerning efficacy, robustness, and mechanism.

\textbf{Main Findings.} Focal Loss provides substantial improvements (+10\% NDCG@10) at high class imbalance (1:50 negative sampling), with a very large effect size (Cohen's $d = 2.632$). However, these benefits are condition-dependent: with default parameters, Focal Loss underperforms BCE at moderate imbalance (1:10). Our mechanism isolation experiment suggests that class reweighting ($\alpha$) rather than the focusing mechanism ($\gamma$) may be the primary driver of improvements at moderate imbalance levels.

\textbf{Contributions.} We provide (1) a controlled evaluation of Focal Loss on standard recommendation benchmarks, isolated from architectural confounds present in prior work; (2) analysis of the alpha-sampling interaction that explains the dependency of optimal hyperparameters on training conditions; and (3) practical guidelines for deploying Focal Loss in recommendation systems based on imbalance severity.

\textbf{Limitations.} Our experiments are limited to the MovieLens 100K dataset. The statistical validation is constrained by sample size, achieving $p = 0.0625$ rather than conventional significance. The high hyperparameter sensitivity (47\% win rate across configurations) limits the generalizability of specific parameter recommendations.

\textbf{Future Work.} Extensions include validation on larger-scale datasets (MovieLens 1M, Amazon, Yelp), development of adaptive $\alpha$ selection formulas that account for sampling ratio, and investigation of whether the focusing mechanism provides benefits at extreme imbalance (1:50+) where full Focal Loss outperformed $\alpha$-BCE in preliminary analysis.

\section{Author Contributions}

\begin{itemize}
    \item \textbf{Rotem Even Zur:} Authored the \textit{Traditional Methods} subsection in the Background and the \textit{Improvements to Traditional Collaborative Filtering} subsection in Related Work. Contributed to problem formulation, experimental design, and interpretation of results.
    
    \item \textbf{Dvir Chitrit:} Authored the \textit{Neural Collaborative Filtering (NCF)} subsection in the Background and co-authored the \textit{Neural Collaborative Filtering Architectures} subsection in Related Work. Contributed to the Methodology and analysis of results.
    
    \item \textbf{Guy Kalati:} Authored the \textit{Addressing Class Imbalance: The Role of Focal Loss} subsection in the Background and co-authored the \textit{Loss Function Innovations for Recommendation} subsection in Related Work. Contributed to the loss formulation and experimental design.
    
    \item \textbf{Omer Eliyahu:} Led the implementation of the models and experiments, and was responsible for the overall organization and consistency of the manuscript. Contributed to the Experimental Setup, Results, and Discussion sections.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define the bibliography style and file.

% \bibliographystyle{ACM-Reference-Format}  -> provided ordering method by course staff - orders alphabeticalliy
\bibliographystyle{unsrt} % number the bib-references by order of citations accurances in paper -> our addition
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
