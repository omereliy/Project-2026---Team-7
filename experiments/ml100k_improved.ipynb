{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "# NCF with Focal Loss - ML-100K Improved Experimental Design\n",
        "\n",
        "**Paper**: \"Addressing Class Imbalance in NCF with Focal Loss\" (AAMAS 2025)\n",
        "\n",
        "## Objective\n",
        "\n",
        "This notebook implements improved experimental methodology to test three key hypotheses:\n",
        "\n",
        "**H1 (Efficacy)**: Focal Loss improves NeuMF performance over standard BCE loss on implicit feedback recommendation.\n",
        "\n",
        "**H2 (Robustness)**: The improvement is robust across different negative sampling ratios (1:4, 1:10, 1:50).\n",
        "\n",
        "**H3 (Mechanism)**: The focusing effect (gamma > 0) is necessary beyond simple class weighting (alpha-balanced BCE).\n",
        "\n",
        "## Improvements Over Original Design\n",
        "\n",
        "1. **Primary experiment at 1:10 sampling** (not just 1:4) - more realistic for real systems\n",
        "2. **Alpha-balanced BCE control** - isolates focusing effect from class weighting\n",
        "3. **Alpha-sampling interaction analysis** - addresses confound between alpha and sampling ratio\n",
        "4. **Robustness study** - tests across 3 sampling ratios (1:4, 1:10, 1:50)\n",
        "5. **Training dynamics tracking** - validates mechanism by analyzing loss contribution by confidence bin\n",
        "6. **Proper statistical framing** - clear hypothesis testing structure\n",
        "\n",
        "## Dataset: MovieLens 100K\n",
        "\n",
        "- 100,000 ratings from 943 users on 1,682 movies\n",
        "- Binarization: ratings >= 4 -> positive interaction\n",
        "- Leave-one-out evaluation (most recent for test)\n",
        "- Full ranking evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {
        "id": "cell-1"
      },
      "source": [
        "## Cell 1: Suppress Warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cell-2",
      "metadata": {
        "id": "cell-2"
      },
      "outputs": [],
      "source": [
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {
        "id": "cell-3"
      },
      "source": [
        "## Cell 2: Install Dependencies\n",
        "\n",
        "**Instructions:**\n",
        "1. Run the install cell below\n",
        "2. **RESTART** the runtime (Runtime -> Restart session)\n",
        "3. Run the verification cell\n",
        "4. Continue with remaining cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cell-4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-4",
        "outputId": "a858c119-ded7-4842-8372-a7cb7a7e5b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hFound existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "============================================================\n",
            "RESTART REQUIRED\n",
            "============================================================\n",
            "Go to: Runtime -> Restart session\n",
            "Then run the NEXT cell to verify installation.\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Install Dependencies (Part 1)\n",
        "# ============================================\n",
        "# After running this cell, RESTART the runtime, then run the next cell\n",
        "\n",
        "%pip install -q ray\n",
        "%pip install -q recbole==1.2.0\n",
        "%pip install -q kmeans-pytorch\n",
        "\n",
        "# Force numpy 1.x (required for RecBole compatibility)\n",
        "%pip uninstall -y numpy\n",
        "%pip install -q \"numpy<2\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESTART REQUIRED\")\n",
        "print(\"=\"*60)\n",
        "print(\"Go to: Runtime -> Restart session\")\n",
        "print(\"Then run the NEXT cell to verify installation.\")"
      ]
    },
    {
      "cell_type": "code",
      "id": "i5jpzem9ydf",
      "source": [
        "# ============================================\n",
        "# Verify Installation (Part 2) - Run AFTER restart\n",
        "# ============================================\n",
        "import numpy as np\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "\n",
        "if np.__version__.startswith(\"2.\"):\n",
        "    print(\"\\nERROR: NumPy 2.x still detected!\")\n",
        "    print(\"Try: Runtime -> Restart session -> Run this cell again\")\n",
        "else:\n",
        "    print(\"SUCCESS: NumPy 1.x installed. Continue to next cell.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5jpzem9ydf",
        "outputId": "adcc5fc5-099e-4b1d-c72c-b628eacb7dee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "SUCCESS: NumPy 1.x installed. Continue to next cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-5",
      "metadata": {
        "id": "cell-5"
      },
      "source": [
        "## Cell 3: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cell-6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-6",
        "outputId": "f210e947-8a86-4094-eba1-2292eea9bce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/Project-2026---Team-7'...\n",
            "remote: Enumerating objects: 211, done.\u001b[K\n",
            "remote: Counting objects: 100% (211/211), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 211 (delta 117), reused 152 (delta 75), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (211/211), 436.08 KiB | 6.51 MiB/s, done.\n",
            "Resolving deltas: 100% (117/117), done.\n",
            "/content/Project-2026---Team-7/experiments\n",
            "Running on Google Colab - repo cloned\n",
            "Applied RecBole distributed fix\n",
            "Using GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "# Standard imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "# ============================================\n",
        "# Environment Setup (Colab / Local)\n",
        "# ============================================\n",
        "if 'google.colab' in sys.modules:\n",
        "    # Clone repo if not already present\n",
        "    if not os.path.exists('/content/Project-2026---Team-7'):\n",
        "        !git clone https://github.com/omereliy/Project-2026---Team-7.git /content/Project-2026---Team-7\n",
        "    # Add experiments folder to path\n",
        "    sys.path.insert(0, '/content/Project-2026---Team-7/experiments')\n",
        "    %cd /content/Project-2026---Team-7/experiments\n",
        "    print(\"Running on Google Colab - repo cloned\")\n",
        "else:\n",
        "    # Local: assume running from experiments folder\n",
        "    sys.path.insert(0, '.')\n",
        "    print(\"Running locally\")\n",
        "\n",
        "# ============================================\n",
        "# RecBole Fix: Patch torch.distributed.barrier for single-GPU\n",
        "# See: https://github.com/RUCAIBox/RecBole/issues/1989\n",
        "# ============================================\n",
        "if not hasattr(dist, '_barrier_patched'):\n",
        "    _original_barrier = dist.barrier\n",
        "    def _patched_barrier(*args, **kwargs):\n",
        "        if dist.is_available() and dist.is_initialized():\n",
        "            return _original_barrier(*args, **kwargs)\n",
        "        # Skip barrier if not in distributed mode\n",
        "    dist.barrier = _patched_barrier\n",
        "    dist._barrier_patched = True\n",
        "    print(\"Applied RecBole distributed fix\")\n",
        "\n",
        "# PyTorch 2.6+ compatibility patch\n",
        "if not hasattr(torch, '_load_patched'):\n",
        "    _original_torch_load = torch.load\n",
        "    def _patched_torch_load(*args, **kwargs):\n",
        "        if 'weights_only' not in kwargs:\n",
        "            kwargs['weights_only'] = False\n",
        "        return _original_torch_load(*args, **kwargs)\n",
        "    torch.load = _patched_torch_load\n",
        "    torch._load_patched = True\n",
        "\n",
        "from focal_loss_utils import (\n",
        "    # Loss functions\n",
        "    FocalLoss, AlphaBalancedBCE,\n",
        "    # Configuration\n",
        "    get_base_config, get_neumf_config,\n",
        "    # Training functions\n",
        "    train_neumf_focal_loss, train_neumf_alpha_bce,\n",
        "    # Evaluation\n",
        "    create_comparison_table, compute_improvement,\n",
        "    # Validation\n",
        "    validate_focal_loss_implementation, demonstrate_focal_loss_effect,\n",
        "    # Alpha-sampling analysis\n",
        "    analyze_alpha_sampling_interaction, compute_effective_class_ratio,\n",
        "    get_balanced_alpha,\n",
        "    # Multi-seed experiments\n",
        "    run_multi_seed_experiment\n",
        ")\n",
        "\n",
        "# RecBole imports\n",
        "from recbole.quick_start import run_recbole\n",
        "from recbole.model.general_recommender.neumf import NeuMF\n",
        "from recbole.config import Config\n",
        "from recbole.data import create_dataset, data_preparation\n",
        "from recbole.trainer import Trainer\n",
        "from recbole.utils import init_seed, init_logger\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {
        "id": "cell-7"
      },
      "source": [
        "## Cell 4: Focal Loss Validation Test\n",
        "\n",
        "Verify that the Focal Loss implementation is correct:\n",
        "1. FL(gamma=0, alpha=0.5) = 0.5 * BCE\n",
        "2. Higher gamma reduces loss for well-classified examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cell-8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-8",
        "outputId": "29b94ed2-1ac0-4ab0-f1cf-0117d5b5f448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "VALIDATION: Focal Loss Implementation\n",
            "======================================================================\n",
            "Focal Loss implementation PASSED all tests!\n",
            "\n",
            "======================================================================\n",
            "DEMONSTRATION: Focal Loss Effect on Easy vs Hard Examples\n",
            "======================================================================\n",
            "\n",
            "                      scenario  bce_loss  focal_loss  reduction_factor\n",
            "Easy negative (pred=0.05, y=0)  0.051293    0.000096        533.332988\n",
            " Hard positive (pred=0.3, y=1)  1.203973    0.147487          8.163266\n",
            " Hard negative (pred=0.7, y=0)  1.203973    0.442460          2.721089\n",
            "Easy positive (pred=0.95, y=1)  0.051293    0.000032       1599.999252\n",
            "\n",
            "Interpretation:\n",
            "- Easy examples (high confidence) are down-weighted by 10-100x\n",
            "- Hard examples (low confidence) retain most of their loss contribution\n",
            "- This forces the model to focus on hard-to-classify examples\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"VALIDATION: Focal Loss Implementation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test implementation correctness\n",
        "validate_focal_loss_implementation()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DEMONSTRATION: Focal Loss Effect on Easy vs Hard Examples\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Demonstrate focusing effect\n",
        "demo_df = demonstrate_focal_loss_effect()\n",
        "print(\"\\n\" + demo_df.to_string(index=False))\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Easy examples (high confidence) are down-weighted by 10-100x\")\n",
        "print(\"- Hard examples (low confidence) retain most of their loss contribution\")\n",
        "print(\"- This forces the model to focus on hard-to-classify examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {
        "id": "cell-9"
      },
      "source": [
        "## Cell 5: Experiment Configuration\n",
        "\n",
        "We test three negative sampling ratios:\n",
        "- **1:4** - Standard ratio used in many NCF papers\n",
        "- **1:10** - **PRIMARY EXPERIMENT** - more realistic for production systems\n",
        "- **1:50** - High negative ratio to test robustness under extreme imbalance\n",
        "\n",
        "The primary experiment uses 1:10 sampling because:\n",
        "1. It's more representative of real recommendation systems\n",
        "2. It provides stronger evidence of robustness than just testing at 1:4\n",
        "3. The class imbalance problem is more pronounced, making Focal Loss improvements more meaningful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell-10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-10",
        "outputId": "dbc47660-d63f-49fb-853c-447d914719d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: ml-100k\n",
            "Negative sampling ratios: [4, 10, 50]\n",
            "Primary experiment: 1:10 sampling\n",
            "\n",
            "Focal Loss hyperparameters:\n",
            "  gamma = 2.0 (focusing parameter)\n",
            "  alpha = 0.25 (class balancing weight for positives)\n"
          ]
        }
      ],
      "source": [
        "# Dataset and sampling ratios\n",
        "DATASET = 'ml-100k'\n",
        "SAMPLING_RATIOS = [4, 10, 50]  # negative samples per positive\n",
        "PRIMARY_RATIO = 10  # Main experiment uses 1:10 sampling\n",
        "\n",
        "# Focal Loss hyperparameters (from literature)\n",
        "GAMMA = 2.0  # Focusing parameter\n",
        "ALPHA = 0.25  # Class balancing weight for positives\n",
        "\n",
        "print(f\"Dataset: {DATASET}\")\n",
        "print(f\"Negative sampling ratios: {SAMPLING_RATIOS}\")\n",
        "print(f\"Primary experiment: 1:{PRIMARY_RATIO} sampling\")\n",
        "print(f\"\\nFocal Loss hyperparameters:\")\n",
        "print(f\"  gamma = {GAMMA} (focusing parameter)\")\n",
        "print(f\"  alpha = {ALPHA} (class balancing weight for positives)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "source": [
        "## Cell 6: Configuration Setup\n",
        "\n",
        "Create configurations for each sampling ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cell-12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-12",
        "outputId": "389f0435-9bb1-4f1c-aeca-0d4e6282f73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration for 1:4 sampling created\n",
            "Configuration for 1:10 sampling created\n",
            "Configuration for 1:50 sampling created\n",
            "\n",
            "NeuMF Architecture:\n",
            "  MF Embedding Size: 64\n",
            "  MLP Embedding Size: 64\n",
            "  MLP Hidden Layers: [128, 64, 32]\n",
            "  Dropout: 0.0\n",
            "\n",
            "Training Settings:\n",
            "  Max Epochs: 100\n",
            "  Early Stopping Patience: 10\n",
            "  Learning Rate: 0.001\n",
            "  Batch Size: 256\n"
          ]
        }
      ],
      "source": [
        "# Create configurations for each sampling ratio\n",
        "configs = {}\n",
        "for ratio in SAMPLING_RATIOS:\n",
        "    base = get_base_config(DATASET, device, neg_sample_num=ratio)\n",
        "    configs[ratio] = get_neumf_config(base)\n",
        "    print(f\"Configuration for 1:{ratio} sampling created\")\n",
        "\n",
        "print(\"\\nNeuMF Architecture:\")\n",
        "print(f\"  MF Embedding Size: {configs[PRIMARY_RATIO]['mf_embedding_size']}\")\n",
        "print(f\"  MLP Embedding Size: {configs[PRIMARY_RATIO]['mlp_embedding_size']}\")\n",
        "print(f\"  MLP Hidden Layers: {configs[PRIMARY_RATIO]['mlp_hidden_size']}\")\n",
        "print(f\"  Dropout: {configs[PRIMARY_RATIO]['dropout_prob']}\")\n",
        "print(f\"\\nTraining Settings:\")\n",
        "print(f\"  Max Epochs: {configs[PRIMARY_RATIO]['epochs']}\")\n",
        "print(f\"  Early Stopping Patience: {configs[PRIMARY_RATIO]['stopping_step']}\")\n",
        "print(f\"  Learning Rate: {configs[PRIMARY_RATIO]['learning_rate']}\")\n",
        "print(f\"  Batch Size: {configs[PRIMARY_RATIO]['train_batch_size']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y9p0jgr1vn",
      "source": [
        "## Cell 6b: Load Saved Results (Optional)\n",
        "\n",
        "**Skip training and load pre-computed results.**\n",
        "\n",
        "If you want to skip re-running experiments (e.g., for analysis or after runtime restart), run this cell to load saved results from `saved_results_ml100k.py`.\n",
        "\n",
        "Set `LOAD_SAVED_RESULTS = True` to use saved results, or `False` to train from scratch."
      ],
      "metadata": {
        "id": "y9p0jgr1vn"
      }
    },
    {
      "cell_type": "code",
      "id": "a06och7xdqf",
      "source": [
        "# ============================================\n",
        "# LOAD SAVED RESULTS (Optional)\n",
        "# ============================================\n",
        "# Set to True to skip training and use pre-computed results\n",
        "# Set to False to train models from scratch\n",
        "\n",
        "LOAD_SAVED_RESULTS = False  # <-- Change this to False to train from scratch\n",
        "\n",
        "if LOAD_SAVED_RESULTS:\n",
        "    from saved_results_ml100k import (\n",
        "        load_all_results, print_summary,\n",
        "        RESULT_BCE, RESULT_ALPHA_BCE, RESULT_FOCAL,\n",
        "        ROBUSTNESS_RESULTS, GRID_RESULTS\n",
        "    )\n",
        "\n",
        "    # Load all results\n",
        "    saved = load_all_results()\n",
        "\n",
        "    # Set variables to match notebook expectations\n",
        "    result_bce = saved['result_bce']\n",
        "    result_alpha_bce = saved['result_alpha_bce']\n",
        "    result_focal = saved['result_focal']\n",
        "    robustness_results = saved['robustness_results']\n",
        "    grid_results = saved['grid_results']\n",
        "\n",
        "    # Print summary\n",
        "    print_summary()\n",
        "\n",
        "    print(\"\\nVariables loaded:\")\n",
        "    print(\"  - result_bce\")\n",
        "    print(\"  - result_alpha_bce\")\n",
        "    print(\"  - result_focal\")\n",
        "    print(\"  - robustness_results\")\n",
        "    print(\"  - grid_results\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"LOAD_SAVED_RESULTS = True\")\n",
        "    print(\"Training cells will be SKIPPED automatically.\")\n",
        "    print(\"You can safely 'Run All' - analysis cells will use saved results.\")\n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"LOAD_SAVED_RESULTS = False\")\n",
        "    print(\"Training cells will run and generate fresh results.\")\n",
        "    print(\"This will take significant time.\")"
      ],
      "metadata": {
        "id": "a06och7xdqf",
        "outputId": "1c72fd76-1082-4060-fe12-6cbda8e20600",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOAD_SAVED_RESULTS = False\n",
            "Training cells will run and generate fresh results.\n",
            "This will take significant time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "source": [
        "## Cell 7: Alpha-Sampling Interaction Analysis\n",
        "\n",
        "**Important Issue**: The alpha parameter and sampling ratio interact in non-obvious ways.\n",
        "\n",
        "With 1:4 sampling and alpha=0.5:\n",
        "- Each batch has 1 positive (weight: 0.5) and 4 negatives (weight: 0.5 each)\n",
        "- **Effective ratio**: (0.5 × 4) / 0.5 = 4:1 (negatives still get 4x more weight!)\n",
        "\n",
        "For balanced weighting with 1:N sampling:\n",
        "- Need alpha = N / (N + 1)\n",
        "- Example: 1:4 -> alpha = 0.8, 1:10 -> alpha = 0.909, 1:50 -> alpha = 0.98\n",
        "\n",
        "This explains why alpha=0.25 (from computer vision) may not be optimal for NCF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cell-14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-14",
        "outputId": "36019947-b9e7-4ce9-e56a-31ad990bce6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ALPHA-SAMPLING INTERACTION ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "neg_ratio  alpha effective_ratio balanced_alpha  is_balanced\n",
            "      1:4   0.25          12.0:1           0.80        False\n",
            "      1:4   0.50           4.0:1           0.80        False\n",
            "      1:4   0.75           1.3:1           0.80        False\n",
            "     1:10   0.25          30.0:1           0.91        False\n",
            "     1:10   0.50          10.0:1           0.91        False\n",
            "     1:10   0.75           3.3:1           0.91        False\n",
            "     1:50   0.25         150.0:1           0.98        False\n",
            "     1:50   0.50          50.0:1           0.98        False\n",
            "     1:50   0.75          16.7:1           0.98        False\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "BALANCED ALPHA VALUES (for effective ratio = 1:1)\n",
            "----------------------------------------------------------------------\n",
            "1:4 sampling -> balanced alpha = 0.800\n",
            "  Using alpha=0.25 -> effective ratio = 12.0:1\n",
            "\n",
            "1:10 sampling -> balanced alpha = 0.909\n",
            "  Using alpha=0.25 -> effective ratio = 30.0:1\n",
            "\n",
            "1:50 sampling -> balanced alpha = 0.980\n",
            "  Using alpha=0.25 -> effective ratio = 150.0:1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ALPHA-SAMPLING INTERACTION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Analyze interaction for various alpha values\n",
        "interaction_df = analyze_alpha_sampling_interaction(\n",
        "    neg_ratios=SAMPLING_RATIOS,\n",
        "    alphas=[0.25, 0.5, 0.75]\n",
        ")\n",
        "print(\"\\n\" + interaction_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"BALANCED ALPHA VALUES (for effective ratio = 1:1)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for ratio in SAMPLING_RATIOS:\n",
        "    balanced = get_balanced_alpha(ratio)\n",
        "    actual_ratio = compute_effective_class_ratio(ALPHA, ratio)\n",
        "    print(f\"1:{ratio} sampling -> balanced alpha = {balanced:.3f}\")\n",
        "    print(f\"  Using alpha={ALPHA} -> effective ratio = {actual_ratio:.1f}:1\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {
        "id": "cell-15"
      },
      "source": [
        "## Experiment 1: NeuMF-BCE Baseline (1:10 Sampling)\n",
        "\n",
        "Train standard NeuMF with Binary Cross-Entropy loss.\n",
        "\n",
        "This serves as the baseline for H1 (efficacy hypothesis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cell-16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-16",
        "outputId": "a1ce2113-b504-40b5-e976-48a792a13ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EXPERIMENT 1: NeuMF-BCE Baseline (1:10 sampling)\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n",
            "Train     0:   0%|                                                         | 0/2326 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n",
            "Train     0: 100%|████████████████████| 2326/2326 [00:16<00:00, 142.74it/s, GPU RAM: 0.03 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 191.90it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train     1: 100%|████████████████████| 2326/2326 [00:15<00:00, 147.59it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 209.94it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train     2: 100%|████████████████████| 2326/2326 [00:15<00:00, 149.95it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 209.53it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train     3: 100%|████████████████████| 2326/2326 [00:15<00:00, 149.15it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 208.90it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train     4: 100%|████████████████████| 2326/2326 [00:16<00:00, 145.16it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 207.31it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train     5: 100%|████████████████████| 2326/2326 [00:15<00:00, 150.13it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 208.19it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train     6: 100%|████████████████████| 2326/2326 [00:15<00:00, 149.86it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 206.76it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train     7: 100%|████████████████████| 2326/2326 [00:15<00:00, 149.78it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 213.36it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train     8: 100%|████████████████████| 2326/2326 [00:15<00:00, 150.02it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 203.13it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train     9: 100%|████████████████████| 2326/2326 [00:15<00:00, 146.94it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 207.92it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train    10: 100%|████████████████████| 2326/2326 [00:16<00:00, 145.22it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 203.17it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train    11: 100%|████████████████████| 2326/2326 [00:15<00:00, 149.28it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 213.48it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train    12: 100%|████████████████████| 2326/2326 [00:15<00:00, 149.16it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 206.45it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train    13: 100%|████████████████████| 2326/2326 [00:16<00:00, 144.99it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 206.18it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train    14: 100%|████████████████████| 2326/2326 [00:15<00:00, 147.64it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 208.57it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train    15: 100%|████████████████████| 2326/2326 [00:15<00:00, 149.43it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 208.39it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train    16: 100%|████████████████████| 2326/2326 [00:15<00:00, 148.15it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 207.93it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train    17: 100%|████████████████████| 2326/2326 [00:15<00:00, 147.71it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 203.32it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Train    18: 100%|████████████████████| 2326/2326 [00:15<00:00, 147.40it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 203.46it/s, GPU RAM: 0.05 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:02<00:00, 205.37it/s, GPU RAM: 0.06 G/39.56 G]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NeuMF-BCE Results:\n",
            "  Best Validation NDCG@10: 0.0675\n",
            "  Test HR@10: 0.1125\n",
            "  Test NDCG@10: 0.0579\n"
          ]
        }
      ],
      "source": [
        "# Skip if using saved results\n",
        "if LOAD_SAVED_RESULTS:\n",
        "    print(\"=\"*70)\n",
        "    print(f\"EXPERIMENT 1: NeuMF-BCE Baseline (1:{PRIMARY_RATIO} sampling)\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"SKIPPED - Using saved results\")\n",
        "    print(f\"  Test NDCG@10: {result_bce['test_result'].get('ndcg@10', 0):.4f}\")\n",
        "    print(f\"  Test HR@10: {result_bce['test_result'].get('hit@10', 0):.4f}\")\n",
        "else:\n",
        "    print(\"=\"*70)\n",
        "    print(f\"EXPERIMENT 1: NeuMF-BCE Baseline (1:{PRIMARY_RATIO} sampling)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    result_bce = run_recbole(\n",
        "        model='NeuMF',\n",
        "        dataset=DATASET,\n",
        "        config_dict=configs[PRIMARY_RATIO]\n",
        "    )\n",
        "\n",
        "    print(\"\\nNeuMF-BCE Results:\")\n",
        "    print(f\"  Best Validation NDCG@10: {result_bce['best_valid_score']:.4f}\")\n",
        "    print(f\"  Test HR@10: {result_bce['test_result'].get('hit@10', 0):.4f}\")\n",
        "    print(f\"  Test NDCG@10: {result_bce['test_result'].get('ndcg@10', 0):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {
        "id": "cell-17"
      },
      "source": [
        "## Experiment 2: NeuMF with Alpha-Balanced BCE (1:10 Sampling)\n",
        "\n",
        "**NEW CONTROL EXPERIMENT**: Train NeuMF with alpha-balanced BCE (Focal Loss with gamma=0).\n",
        "\n",
        "This isolates the **focusing effect** (gamma > 0) from simple **class weighting** (alpha).\n",
        "\n",
        "- If Alpha-BCE ≈ FL: Improvement comes mainly from class weighting (H3 false)\n",
        "- If Alpha-BCE < FL: Focusing effect is necessary (H3 true)\n",
        "\n",
        "We use alpha=0.25 (same as Focal Loss) for direct comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cell-18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-18",
        "outputId": "393e4e1f-954b-4df5-a8ae-e21ae9bfd655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EXPERIMENT 2: NeuMF-AlphaBCE Control (1:10 sampling)\n",
            "======================================================================\n",
            "Alpha-Balanced BCE: gamma=0, alpha=0.25\n",
            "This isolates class weighting from the focusing effect.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized NeuMF with Alpha-Balanced BCE (alpha=0.25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NeuMF-AlphaBCE Results:\n",
            "  Best Validation NDCG@10: 0.0682\n",
            "  Test HR@10: 0.1231\n",
            "  Test NDCG@10: 0.0598\n"
          ]
        }
      ],
      "source": [
        "# Skip if using saved results\n",
        "if LOAD_SAVED_RESULTS:\n",
        "    print(\"=\"*70)\n",
        "    print(f\"EXPERIMENT 2: NeuMF-AlphaBCE Control (1:{PRIMARY_RATIO} sampling)\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"SKIPPED - Using saved results\")\n",
        "    print(f\"  Test NDCG@10: {result_alpha_bce['test_result'].get('ndcg@10', 0):.4f}\")\n",
        "    print(f\"  Test HR@10: {result_alpha_bce['test_result'].get('hit@10', 0):.4f}\")\n",
        "else:\n",
        "    print(\"=\"*70)\n",
        "    print(f\"EXPERIMENT 2: NeuMF-AlphaBCE Control (1:{PRIMARY_RATIO} sampling)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Alpha-Balanced BCE: gamma=0, alpha={ALPHA}\")\n",
        "    print(\"This isolates class weighting from the focusing effect.\\n\")\n",
        "\n",
        "    result_alpha_bce = train_neumf_alpha_bce(\n",
        "        config_dict=configs[PRIMARY_RATIO],\n",
        "        dataset=DATASET,\n",
        "        alpha=ALPHA,\n",
        "        seed=42,\n",
        "        track_dynamics=False\n",
        "    )\n",
        "\n",
        "    print(\"\\nNeuMF-AlphaBCE Results:\")\n",
        "    print(f\"  Best Validation NDCG@10: {result_alpha_bce['best_valid_score']:.4f}\")\n",
        "    print(f\"  Test HR@10: {result_alpha_bce['test_result'].get('hit@10', 0):.4f}\")\n",
        "    print(f\"  Test NDCG@10: {result_alpha_bce['test_result'].get('ndcg@10', 0):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-19",
      "metadata": {
        "id": "cell-19"
      },
      "source": [
        "## Experiment 3: NeuMF with Focal Loss (1:10 Sampling)\n",
        "\n",
        "Train NeuMF with Focal Loss (gamma=2.0, alpha=0.25).\n",
        "\n",
        "This is the main proposed method for H1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cell-20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-20",
        "outputId": "f49453a2-7203-45c6-c208-650738ed635d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EXPERIMENT 3: NeuMF-FocalLoss (1:10 sampling)\n",
            "======================================================================\n",
            "Focal Loss: gamma=2.0, alpha=0.25\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized NeuMF with Focal Loss (gamma=2.0, alpha=0.25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NeuMF-FocalLoss Results:\n",
            "  Best Validation NDCG@10: 0.0637\n",
            "  Test HR@10: 0.1115\n",
            "  Test NDCG@10: 0.0564\n"
          ]
        }
      ],
      "source": [
        "# Skip if using saved results\n",
        "if LOAD_SAVED_RESULTS:\n",
        "    print(\"=\"*70)\n",
        "    print(f\"EXPERIMENT 3: NeuMF-FocalLoss (1:{PRIMARY_RATIO} sampling)\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"SKIPPED - Using saved results\")\n",
        "    print(f\"  Test NDCG@10: {result_focal['test_result'].get('ndcg@10', 0):.4f}\")\n",
        "    print(f\"  Test HR@10: {result_focal['test_result'].get('hit@10', 0):.4f}\")\n",
        "else:\n",
        "    print(\"=\"*70)\n",
        "    print(f\"EXPERIMENT 3: NeuMF-FocalLoss (1:{PRIMARY_RATIO} sampling)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Focal Loss: gamma={GAMMA}, alpha={ALPHA}\\n\")\n",
        "\n",
        "    result_focal = train_neumf_focal_loss(\n",
        "        config_dict=configs[PRIMARY_RATIO],\n",
        "        dataset=DATASET,\n",
        "        gamma=GAMMA,\n",
        "        alpha=ALPHA,\n",
        "        seed=42,\n",
        "        track_dynamics=False\n",
        "    )\n",
        "\n",
        "    print(\"\\nNeuMF-FocalLoss Results:\")\n",
        "    print(f\"  Best Validation NDCG@10: {result_focal['best_valid_score']:.4f}\")\n",
        "    print(f\"  Test HR@10: {result_focal['test_result'].get('hit@10', 0):.4f}\")\n",
        "    print(f\"  Test NDCG@10: {result_focal['test_result'].get('ndcg@10', 0):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-21",
      "metadata": {
        "id": "cell-21"
      },
      "source": [
        "## Hypothesis Testing: Primary Results (1:10 Sampling)\n",
        "\n",
        "Compare BCE vs Alpha-BCE vs Focal Loss to test H1 and H3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cell-22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-22",
        "outputId": "17c1286b-c4d2-4ee0-acb6-ace20efaf7c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "HYPOTHESIS TESTING: Primary Results (1:10 sampling)\n",
            "======================================================================\n",
            "\n",
            " Metric    BCE AlphaBCE FocalLoss\n",
            "  HIT@5 0.0616   0.0669    0.0701\n",
            " HIT@10 0.1125   0.1231    0.1115\n",
            " HIT@20 0.1794   0.1921    0.1921\n",
            " NDCG@5 0.0415   0.0416    0.0433\n",
            "NDCG@10 0.0579   0.0598    0.0564\n",
            "NDCG@20 0.0746   0.0770    0.0767\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "IMPROVEMENT ANALYSIS\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "H1: Focal Loss vs BCE (Efficacy Hypothesis)\n",
            "  NDCG@10: 0.0579 -> 0.0564 (-2.59%)\n",
            "  HIT@10: 0.1125 -> 0.1115 (-0.89%)\n",
            "\n",
            "H3: Focal Loss vs Alpha-BCE (Mechanism - Focusing Effect)\n",
            "  NDCG@10: 0.0598 -> 0.0564 (-5.69%)\n",
            "  HIT@10: 0.1231 -> 0.1115 (-9.42%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "INTERPRETATION\n",
            "----------------------------------------------------------------------\n",
            "✗ H1 NOT SUPPORTED: Focal Loss does not improve over BCE\n",
            "✗ H3 NOT SUPPORTED: Class weighting alone (Alpha-BCE) is sufficient\n",
            "  Note: Alpha-BCE also improves over BCE, suggesting class weighting helps\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(f\"HYPOTHESIS TESTING: Primary Results (1:{PRIMARY_RATIO} sampling)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create comparison table\n",
        "comparison_df = create_comparison_table(\n",
        "    results_list=[result_bce, result_alpha_bce, result_focal],\n",
        "    model_names=['BCE', 'AlphaBCE', 'FocalLoss']\n",
        ")\n",
        "print(\"\\n\" + comparison_df.to_string(index=False))\n",
        "\n",
        "# Compute improvements\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"IMPROVEMENT ANALYSIS\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# H1: Focal Loss vs BCE\n",
        "print(\"\\nH1: Focal Loss vs BCE (Efficacy Hypothesis)\")\n",
        "improvements_fl_vs_bce = compute_improvement(result_bce, result_focal)\n",
        "for metric in ['ndcg@10', 'hit@10']:\n",
        "    imp = improvements_fl_vs_bce[metric]\n",
        "    print(f\"  {metric.upper()}: {imp['baseline']:.4f} -> {imp['comparison']:.4f} ({imp['pct_change']:+.2f}%)\")\n",
        "\n",
        "# H3: Focal Loss vs Alpha-BCE (Mechanism Hypothesis)\n",
        "print(\"\\nH3: Focal Loss vs Alpha-BCE (Mechanism - Focusing Effect)\")\n",
        "improvements_fl_vs_alpha = compute_improvement(result_alpha_bce, result_focal)\n",
        "for metric in ['ndcg@10', 'hit@10']:\n",
        "    imp = improvements_fl_vs_alpha[metric]\n",
        "    print(f\"  {metric.upper()}: {imp['baseline']:.4f} -> {imp['comparison']:.4f} ({imp['pct_change']:+.2f}%)\")\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"INTERPRETATION\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "ndcg_fl = result_focal['test_result'].get('ndcg@10', 0)\n",
        "ndcg_bce = result_bce['test_result'].get('ndcg@10', 0)\n",
        "ndcg_alpha = result_alpha_bce['test_result'].get('ndcg@10', 0)\n",
        "\n",
        "if ndcg_fl > ndcg_bce:\n",
        "    print(\"✓ H1 SUPPORTED: Focal Loss improves over BCE\")\n",
        "else:\n",
        "    print(\"✗ H1 NOT SUPPORTED: Focal Loss does not improve over BCE\")\n",
        "\n",
        "if ndcg_fl > ndcg_alpha:\n",
        "    print(\"✓ H3 SUPPORTED: Focusing effect (gamma > 0) is necessary beyond class weighting\")\n",
        "else:\n",
        "    print(\"✗ H3 NOT SUPPORTED: Class weighting alone (Alpha-BCE) is sufficient\")\n",
        "\n",
        "if ndcg_alpha > ndcg_bce:\n",
        "    print(\"  Note: Alpha-BCE also improves over BCE, suggesting class weighting helps\")\n",
        "else:\n",
        "    print(\"  Note: Alpha-BCE does not improve over BCE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-23",
      "metadata": {
        "id": "cell-23"
      },
      "source": [
        "## Robustness Study: Multiple Sampling Ratios (H2)\n",
        "\n",
        "Test whether Focal Loss improvements are robust across different sampling ratios.\n",
        "\n",
        "We compare BCE vs Focal Loss at 1:4, 1:10, and 1:50 sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cell-24",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-24",
        "outputId": "20a1fab7-0eb8-4ab3-d7fa-66d88dfc7da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "H2: ROBUSTNESS STUDY - Multiple Sampling Ratios\n",
            "======================================================================\n",
            "Testing BCE vs Focal Loss at 1:4, 1:10, and 1:50 sampling\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Sampling Ratio: 1:4\n",
            "======================================================================\n",
            "\n",
            "Training NeuMF-BCE (1:4)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n",
            "Train     0:   0%|                                                         | 0/1049 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n",
            "Train     0: 100%|████████████████████| 1049/1049 [00:09<00:00, 110.93it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 138.75it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train     1: 100%|████████████████████| 1049/1049 [00:09<00:00, 110.41it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 140.89it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train     2: 100%|████████████████████| 1049/1049 [00:09<00:00, 111.93it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 144.41it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train     3: 100%|████████████████████| 1049/1049 [00:09<00:00, 110.52it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 144.70it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train     4: 100%|████████████████████| 1049/1049 [00:09<00:00, 110.95it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 147.19it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train     5: 100%|████████████████████| 1049/1049 [00:09<00:00, 112.00it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 144.49it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train     6: 100%|████████████████████| 1049/1049 [00:09<00:00, 110.24it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 133.26it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train     7: 100%|████████████████████| 1049/1049 [00:09<00:00, 111.07it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 145.34it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train     8: 100%|████████████████████| 1049/1049 [00:09<00:00, 109.75it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 141.45it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train     9: 100%|████████████████████| 1049/1049 [00:09<00:00, 111.56it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 141.33it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train    10: 100%|████████████████████| 1049/1049 [00:09<00:00, 112.61it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 142.31it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train    11: 100%|████████████████████| 1049/1049 [00:09<00:00, 113.01it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 140.42it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train    12: 100%|████████████████████| 1049/1049 [00:09<00:00, 108.44it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 141.37it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train    13: 100%|████████████████████| 1049/1049 [00:09<00:00, 109.59it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 144.09it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train    14: 100%|████████████████████| 1049/1049 [00:09<00:00, 109.92it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 141.55it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train    15: 100%|████████████████████| 1049/1049 [00:09<00:00, 109.50it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 142.81it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Train    16: 100%|████████████████████| 1049/1049 [00:09<00:00, 109.07it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 141.92it/s, GPU RAM: 0.06 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 140.89it/s, GPU RAM: 0.07 G/39.56 G]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training NeuMF-FocalLoss (1:4)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized NeuMF with Focal Loss (gamma=2.0, alpha=0.25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for 1:4 sampling:\n",
            "  BCE NDCG@10: 0.0640\n",
            "  Focal NDCG@10: 0.0604\n",
            "  Improvement: -5.62%\n",
            "\n",
            "======================================================================\n",
            "Sampling Ratio: 1:10\n",
            "======================================================================\n",
            "Using existing results from primary experiment\n",
            "\n",
            "======================================================================\n",
            "Sampling Ratio: 1:50\n",
            "======================================================================\n",
            "\n",
            "Training NeuMF-BCE (1:50)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n",
            "Train     0:   0%|                                                        | 0/10699 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n",
            "Train     0: 100%|███████████████████| 10699/10699 [01:51<00:00, 95.55it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:04<00:00, 117.68it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train     1: 100%|███████████████████| 10699/10699 [01:52<00:00, 95.29it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 119.96it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train     2: 100%|███████████████████| 10699/10699 [01:52<00:00, 95.52it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 117.82it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train     3: 100%|███████████████████| 10699/10699 [01:51<00:00, 95.63it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 118.65it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train     4: 100%|███████████████████| 10699/10699 [01:51<00:00, 95.60it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 119.41it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train     5: 100%|███████████████████| 10699/10699 [01:51<00:00, 96.09it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 120.82it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train     6: 100%|███████████████████| 10699/10699 [01:51<00:00, 96.21it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:04<00:00, 117.71it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train     7: 100%|███████████████████| 10699/10699 [01:51<00:00, 96.02it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 118.51it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train     8: 100%|███████████████████| 10699/10699 [01:52<00:00, 95.49it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 118.47it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train     9: 100%|███████████████████| 10699/10699 [01:52<00:00, 94.69it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:04<00:00, 113.97it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train    10: 100%|███████████████████| 10699/10699 [01:51<00:00, 95.93it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:04<00:00, 117.24it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train    11: 100%|███████████████████| 10699/10699 [01:51<00:00, 96.07it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 118.80it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Train    12: 100%|███████████████████| 10699/10699 [01:51<00:00, 95.80it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 118.55it/s, GPU RAM: 0.07 G/39.56 G]\n",
            "Evaluate   : 100%|██████████████████████| 471/471 [00:03<00:00, 120.33it/s, GPU RAM: 0.07 G/39.56 G]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training NeuMF-FocalLoss (1:50)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized NeuMF with Focal Loss (gamma=2.0, alpha=0.25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for 1:50 sampling:\n",
            "  BCE NDCG@10: 0.0530\n",
            "  Focal NDCG@10: 0.0672\n",
            "  Improvement: +26.79%\n"
          ]
        }
      ],
      "source": [
        "# Skip if using saved results\n",
        "if LOAD_SAVED_RESULTS:\n",
        "    print(\"=\"*70)\n",
        "    print(\"H2: ROBUSTNESS STUDY - Multiple Sampling Ratios\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"SKIPPED - Using saved results\\n\")\n",
        "\n",
        "    # Show saved results summary\n",
        "    for ratio in SAMPLING_RATIOS:\n",
        "        bce_ndcg = robustness_results['bce'][ratio]['test_result'].get('ndcg@10', 0)\n",
        "        fl_ndcg = robustness_results['focal'][ratio]['test_result'].get('ndcg@10', 0)\n",
        "        imp = (fl_ndcg - bce_ndcg) / bce_ndcg * 100 if bce_ndcg > 0 else 0\n",
        "        print(f\"1:{ratio}: BCE={bce_ndcg:.4f}, FL={fl_ndcg:.4f} ({imp:+.1f}%)\")\n",
        "else:\n",
        "    print(\"=\"*70)\n",
        "    print(\"H2: ROBUSTNESS STUDY - Multiple Sampling Ratios\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Testing BCE vs Focal Loss at 1:4, 1:10, and 1:50 sampling\\n\")\n",
        "\n",
        "    robustness_results = {\n",
        "        'bce': {},\n",
        "        'focal': {}\n",
        "    }\n",
        "\n",
        "    for ratio in SAMPLING_RATIOS:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Sampling Ratio: 1:{ratio}\")\n",
        "        print('='*70)\n",
        "\n",
        "        # Skip if we already trained at PRIMARY_RATIO\n",
        "        if ratio == PRIMARY_RATIO:\n",
        "            print(f\"Using existing results from primary experiment\")\n",
        "            robustness_results['bce'][ratio] = result_bce\n",
        "            robustness_results['focal'][ratio] = result_focal\n",
        "            continue\n",
        "\n",
        "        # Train BCE\n",
        "        print(f\"\\nTraining NeuMF-BCE (1:{ratio})...\")\n",
        "        result_bce_ratio = run_recbole(\n",
        "            model='NeuMF',\n",
        "            dataset=DATASET,\n",
        "            config_dict=configs[ratio]\n",
        "        )\n",
        "        robustness_results['bce'][ratio] = result_bce_ratio\n",
        "\n",
        "        # Train Focal Loss\n",
        "        print(f\"Training NeuMF-FocalLoss (1:{ratio})...\")\n",
        "        result_focal_ratio = train_neumf_focal_loss(\n",
        "            config_dict=configs[ratio],\n",
        "            dataset=DATASET,\n",
        "            gamma=GAMMA,\n",
        "            alpha=ALPHA,\n",
        "            seed=42,\n",
        "            track_dynamics=False\n",
        "        )\n",
        "        robustness_results['focal'][ratio] = result_focal_ratio\n",
        "\n",
        "        # Show results\n",
        "        bce_ndcg = result_bce_ratio['test_result'].get('ndcg@10', 0)\n",
        "        fl_ndcg = result_focal_ratio['test_result'].get('ndcg@10', 0)\n",
        "        improvement = (fl_ndcg - bce_ndcg) / bce_ndcg * 100 if bce_ndcg > 0 else 0\n",
        "\n",
        "        print(f\"\\nResults for 1:{ratio} sampling:\")\n",
        "        print(f\"  BCE NDCG@10: {bce_ndcg:.4f}\")\n",
        "        print(f\"  Focal NDCG@10: {fl_ndcg:.4f}\")\n",
        "        print(f\"  Improvement: {improvement:+.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-25",
      "metadata": {
        "id": "cell-25"
      },
      "source": [
        "## Robustness Study: Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cell-26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-26",
        "outputId": "e3f4cd73-0bfc-488e-9578-ca3d2106ac2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ROBUSTNESS STUDY SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Sampling BCE_NDCG@10 FL_NDCG@10 NDCG_Improvement BCE_HR@10 FL_HR@10 HR_Improvement\n",
            "     1:4      0.0640     0.0604           -5.62%    0.1263   0.1242         -1.66%\n",
            "    1:10      0.0579     0.0564           -2.59%    0.1125   0.1115         -0.89%\n",
            "    1:50      0.0530     0.0672          +26.79%    0.1083   0.1231        +13.67%\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "H2 INTERPRETATION (Robustness)\n",
            "----------------------------------------------------------------------\n",
            "✗ H2 NOT SUPPORTED: Focal Loss improvements are not robust\n",
            "\n",
            "Focal Loss wins at 1/3 sampling ratios\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ROBUSTNESS STUDY SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create summary table\n",
        "summary_data = []\n",
        "for ratio in SAMPLING_RATIOS:\n",
        "    bce_result = robustness_results['bce'][ratio]\n",
        "    fl_result = robustness_results['focal'][ratio]\n",
        "\n",
        "    bce_ndcg = bce_result['test_result'].get('ndcg@10', 0)\n",
        "    fl_ndcg = fl_result['test_result'].get('ndcg@10', 0)\n",
        "    bce_hr = bce_result['test_result'].get('hit@10', 0)\n",
        "    fl_hr = fl_result['test_result'].get('hit@10', 0)\n",
        "\n",
        "    ndcg_imp = (fl_ndcg - bce_ndcg) / bce_ndcg * 100 if bce_ndcg > 0 else 0\n",
        "    hr_imp = (fl_hr - bce_hr) / bce_hr * 100 if bce_hr > 0 else 0\n",
        "\n",
        "    summary_data.append({\n",
        "        'Sampling': f'1:{ratio}',\n",
        "        'BCE_NDCG@10': f'{bce_ndcg:.4f}',\n",
        "        'FL_NDCG@10': f'{fl_ndcg:.4f}',\n",
        "        'NDCG_Improvement': f'{ndcg_imp:+.2f}%',\n",
        "        'BCE_HR@10': f'{bce_hr:.4f}',\n",
        "        'FL_HR@10': f'{fl_hr:.4f}',\n",
        "        'HR_Improvement': f'{hr_imp:+.2f}%'\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\n\" + summary_df.to_string(index=False))\n",
        "\n",
        "# H2 interpretation\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"H2 INTERPRETATION (Robustness)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "improvements = []\n",
        "for ratio in SAMPLING_RATIOS:\n",
        "    bce_ndcg = robustness_results['bce'][ratio]['test_result'].get('ndcg@10', 0)\n",
        "    fl_ndcg = robustness_results['focal'][ratio]['test_result'].get('ndcg@10', 0)\n",
        "    improvements.append(fl_ndcg > bce_ndcg)\n",
        "\n",
        "if all(improvements):\n",
        "    print(\"✓ H2 STRONGLY SUPPORTED: Focal Loss improves over BCE at ALL sampling ratios\")\n",
        "elif sum(improvements) >= 2:\n",
        "    print(\"✓ H2 PARTIALLY SUPPORTED: Focal Loss improves at most sampling ratios\")\n",
        "else:\n",
        "    print(\"✗ H2 NOT SUPPORTED: Focal Loss improvements are not robust\")\n",
        "\n",
        "print(f\"\\nFocal Loss wins at {sum(improvements)}/{len(SAMPLING_RATIOS)} sampling ratios\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-27",
      "metadata": {
        "id": "cell-27"
      },
      "source": [
        "## Training Dynamics Analysis\n",
        "\n",
        "**NEW ANALYSIS**: Track training dynamics to validate the Focal Loss mechanism.\n",
        "\n",
        "We track loss contribution by confidence bin to verify:\n",
        "1. Focal Loss down-weights easy examples (high confidence)\n",
        "2. Focal Loss emphasizes hard examples (low confidence)\n",
        "3. This leads to better focusing on difficult instances\n",
        "\n",
        "This provides direct evidence for the claimed mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cell-28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-28",
        "outputId": "5e0daee0-8f45-49e2-c645-44b6d40dd746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TRAINING DYNAMICS ANALYSIS\n",
            "======================================================================\n",
            "Re-training models with dynamics tracking enabled...\n",
            "\n",
            "Training BCE with dynamics tracking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized NeuMF with Alpha-Balanced BCE (alpha=0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Focal Loss with dynamics tracking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized NeuMF with Focal Loss (gamma=2.0, alpha=0.25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete. Analyzing dynamics...\n"
          ]
        }
      ],
      "source": [
        "# Skip if using saved results (dynamics data not saved)\n",
        "if LOAD_SAVED_RESULTS:\n",
        "    print(\"=\"*70)\n",
        "    print(\"TRAINING DYNAMICS ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"SKIPPED - Dynamics tracking requires fresh training\")\n",
        "    print(\"Set LOAD_SAVED_RESULTS = False to run dynamics analysis\")\n",
        "    bce_dynamics = {'dynamics': None}\n",
        "    focal_dynamics = {'dynamics': None}\n",
        "else:\n",
        "    print(\"=\"*70)\n",
        "    print(\"TRAINING DYNAMICS ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Re-training models with dynamics tracking enabled...\\n\")\n",
        "\n",
        "    # Train BCE with dynamics tracking\n",
        "    print(\"Training BCE with dynamics tracking...\")\n",
        "    # Note: Standard BCE from RecBole doesn't support dynamics tracking\n",
        "    # We approximate by using Alpha-BCE with alpha=0.5\n",
        "    bce_dynamics = train_neumf_alpha_bce(\n",
        "        config_dict=configs[PRIMARY_RATIO],\n",
        "        dataset=DATASET,\n",
        "        alpha=0.5,  # Standard BCE\n",
        "        seed=42,\n",
        "        track_dynamics=True\n",
        "    )\n",
        "\n",
        "    # Train Focal Loss with dynamics tracking\n",
        "    print(\"\\nTraining Focal Loss with dynamics tracking...\")\n",
        "    focal_dynamics = train_neumf_focal_loss(\n",
        "        config_dict=configs[PRIMARY_RATIO],\n",
        "        dataset=DATASET,\n",
        "        gamma=GAMMA,\n",
        "        alpha=ALPHA,\n",
        "        seed=42,\n",
        "        track_dynamics=True\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining complete. Analyzing dynamics...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-29",
      "metadata": {
        "id": "cell-29"
      },
      "source": [
        "## Training Dynamics: Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cell-30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-30",
        "outputId": "eca85943-82e2-4417-d86c-becaa86a9699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TRAINING DYNAMICS: Loss by Confidence Bin\n",
            "======================================================================\n",
            "Dynamics tracking did not record data. Check model implementation.\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TRAINING DYNAMICS: Loss by Confidence Bin\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if 'dynamics' in bce_dynamics and 'dynamics' in focal_dynamics:\n",
        "    bce_dyn_df = bce_dynamics['dynamics']\n",
        "    focal_dyn_df = focal_dynamics['dynamics']\n",
        "\n",
        "    # Show final epoch dynamics\n",
        "    if len(bce_dyn_df) > 0 and len(focal_dyn_df) > 0:\n",
        "        print(\"\\nBCE - Final Epoch Loss by Confidence Bin:\")\n",
        "        print(bce_dyn_df.tail(1).to_string(index=False))\n",
        "\n",
        "        print(\"\\nFocal Loss - Final Epoch Loss by Confidence Bin:\")\n",
        "        print(focal_dyn_df.tail(1).to_string(index=False))\n",
        "\n",
        "        print(\"\\n\" + \"-\"*70)\n",
        "        print(\"INTERPRETATION\")\n",
        "        print(\"-\"*70)\n",
        "        print(\"Expected pattern for Focal Loss:\")\n",
        "        print(\"  - LOW loss in high confidence bins [0.8,1.0) - easy examples down-weighted\")\n",
        "        print(\"  - HIGH loss in low confidence bins [0.0,0.4) - hard examples emphasized\")\n",
        "        print(\"\\nThis validates the focusing mechanism.\")\n",
        "    else:\n",
        "        print(\"Dynamics tracking did not record data. Check model implementation.\")\n",
        "else:\n",
        "    print(\"Dynamics tracking not available. Models may not support this feature.\")\n",
        "    print(\"This is optional analysis - main results are still valid.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-31",
      "metadata": {
        "id": "cell-31"
      },
      "source": [
        "## Results Summary: Full Comparison\n",
        "\n",
        "Complete results across all experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cell-32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-32",
        "outputId": "7c4c3731-abb1-4483-ac48-da64e7cb6318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "FINAL RESULTS SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Dataset: ml-100k\n",
            "Primary Experiment: 1:10 sampling\n",
            "Focal Loss: gamma=2.0, alpha=0.25\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "PRIMARY RESULTS (1:10 sampling)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " Metric    BCE AlphaBCE FocalLoss\n",
            "  HIT@5 0.0616   0.0669    0.0701\n",
            " HIT@10 0.1125   0.1231    0.1115\n",
            " HIT@20 0.1794   0.1921    0.1921\n",
            " NDCG@5 0.0415   0.0416    0.0433\n",
            "NDCG@10 0.0579   0.0598    0.0564\n",
            "NDCG@20 0.0746   0.0770    0.0767\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "ROBUSTNESS ACROSS SAMPLING RATIOS\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Sampling BCE_NDCG@10 FL_NDCG@10 NDCG_Improvement BCE_HR@10 FL_HR@10 HR_Improvement\n",
            "     1:4      0.0640     0.0604           -5.62%    0.1263   0.1242         -1.66%\n",
            "    1:10      0.0579     0.0564           -2.59%    0.1125   0.1115         -0.89%\n",
            "    1:50      0.0530     0.0672          +26.79%    0.1083   0.1231        +13.67%\n",
            "\n",
            "======================================================================\n",
            "HYPOTHESIS TESTING CONCLUSIONS\n",
            "======================================================================\n",
            "\n",
            "H1 (Efficacy): Focal Loss improves NeuMF over BCE\n",
            "  ✗ NOT SUPPORTED: -2.59% change in NDCG@10\n",
            "\n",
            "H2 (Robustness): Improvements are robust across sampling ratios\n",
            "  ✗ NOT SUPPORTED: FL wins at only 1/3 ratios\n",
            "\n",
            "H3 (Mechanism): Focusing effect (gamma > 0) is necessary beyond class weighting\n",
            "  ✗ NOT SUPPORTED: Alpha-BCE is sufficient (-5.69%)\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nDataset: {DATASET}\")\n",
        "print(f\"Primary Experiment: 1:{PRIMARY_RATIO} sampling\")\n",
        "print(f\"Focal Loss: gamma={GAMMA}, alpha={ALPHA}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(f\"PRIMARY RESULTS (1:{PRIMARY_RATIO} sampling)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "primary_comparison = create_comparison_table(\n",
        "    results_list=[result_bce, result_alpha_bce, result_focal],\n",
        "    model_names=['BCE', 'AlphaBCE', 'FocalLoss']\n",
        ")\n",
        "print(\"\\n\" + primary_comparison.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"ROBUSTNESS ACROSS SAMPLING RATIOS\")\n",
        "print(\"-\"*70)\n",
        "print(\"\\n\" + summary_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HYPOTHESIS TESTING CONCLUSIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# H1\n",
        "ndcg_fl = result_focal['test_result'].get('ndcg@10', 0)\n",
        "ndcg_bce = result_bce['test_result'].get('ndcg@10', 0)\n",
        "h1_imp = (ndcg_fl - ndcg_bce) / ndcg_bce * 100 if ndcg_bce > 0 else 0\n",
        "\n",
        "print(\"\\nH1 (Efficacy): Focal Loss improves NeuMF over BCE\")\n",
        "if ndcg_fl > ndcg_bce:\n",
        "    print(f\"  ✓ SUPPORTED: {h1_imp:+.2f}% improvement in NDCG@10\")\n",
        "else:\n",
        "    print(f\"  ✗ NOT SUPPORTED: {h1_imp:+.2f}% change in NDCG@10\")\n",
        "\n",
        "# H2\n",
        "robust_count = sum([robustness_results['focal'][r]['test_result'].get('ndcg@10', 0) >\n",
        "                    robustness_results['bce'][r]['test_result'].get('ndcg@10', 0)\n",
        "                    for r in SAMPLING_RATIOS])\n",
        "\n",
        "print(f\"\\nH2 (Robustness): Improvements are robust across sampling ratios\")\n",
        "if robust_count == len(SAMPLING_RATIOS):\n",
        "    print(f\"  ✓ STRONGLY SUPPORTED: FL wins at {robust_count}/{len(SAMPLING_RATIOS)} ratios\")\n",
        "elif robust_count >= 2:\n",
        "    print(f\"  ✓ PARTIALLY SUPPORTED: FL wins at {robust_count}/{len(SAMPLING_RATIOS)} ratios\")\n",
        "else:\n",
        "    print(f\"  ✗ NOT SUPPORTED: FL wins at only {robust_count}/{len(SAMPLING_RATIOS)} ratios\")\n",
        "\n",
        "# H3\n",
        "ndcg_alpha = result_alpha_bce['test_result'].get('ndcg@10', 0)\n",
        "h3_imp = (ndcg_fl - ndcg_alpha) / ndcg_alpha * 100 if ndcg_alpha > 0 else 0\n",
        "\n",
        "print(f\"\\nH3 (Mechanism): Focusing effect (gamma > 0) is necessary beyond class weighting\")\n",
        "if ndcg_fl > ndcg_alpha:\n",
        "    print(f\"  ✓ SUPPORTED: FL gains {h3_imp:+.2f}% over Alpha-BCE\")\n",
        "else:\n",
        "    print(f\"  ✗ NOT SUPPORTED: Alpha-BCE is sufficient ({h3_imp:+.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-33",
      "metadata": {
        "id": "cell-33"
      },
      "source": [
        "## Optional: Grid Search for Hyperparameter Tuning\n",
        "\n",
        "**IMPORTANT**: This is computationally expensive. Only run if you want to optimize gamma and alpha.\n",
        "\n",
        "The grid search tests:\n",
        "- Multiple gamma values: [0.5, 1.0, 2.0, 3.0]\n",
        "- Multiple alpha values: [0.25, 0.5, 0.75]\n",
        "- Multiple sampling ratios: [4, 10, 50]\n",
        "\n",
        "**Uncomment to run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-34",
        "outputId": "3891d902-9e81-45b1-d2e3-88736e84892d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "GRID SEARCH: Hyperparameter Tuning\n",
            "======================================================================\n",
            "Testing 4 gamma x 3 alpha x 3 ratios\n",
            "Total: 36 experiments\n",
            "\n",
            "\n",
            "Sampling Ratio: 1:4\n",
            "----------------------------------------------------------------------\n",
            "Training: gamma=0.5, alpha=0.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized NeuMF with Focal Loss (gamma=0.5, alpha=0.25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NDCG@10: 0.0636, HR@10: 0.1253\n",
            "Training: gamma=0.5, alpha=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized NeuMF with Focal Loss (gamma=0.5, alpha=0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NDCG@10: 0.0708, HR@10: 0.1316\n",
            "Training: gamma=0.5, alpha=0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-f10014c7-055d-4c47-bdca-346decb65770.json] will not be used in RecBole\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
            "WARNING:root:All the same value in [label] from [       user_id  item_id  timestamp  label\n",
            "0            1        1   0.509543      1\n",
            "1            2        2   0.910668      1\n",
            "2            3        3   0.272408      1\n",
            "3            4        4   0.070986      1\n",
            "4            5        5   0.244896      1\n",
            "...        ...      ...        ...    ...\n",
            "55370      424       54   0.943686      1\n",
            "55371      486      856   0.749534      1\n",
            "55372      802      334   0.412898      1\n",
            "55373      663      854   0.967611      1\n",
            "55374      701      152   0.273185      1\n",
            "\n",
            "[55375 rows x 4 columns]_feat].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized NeuMF with Focal Loss (gamma=0.5, alpha=0.75)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# GRID SEARCH (OPTIONAL - COMPUTATIONALLY EXPENSIVE)\n",
        "# ============================================\n",
        "\n",
        "GAMMA_VALUES = [0.5, 1.0, 2.0, 3.0]\n",
        "ALPHA_VALUES = [0.25, 0.5, 0.75]\n",
        "\n",
        "# Skip if using saved results\n",
        "if LOAD_SAVED_RESULTS:\n",
        "    print(\"=\"*70)\n",
        "    print(\"GRID SEARCH: Hyperparameter Tuning\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"SKIPPED - Using saved results\")\n",
        "    print(f\"Loaded {len(grid_results)} pre-computed configurations\\n\")\n",
        "\n",
        "    # Convert to DataFrame for display\n",
        "    grid_df = pd.DataFrame(grid_results)\n",
        "\n",
        "    # Show best per ratio\n",
        "    print(\"Best configurations per sampling ratio:\")\n",
        "    for ratio in SAMPLING_RATIOS:\n",
        "        ratio_results = [r for r in grid_results if r['ratio'] == ratio]\n",
        "        best = max(ratio_results, key=lambda x: x['ndcg@10'])\n",
        "        print(f\"  1:{ratio}: gamma={best['gamma']}, alpha={best['alpha']} -> NDCG@10={best['ndcg@10']:.4f}\")\n",
        "else:\n",
        "    print(\"=\"*70)\n",
        "    print(\"GRID SEARCH: Hyperparameter Tuning\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Testing {len(GAMMA_VALUES)} gamma x {len(ALPHA_VALUES)} alpha x {len(SAMPLING_RATIOS)} ratios\")\n",
        "    print(f\"Total: {len(GAMMA_VALUES) * len(ALPHA_VALUES) * len(SAMPLING_RATIOS)} experiments\\n\")\n",
        "\n",
        "    grid_results = []\n",
        "\n",
        "    for ratio in SAMPLING_RATIOS:\n",
        "        print(f\"\\nSampling Ratio: 1:{ratio}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        for gamma in GAMMA_VALUES:\n",
        "            for alpha in ALPHA_VALUES:\n",
        "                print(f\"Training: gamma={gamma}, alpha={alpha}\")\n",
        "\n",
        "                result = train_neumf_focal_loss(\n",
        "                    config_dict=configs[ratio],\n",
        "                    dataset=DATASET,\n",
        "                    gamma=gamma,\n",
        "                    alpha=alpha,\n",
        "                    seed=42,\n",
        "                    track_dynamics=False\n",
        "                )\n",
        "\n",
        "                ndcg10 = result['test_result'].get('ndcg@10', 0)\n",
        "                hr10 = result['test_result'].get('hit@10', 0)\n",
        "\n",
        "                grid_results.append({\n",
        "                    'ratio': ratio,\n",
        "                    'gamma': gamma,\n",
        "                    'alpha': alpha,\n",
        "                    'ndcg@10': ndcg10,\n",
        "                    'hit@10': hr10\n",
        "                })\n",
        "\n",
        "                print(f\"  NDCG@10: {ndcg10:.4f}, HR@10: {hr10:.4f}\")\n",
        "\n",
        "    # Show grid search results\n",
        "    grid_df = pd.DataFrame(grid_results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GRID SEARCH RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\n\" + grid_df.to_string(index=False))\n",
        "\n",
        "    # Find best configuration for each ratio\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"BEST CONFIGURATIONS PER SAMPLING RATIO\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    for ratio in SAMPLING_RATIOS:\n",
        "        ratio_results = grid_df[grid_df['ratio'] == ratio]\n",
        "        best_idx = ratio_results['ndcg@10'].idxmax()\n",
        "        best_row = ratio_results.loc[best_idx]\n",
        "\n",
        "        print(f\"\\n1:{ratio} sampling:\")\n",
        "        print(f\"  Best: gamma={best_row['gamma']}, alpha={best_row['alpha']}\")\n",
        "        print(f\"  NDCG@10: {best_row['ndcg@10']:.4f}\")\n",
        "        print(f\"  HR@10: {best_row['hit@10']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9l27axmj43a",
      "source": [
        "## Statistical Analysis: Existing Results\n",
        "\n",
        "Before running multi-seed experiments, let's analyze the statistical patterns in our existing results from the grid search. While single-seed results don't provide true statistical significance, we can:\n",
        "\n",
        "1. **Aggregate across hyperparameter configurations** - Treat different (gamma, alpha) combinations as repeated measurements\n",
        "2. **Sign test across sampling ratios** - Check consistency of FL > BCE across different conditions\n",
        "3. **Effect size estimation** - Quantify the magnitude of improvements\n",
        "\n",
        "This provides preliminary evidence before investing in full multi-seed experiments."
      ],
      "metadata": {
        "id": "9l27axmj43a"
      }
    },
    {
      "cell_type": "code",
      "id": "1tp0mi65chai",
      "source": [
        "# ============================================\n",
        "# STATISTICAL ANALYSIS OF EXISTING RESULTS\n",
        "# Using Grid Search and Robustness Study Data\n",
        "# ============================================\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STATISTICAL ANALYSIS: Existing Experimental Results\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================\n",
        "# 1. AGGREGATE GRID SEARCH RESULTS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"1. GRID SEARCH ANALYSIS\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Get BCE baselines for each sampling ratio (from robustness study)\n",
        "bce_baselines = {}\n",
        "for ratio in SAMPLING_RATIOS:\n",
        "    bce_baselines[ratio] = {\n",
        "        'ndcg@10': robustness_results['bce'][ratio]['test_result'].get('ndcg@10', 0),\n",
        "        'hit@10': robustness_results['bce'][ratio]['test_result'].get('hit@10', 0)\n",
        "    }\n",
        "    print(f\"BCE Baseline (1:{ratio}): NDCG@10={bce_baselines[ratio]['ndcg@10']:.4f}, HR@10={bce_baselines[ratio]['hit@10']:.4f}\")\n",
        "\n",
        "# Analyze grid search results\n",
        "print(\"\\nFocal Loss configurations that beat BCE baseline:\")\n",
        "fl_wins = {'ndcg@10': 0, 'hit@10': 0}\n",
        "fl_total = 0\n",
        "improvements_ndcg = []\n",
        "improvements_hr = []\n",
        "\n",
        "for row in grid_results:\n",
        "    ratio = row['ratio']\n",
        "    fl_ndcg = row['ndcg@10']\n",
        "    fl_hr = row['hit@10']\n",
        "    bce_ndcg = bce_baselines[ratio]['ndcg@10']\n",
        "    bce_hr = bce_baselines[ratio]['hit@10']\n",
        "\n",
        "    fl_total += 1\n",
        "    if fl_ndcg > bce_ndcg:\n",
        "        fl_wins['ndcg@10'] += 1\n",
        "    if fl_hr > bce_hr:\n",
        "        fl_wins['hit@10'] += 1\n",
        "\n",
        "    # Store improvements for effect size calculation\n",
        "    improvements_ndcg.append((fl_ndcg - bce_ndcg) / bce_ndcg * 100)\n",
        "    improvements_hr.append((fl_hr - bce_hr) / bce_hr * 100)\n",
        "\n",
        "print(f\"\\nFocal Loss wins (NDCG@10): {fl_wins['ndcg@10']}/{fl_total} ({fl_wins['ndcg@10']/fl_total*100:.1f}%)\")\n",
        "print(f\"Focal Loss wins (HR@10):   {fl_wins['hit@10']}/{fl_total} ({fl_wins['hit@10']/fl_total*100:.1f}%)\")\n",
        "\n",
        "# ============================================\n",
        "# 2. SIGN TEST (Non-parametric)\n",
        "# ============================================\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"2. SIGN TEST (Binomial Test)\")\n",
        "print(\"-\"*70)\n",
        "print(\"H0: P(FL > BCE) = 0.5 (Focal Loss is no better than random)\")\n",
        "print(\"H1: P(FL > BCE) > 0.5 (Focal Loss systematically improves over BCE)\")\n",
        "\n",
        "# Sign test for NDCG@10\n",
        "n_positive_ndcg = fl_wins['ndcg@10']\n",
        "n_total = fl_total\n",
        "p_value_ndcg = stats.binom_test(n_positive_ndcg, n_total, 0.5, alternative='greater')\n",
        "print(f\"\\nNDCG@10: {n_positive_ndcg}/{n_total} positive differences\")\n",
        "print(f\"  Binomial test p-value: {p_value_ndcg:.4f}\")\n",
        "print(f\"  Significant (p < 0.05): {'YES' if p_value_ndcg < 0.05 else 'NO'}\")\n",
        "\n",
        "# Sign test for HR@10\n",
        "n_positive_hr = fl_wins['hit@10']\n",
        "p_value_hr = stats.binom_test(n_positive_hr, n_total, 0.5, alternative='greater')\n",
        "print(f\"\\nHR@10: {n_positive_hr}/{n_total} positive differences\")\n",
        "print(f\"  Binomial test p-value: {p_value_hr:.4f}\")\n",
        "print(f\"  Significant (p < 0.05): {'YES' if p_value_hr < 0.05 else 'NO'}\")\n",
        "\n",
        "# ============================================\n",
        "# 3. EFFECT SIZE ANALYSIS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"3. EFFECT SIZE ANALYSIS\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "improvements_ndcg = np.array(improvements_ndcg)\n",
        "improvements_hr = np.array(improvements_hr)\n",
        "\n",
        "print(f\"\\nNDCG@10 Improvement over BCE:\")\n",
        "print(f\"  Mean:   {np.mean(improvements_ndcg):+.2f}%\")\n",
        "print(f\"  Median: {np.median(improvements_ndcg):+.2f}%\")\n",
        "print(f\"  Std:    {np.std(improvements_ndcg):.2f}%\")\n",
        "print(f\"  Range:  [{np.min(improvements_ndcg):+.2f}%, {np.max(improvements_ndcg):+.2f}%]\")\n",
        "\n",
        "print(f\"\\nHR@10 Improvement over BCE:\")\n",
        "print(f\"  Mean:   {np.mean(improvements_hr):+.2f}%\")\n",
        "print(f\"  Median: {np.median(improvements_hr):+.2f}%\")\n",
        "print(f\"  Std:    {np.std(improvements_hr):.2f}%\")\n",
        "print(f\"  Range:  [{np.min(improvements_hr):+.2f}%, {np.max(improvements_hr):+.2f}%]\")\n",
        "\n",
        "# Cohen's d effect size (treating grid configs as samples)\n",
        "cohens_d_ndcg = np.mean(improvements_ndcg) / np.std(improvements_ndcg) if np.std(improvements_ndcg) > 0 else 0\n",
        "cohens_d_hr = np.mean(improvements_hr) / np.std(improvements_hr) if np.std(improvements_hr) > 0 else 0\n",
        "\n",
        "def interpret_cohens_d(d):\n",
        "    d = abs(d)\n",
        "    if d < 0.2: return \"negligible\"\n",
        "    elif d < 0.5: return \"small\"\n",
        "    elif d < 0.8: return \"medium\"\n",
        "    else: return \"large\"\n",
        "\n",
        "print(f\"\\nCohen's d effect size:\")\n",
        "print(f\"  NDCG@10: {cohens_d_ndcg:.3f} ({interpret_cohens_d(cohens_d_ndcg)})\")\n",
        "print(f\"  HR@10:   {cohens_d_hr:.3f} ({interpret_cohens_d(cohens_d_hr)})\")\n",
        "\n",
        "# ============================================\n",
        "# 4. ONE-SAMPLE T-TEST (Improvements > 0)\n",
        "# ============================================\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"4. ONE-SAMPLE T-TEST (Mean Improvement > 0)\")\n",
        "print(\"-\"*70)\n",
        "print(\"H0: Mean improvement = 0\")\n",
        "print(\"H1: Mean improvement > 0\")\n",
        "\n",
        "t_stat_ndcg, p_value_t_ndcg = stats.ttest_1samp(improvements_ndcg, 0)\n",
        "p_value_t_ndcg_one = p_value_t_ndcg / 2 if t_stat_ndcg > 0 else 1 - p_value_t_ndcg / 2\n",
        "\n",
        "print(f\"\\nNDCG@10:\")\n",
        "print(f\"  t-statistic: {t_stat_ndcg:.3f}\")\n",
        "print(f\"  p-value (one-tailed): {p_value_t_ndcg_one:.4f}\")\n",
        "print(f\"  Significant (p < 0.05): {'YES' if p_value_t_ndcg_one < 0.05 else 'NO'}\")\n",
        "\n",
        "t_stat_hr, p_value_t_hr = stats.ttest_1samp(improvements_hr, 0)\n",
        "p_value_t_hr_one = p_value_t_hr / 2 if t_stat_hr > 0 else 1 - p_value_t_hr / 2\n",
        "\n",
        "print(f\"\\nHR@10:\")\n",
        "print(f\"  t-statistic: {t_stat_hr:.3f}\")\n",
        "print(f\"  p-value (one-tailed): {p_value_t_hr_one:.4f}\")\n",
        "print(f\"  Significant (p < 0.05): {'YES' if p_value_t_hr_one < 0.05 else 'NO'}\")\n",
        "\n",
        "# ============================================\n",
        "# 5. ROBUSTNESS ACROSS SAMPLING RATIOS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"5. ROBUSTNESS ANALYSIS BY SAMPLING RATIO\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for ratio in SAMPLING_RATIOS:\n",
        "    ratio_results = [r for r in grid_results if r['ratio'] == ratio]\n",
        "    ratio_improvements = [(r['ndcg@10'] - bce_baselines[ratio]['ndcg@10']) / bce_baselines[ratio]['ndcg@10'] * 100\n",
        "                          for r in ratio_results]\n",
        "    wins = sum(1 for imp in ratio_improvements if imp > 0)\n",
        "\n",
        "    print(f\"\\n1:{ratio} sampling:\")\n",
        "    print(f\"  FL wins: {wins}/{len(ratio_results)} configs\")\n",
        "    print(f\"  Mean improvement: {np.mean(ratio_improvements):+.2f}%\")\n",
        "    print(f\"  Best improvement: {np.max(ratio_improvements):+.2f}%\")\n",
        "\n",
        "    # Find best config\n",
        "    best_idx = np.argmax([r['ndcg@10'] for r in ratio_results])\n",
        "    best_config = ratio_results[best_idx]\n",
        "    print(f\"  Best config: gamma={best_config['gamma']}, alpha={best_config['alpha']}\")\n",
        "\n",
        "# ============================================\n",
        "# 6. SUMMARY\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STATISTICAL ANALYSIS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "Based on {fl_total} Focal Loss configurations vs BCE baselines:\n",
        "\n",
        "EVIDENCE FOR H1 (Focal Loss improves NeuMF):\n",
        "  - Win rate: {fl_wins['ndcg@10']}/{fl_total} ({fl_wins['ndcg@10']/fl_total*100:.1f}%) on NDCG@10\n",
        "  - Sign test: p = {p_value_ndcg:.4f} {'(significant)' if p_value_ndcg < 0.05 else '(not significant)'}\n",
        "  - T-test: p = {p_value_t_ndcg_one:.4f} {'(significant)' if p_value_t_ndcg_one < 0.05 else '(not significant)'}\n",
        "  - Effect size: {cohens_d_ndcg:.3f} ({interpret_cohens_d(cohens_d_ndcg)})\n",
        "\n",
        "EVIDENCE FOR H2 (Robustness across sampling ratios):\n",
        "  - Improvements seen at: {sum(1 for r in SAMPLING_RATIOS if np.mean([(row['ndcg@10'] - bce_baselines[r]['ndcg@10'])/bce_baselines[r]['ndcg@10']*100 for row in grid_results if row['ratio'] == r]) > 0)}/{len(SAMPLING_RATIOS)} ratios\n",
        "\n",
        "NOTE: These results are from single-seed experiments across hyperparameter\n",
        "configurations. For publication-quality claims, use the multi-seed Wilcoxon\n",
        "test in the next section.\n",
        "\"\"\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "1tp0mi65chai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Significance Testing: Wilcoxon Signed-Rank Test (A100 Optimized)\n",
        "\n",
        "**Purpose**: Determine if the differences between BCE and Focal Loss are statistically significant.\n",
        "\n",
        "### A100 GPU Optimizations\n",
        "\n",
        "This cell is optimized for NVIDIA A100 GPU with the following settings:\n",
        "\n",
        "| Setting | Default | A100 Optimized | Speedup Factor |\n",
        "|---------|---------|----------------|----------------|\n",
        "| Train batch size | 256 | 4096 | ~16x fewer iterations |\n",
        "| Eval batch size | 4096 | 8192 | ~2x faster eval |\n",
        "| Mixed precision (AMP) | Off | On | ~2x via Tensor Cores |\n",
        "| cuDNN benchmark | Off | On | Optimized convolutions |\n",
        "| DataLoader workers | 0 | 4 | Parallel data loading |\n",
        "| Eval frequency | Every epoch | Every 3 epochs | ~3x less eval overhead |\n",
        "\n",
        "**Expected speedup**: 3-5x faster than default settings\n",
        "\n",
        "### Statistical Methodology\n",
        "\n",
        "The Wilcoxon signed-rank test is a non-parametric test for paired comparisons that:\n",
        "- Does not assume normality of the data\n",
        "- Is appropriate for comparing the same model trained with different loss functions\n",
        "- Uses paired observations (same seed, different methods)\n",
        "\n",
        "**Methodology** (from Section 5.6 of the paper):\n",
        "- Run each configuration with 10 different random seeds\n",
        "- Perform Wilcoxon signed-rank test for paired comparisons\n",
        "- Apply Bonferroni correction for multiple comparisons\n",
        "- Report effect size (rank-biserial correlation)\n",
        "\n",
        "**Significance Level**: p < 0.05 (after Bonferroni correction)"
      ],
      "metadata": {
        "id": "1qWjGTOytNBl"
      },
      "id": "1qWjGTOytNBl"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STATISTICAL SIGNIFICANCE TESTING\n",
        "# Wilcoxon Signed-Rank Test - A100 OPTIMIZED\n",
        "# ============================================\n",
        "# NOTE: This cell runs multi-seed experiments for statistical validation.\n",
        "# The Wilcoxon test requires PAIRED observations (same seed, different methods).\n",
        "#\n",
        "# A100 OPTIMIZATIONS APPLIED:\n",
        "# - Larger batch sizes (BCE: 2048, Custom losses: 1024)\n",
        "# - cuDNN benchmark mode enabled\n",
        "# - Increased DataLoader workers\n",
        "# - Reduced evaluation frequency\n",
        "#\n",
        "# NOTE: AMP disabled and smaller batch for custom loss functions (FocalLoss/AlphaBCE)\n",
        "# to ensure numerical stability with torch.log() operations.\n",
        "#\n",
        "# Expected speedup: 2-3x faster than default settings\n",
        "# ============================================\n",
        "from scipy import stats\n",
        "import time\n",
        "\n",
        "# ============================================\n",
        "# A100 GPU OPTIMIZATIONS\n",
        "# ============================================\n",
        "def get_a100_optimized_config(base_config, seed, use_amp=True, batch_size=2048):\n",
        "    \"\"\"\n",
        "    Apply A100-specific optimizations to config.\n",
        "\n",
        "    Optimizations:\n",
        "    - train_batch_size: Configurable (2048 for BCE, 1024 for custom losses)\n",
        "    - eval_batch_size: 8192 (2x increase)\n",
        "    - enable_amp: Only for standard BCE (disabled for custom losses)\n",
        "    - worker: 4 (parallel data loading)\n",
        "    - eval_step: 3 (evaluate every 3 epochs instead of 1)\n",
        "    - show_progress: False (reduce I/O overhead)\n",
        "    \"\"\"\n",
        "    config = base_config.copy()\n",
        "    config.update({\n",
        "        'seed': seed,\n",
        "        # Batch size optimizations\n",
        "        'train_batch_size': batch_size,\n",
        "        'eval_batch_size': 8192,\n",
        "        # Mixed precision - only for standard loss functions\n",
        "        'enable_amp': use_amp,\n",
        "        # DataLoader optimization\n",
        "        'worker': 4,\n",
        "        # Reduce evaluation overhead\n",
        "        'eval_step': 3,\n",
        "        # Reduce logging overhead\n",
        "        'show_progress': False,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "# Enable cuDNN autotuning for optimal convolution algorithms\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    # Check if A100 is available\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    is_a100 = 'A100' in gpu_name\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"A100 detected: {is_a100}\")\n",
        "    print(f\"cuDNN benchmark: ENABLED\")\n",
        "    if is_a100:\n",
        "        print(\"Using A100-optimized settings\")\n",
        "    else:\n",
        "        print(\"NOTE: Not an A100, but optimizations will still help\")\n",
        "else:\n",
        "    is_a100 = False\n",
        "    print(\"WARNING: No GPU detected, running on CPU (will be slow)\")\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "REUSE_RESULTS = False  # Set True to skip training and use pre-computed results\n",
        "TEST_RATIO = 50        # Primary sampling ratio (1:50)\n",
        "NUM_SEEDS = 10\n",
        "SEEDS = list(range(42, 42 + NUM_SEEDS))\n",
        "\n",
        "# Batch sizes: larger for BCE (stable), smaller for custom losses (numerical safety)\n",
        "BCE_BATCH_SIZE = 2048      # 8x default\n",
        "CUSTOM_BATCH_SIZE = 1024   # 4x default (more conservative for custom losses)\n",
        "\n",
        "if REUSE_RESULTS:\n",
        "    # ============================================\n",
        "    # OPTION 1: Use pre-computed multi-seed results\n",
        "    # Replace these with your actual 10-seed results\n",
        "    # ============================================\n",
        "    bce_scores = {\n",
        "        'ndcg@10': [0.054, 0.052, 0.055, 0.053, 0.056, 0.051, 0.054, 0.055, 0.053, 0.052],\n",
        "        'hit@10':  [0.112, 0.108, 0.115, 0.110, 0.118, 0.105, 0.112, 0.114, 0.109, 0.107]\n",
        "    }\n",
        "    focal_scores = {\n",
        "        'ndcg@10': [0.059, 0.058, 0.061, 0.057, 0.062, 0.056, 0.060, 0.059, 0.058, 0.057],\n",
        "        'hit@10':  [0.125, 0.122, 0.130, 0.120, 0.132, 0.118, 0.127, 0.124, 0.121, 0.119]\n",
        "    }\n",
        "    alpha_bce_scores = {\n",
        "        'ndcg@10': [0.058, 0.056, 0.059, 0.055, 0.060, 0.054, 0.058, 0.057, 0.056, 0.055],\n",
        "        'hit@10':  [0.120, 0.116, 0.124, 0.114, 0.126, 0.112, 0.121, 0.118, 0.115, 0.113]\n",
        "    }\n",
        "    print(f\"\\nUsing PRE-COMPUTED results from {NUM_SEEDS} seeds\")\n",
        "    print(\"(Set REUSE_RESULTS = False to run fresh experiments)\")\n",
        "\n",
        "else:\n",
        "    # ============================================\n",
        "    # OPTION 2: Run fresh multi-seed experiments (A100 OPTIMIZED)\n",
        "    # ============================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"WILCOXON SIGNED-RANK TEST: Statistical Significance (A100 OPTIMIZED)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Running {NUM_SEEDS} seeds per configuration...\")\n",
        "    print(f\"Seeds: {SEEDS}\")\n",
        "    print(f\"Sampling ratio: 1:{TEST_RATIO}\")\n",
        "    print(f\"Total models to train: {NUM_SEEDS * 3}\")\n",
        "    print()\n",
        "    print(\"A100 Optimizations:\")\n",
        "    print(f\"  - BCE batch size: {BCE_BATCH_SIZE} (vs default 256)\")\n",
        "    print(f\"  - Custom loss batch size: {CUSTOM_BATCH_SIZE} (conservative)\")\n",
        "    print(f\"  - Mixed precision (AMP): BCE only\")\n",
        "    print(f\"  - cuDNN benchmark: Enabled\")\n",
        "    print(f\"  - DataLoader workers: 4\")\n",
        "    print(f\"  - Eval frequency: Every 3 epochs\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Reload focal_loss_utils to get updated loss functions\n",
        "    import importlib\n",
        "    import focal_loss_utils\n",
        "    importlib.reload(focal_loss_utils)\n",
        "    from focal_loss_utils import train_neumf_focal_loss, train_neumf_alpha_bce\n",
        "    print(\"Reloaded focal_loss_utils with improved numerical stability\")\n",
        "\n",
        "    bce_scores = {'ndcg@10': [], 'hit@10': []}\n",
        "    focal_scores = {'ndcg@10': [], 'hit@10': []}\n",
        "    alpha_bce_scores = {'ndcg@10': [], 'hit@10': []}\n",
        "\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    for i, seed in enumerate(SEEDS):\n",
        "        seed_start_time = time.time()\n",
        "        print(f\"\\n--- Seed {seed} ({i+1}/{NUM_SEEDS}) ---\")\n",
        "\n",
        "        # Train BCE (can use AMP and larger batch safely)\n",
        "        print(\"  Training BCE...\", end=\" \", flush=True)\n",
        "        t0 = time.time()\n",
        "        bce_config = get_a100_optimized_config(\n",
        "            configs[TEST_RATIO], seed, use_amp=True, batch_size=BCE_BATCH_SIZE\n",
        "        )\n",
        "        bce_config['loss_type'] = 'BCE'\n",
        "        result_bce_seed = run_recbole(\n",
        "            model='NeuMF',\n",
        "            dataset=DATASET,\n",
        "            config_dict=bce_config\n",
        "        )\n",
        "        bce_scores['ndcg@10'].append(result_bce_seed['test_result'].get('ndcg@10', 0))\n",
        "        bce_scores['hit@10'].append(result_bce_seed['test_result'].get('hit@10', 0))\n",
        "        print(f\"done ({time.time()-t0:.1f}s)\")\n",
        "\n",
        "        # Train Focal Loss (disable AMP, use smaller batch for stability)\n",
        "        print(\"  Training Focal Loss...\", end=\" \", flush=True)\n",
        "        t0 = time.time()\n",
        "        fl_config = get_a100_optimized_config(\n",
        "            configs[TEST_RATIO], seed, use_amp=False, batch_size=CUSTOM_BATCH_SIZE\n",
        "        )\n",
        "        if 'loss_type' in fl_config:\n",
        "            del fl_config['loss_type']\n",
        "        result_fl_seed = train_neumf_focal_loss(\n",
        "            config_dict=fl_config,\n",
        "            dataset=DATASET,\n",
        "            gamma=GAMMA,\n",
        "            alpha=ALPHA,\n",
        "            seed=seed,\n",
        "            track_dynamics=False\n",
        "        )\n",
        "        focal_scores['ndcg@10'].append(result_fl_seed['test_result'].get('ndcg@10', 0))\n",
        "        focal_scores['hit@10'].append(result_fl_seed['test_result'].get('hit@10', 0))\n",
        "        print(f\"done ({time.time()-t0:.1f}s)\")\n",
        "\n",
        "        # Train Alpha-BCE (disable AMP, use smaller batch for stability)\n",
        "        print(\"  Training Alpha-BCE...\", end=\" \", flush=True)\n",
        "        t0 = time.time()\n",
        "        alpha_config = get_a100_optimized_config(\n",
        "            configs[TEST_RATIO], seed, use_amp=False, batch_size=CUSTOM_BATCH_SIZE\n",
        "        )\n",
        "        if 'loss_type' in alpha_config:\n",
        "            del alpha_config['loss_type']\n",
        "        result_alpha_seed = train_neumf_alpha_bce(\n",
        "            config_dict=alpha_config,\n",
        "            dataset=DATASET,\n",
        "            alpha=ALPHA,\n",
        "            seed=seed,\n",
        "            track_dynamics=False\n",
        "        )\n",
        "        alpha_bce_scores['ndcg@10'].append(result_alpha_seed['test_result'].get('ndcg@10', 0))\n",
        "        alpha_bce_scores['hit@10'].append(result_alpha_seed['test_result'].get('hit@10', 0))\n",
        "        print(f\"done ({time.time()-t0:.1f}s)\")\n",
        "\n",
        "        seed_time = time.time() - seed_start_time\n",
        "        print(f\"  Results - BCE: {bce_scores['ndcg@10'][-1]:.4f}, FL: {focal_scores['ndcg@10'][-1]:.4f}, Alpha-BCE: {alpha_bce_scores['ndcg@10'][-1]:.4f}\")\n",
        "        print(f\"  Seed {seed} completed in {seed_time:.1f}s\")\n",
        "\n",
        "        # Estimate remaining time\n",
        "        elapsed = time.time() - total_start_time\n",
        "        avg_per_seed = elapsed / (i + 1)\n",
        "        remaining = avg_per_seed * (NUM_SEEDS - i - 1)\n",
        "        print(f\"  Estimated time remaining: {remaining/60:.1f} min\")\n",
        "\n",
        "    total_time = time.time() - total_start_time\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Multi-seed training complete!\")\n",
        "    print(f\"Total time: {total_time/60:.1f} minutes ({total_time:.0f}s)\")\n",
        "    print(f\"Average per seed: {total_time/NUM_SEEDS:.1f}s\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "dWA7knQHtO0s"
      },
      "id": "dWA7knQHtO0s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# WILCOXON TEST RESULTS\n",
        "# ============================================\n",
        "\n",
        "def compute_rank_biserial(x, y):\n",
        "    \"\"\"\n",
        "    Compute rank-biserial correlation as effect size for Wilcoxon test.\n",
        "    r = (W+ - W-) / (n*(n+1)/2)\n",
        "    where W+ and W- are sums of positive and negative ranks.\n",
        "    \"\"\"\n",
        "    n = len(x)\n",
        "    diff = np.array(x) - np.array(y)\n",
        "    # Remove zero differences\n",
        "    diff = diff[diff != 0]\n",
        "    if len(diff) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Rank absolute differences\n",
        "    abs_diff = np.abs(diff)\n",
        "    ranks = stats.rankdata(abs_diff)\n",
        "\n",
        "    # Sum of positive and negative ranks\n",
        "    pos_ranks = np.sum(ranks[diff > 0])\n",
        "    neg_ranks = np.sum(ranks[diff < 0])\n",
        "\n",
        "    # Rank-biserial correlation\n",
        "    n_nonzero = len(diff)\n",
        "    r = (pos_ranks - neg_ranks) / (n_nonzero * (n_nonzero + 1) / 2)\n",
        "    return r\n",
        "\n",
        "def interpret_effect_size(r):\n",
        "    \"\"\"Interpret rank-biserial correlation effect size.\"\"\"\n",
        "    r_abs = abs(r)\n",
        "    if r_abs < 0.1:\n",
        "        return \"negligible\"\n",
        "    elif r_abs < 0.3:\n",
        "        return \"small\"\n",
        "    elif r_abs < 0.5:\n",
        "        return \"medium\"\n",
        "    else:\n",
        "        return \"large\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"WILCOXON SIGNED-RANK TEST RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Number of comparisons for Bonferroni correction\n",
        "NUM_COMPARISONS = 4  # 2 metrics x 2 comparisons (FL vs BCE, FL vs Alpha-BCE)\n",
        "ALPHA_LEVEL = 0.05\n",
        "CORRECTED_ALPHA = ALPHA_LEVEL / NUM_COMPARISONS\n",
        "\n",
        "print(f\"\\nSignificance level: {ALPHA_LEVEL}\")\n",
        "print(f\"Bonferroni-corrected alpha: {CORRECTED_ALPHA:.4f} (for {NUM_COMPARISONS} comparisons)\")\n",
        "\n",
        "# Store all test results\n",
        "test_results = []\n",
        "\n",
        "# Test 1: Focal Loss vs BCE\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"H1: Focal Loss vs BCE\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for metric in ['ndcg@10', 'hit@10']:\n",
        "    bce = np.array(bce_scores[metric])\n",
        "    fl = np.array(focal_scores[metric])\n",
        "\n",
        "    # Wilcoxon signed-rank test (two-sided)\n",
        "    statistic, p_value = stats.wilcoxon(fl, bce, alternative='two-sided')\n",
        "\n",
        "    # Effect size\n",
        "    effect_size = compute_rank_biserial(fl, bce)\n",
        "    effect_interp = interpret_effect_size(effect_size)\n",
        "\n",
        "    # Mean and std\n",
        "    bce_mean, bce_std = np.mean(bce), np.std(bce)\n",
        "    fl_mean, fl_std = np.mean(fl), np.std(fl)\n",
        "    pct_change = (fl_mean - bce_mean) / bce_mean * 100\n",
        "\n",
        "    # Significance\n",
        "    is_significant = p_value < CORRECTED_ALPHA\n",
        "\n",
        "    print(f\"\\n{metric.upper()}:\")\n",
        "    print(f\"  BCE:        {bce_mean:.4f} +/- {bce_std:.4f}\")\n",
        "    print(f\"  Focal Loss: {fl_mean:.4f} +/- {fl_std:.4f}\")\n",
        "    print(f\"  Change:     {pct_change:+.2f}%\")\n",
        "    print(f\"  Wilcoxon W: {statistic:.1f}\")\n",
        "    print(f\"  p-value:    {p_value:.4f} {'*' if is_significant else ''}\")\n",
        "    print(f\"  Effect size (r): {effect_size:+.3f} ({effect_interp})\")\n",
        "    print(f\"  Significant (p < {CORRECTED_ALPHA:.4f}): {'YES' if is_significant else 'NO'}\")\n",
        "\n",
        "    test_results.append({\n",
        "        'comparison': 'FL vs BCE',\n",
        "        'metric': metric,\n",
        "        'baseline_mean': bce_mean,\n",
        "        'baseline_std': bce_std,\n",
        "        'test_mean': fl_mean,\n",
        "        'test_std': fl_std,\n",
        "        'pct_change': pct_change,\n",
        "        'wilcoxon_W': statistic,\n",
        "        'p_value': p_value,\n",
        "        'effect_size': effect_size,\n",
        "        'significant': is_significant\n",
        "    })\n",
        "\n",
        "# Test 2: Focal Loss vs Alpha-BCE\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"H3: Focal Loss vs Alpha-BCE (Mechanism)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for metric in ['ndcg@10', 'hit@10']:\n",
        "    alpha = np.array(alpha_bce_scores[metric])\n",
        "    fl = np.array(focal_scores[metric])\n",
        "\n",
        "    # Wilcoxon signed-rank test (two-sided)\n",
        "    statistic, p_value = stats.wilcoxon(fl, alpha, alternative='two-sided')\n",
        "\n",
        "    # Effect size\n",
        "    effect_size = compute_rank_biserial(fl, alpha)\n",
        "    effect_interp = interpret_effect_size(effect_size)\n",
        "\n",
        "    # Mean and std\n",
        "    alpha_mean, alpha_std = np.mean(alpha), np.std(alpha)\n",
        "    fl_mean, fl_std = np.mean(fl), np.std(fl)\n",
        "    pct_change = (fl_mean - alpha_mean) / alpha_mean * 100\n",
        "\n",
        "    # Significance\n",
        "    is_significant = p_value < CORRECTED_ALPHA\n",
        "\n",
        "    print(f\"\\n{metric.upper()}:\")\n",
        "    print(f\"  Alpha-BCE:  {alpha_mean:.4f} +/- {alpha_std:.4f}\")\n",
        "    print(f\"  Focal Loss: {fl_mean:.4f} +/- {fl_std:.4f}\")\n",
        "    print(f\"  Change:     {pct_change:+.2f}%\")\n",
        "    print(f\"  Wilcoxon W: {statistic:.1f}\")\n",
        "    print(f\"  p-value:    {p_value:.4f} {'*' if is_significant else ''}\")\n",
        "    print(f\"  Effect size (r): {effect_size:+.3f} ({effect_interp})\")\n",
        "    print(f\"  Significant (p < {CORRECTED_ALPHA:.4f}): {'YES' if is_significant else 'NO'}\")\n",
        "\n",
        "    test_results.append({\n",
        "        'comparison': 'FL vs Alpha-BCE',\n",
        "        'metric': metric,\n",
        "        'baseline_mean': alpha_mean,\n",
        "        'baseline_std': alpha_std,\n",
        "        'test_mean': fl_mean,\n",
        "        'test_std': fl_std,\n",
        "        'pct_change': pct_change,\n",
        "        'wilcoxon_W': statistic,\n",
        "        'p_value': p_value,\n",
        "        'effect_size': effect_size,\n",
        "        'significant': is_significant\n",
        "    })"
      ],
      "metadata": {
        "id": "Dl0juTd-uIuS"
      },
      "id": "Dl0juTd-uIuS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# SUMMARY TABLE\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STATISTICAL SIGNIFICANCE SUMMARY TABLE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "summary_stats = []\n",
        "for r in test_results:\n",
        "    summary_stats.append({\n",
        "        'Comparison': r['comparison'],\n",
        "        'Metric': r['metric'].upper(),\n",
        "        'Baseline': f\"{r['baseline_mean']:.4f}+/-{r['baseline_std']:.4f}\",\n",
        "        'Focal Loss': f\"{r['test_mean']:.4f}+/-{r['test_std']:.4f}\",\n",
        "        'Change': f\"{r['pct_change']:+.2f}%\",\n",
        "        'p-value': f\"{r['p_value']:.4f}\",\n",
        "        'Effect (r)': f\"{r['effect_size']:+.3f}\",\n",
        "        'Sig?': 'Yes*' if r['significant'] else 'No'\n",
        "    })\n",
        "\n",
        "stats_df = pd.DataFrame(summary_stats)\n",
        "print(\"\\n\" + stats_df.to_string(index=False))\n",
        "\n",
        "print(f\"\\n* Significant at Bonferroni-corrected alpha = {CORRECTED_ALPHA:.4f}\")\n",
        "\n",
        "# Final interpretation\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STATISTICAL CONCLUSIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "h1_sig = any(r['significant'] and r['comparison'] == 'FL vs BCE' and r['pct_change'] > 0\n",
        "             for r in test_results)\n",
        "h3_sig = any(r['significant'] and r['comparison'] == 'FL vs Alpha-BCE' and r['pct_change'] > 0\n",
        "             for r in test_results)\n",
        "\n",
        "print(\"\\nH1 (Focal Loss vs BCE):\")\n",
        "if h1_sig:\n",
        "    print(\"  STATISTICALLY SIGNIFICANT improvement detected\")\n",
        "else:\n",
        "    sig_results = [r for r in test_results if r['comparison'] == 'FL vs BCE']\n",
        "    if all(r['pct_change'] < 0 for r in sig_results):\n",
        "        print(\"  No significant improvement; Focal Loss may underperform BCE\")\n",
        "    else:\n",
        "        print(\"  No statistically significant difference detected\")\n",
        "\n",
        "print(\"\\nH3 (Focal Loss vs Alpha-BCE - Mechanism):\")\n",
        "if h3_sig:\n",
        "    print(\"  STATISTICALLY SIGNIFICANT: Focusing effect (gamma > 0) provides benefit\")\n",
        "else:\n",
        "    print(\"  NOT SIGNIFICANT: Class weighting alone (Alpha-BCE) may be sufficient\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Note: Effect size interpretation (rank-biserial r):\")\n",
        "print(\"  |r| < 0.1: negligible, 0.1-0.3: small, 0.3-0.5: medium, > 0.5: large\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "hoCkEQINvYtz"
      },
      "id": "hoCkEQINvYtz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paired Results Visualization"
      ],
      "metadata": {
        "id": "xiuoSb2-vedY"
      },
      "id": "xiuoSb2-vedY"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PAIRED RESULTS VISUALIZATION\n",
        "# ============================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: NDCG@10 across seeds\n",
        "ax1 = axes[0]\n",
        "seeds_idx = range(1, NUM_SEEDS + 1)\n",
        "ax1.plot(seeds_idx, bce_scores['ndcg@10'], 'o-', label='BCE', color='blue', markersize=8)\n",
        "ax1.plot(seeds_idx, focal_scores['ndcg@10'], 's-', label='Focal Loss', color='red', markersize=8)\n",
        "ax1.plot(seeds_idx, alpha_bce_scores['ndcg@10'], '^-', label='Alpha-BCE', color='green', markersize=8)\n",
        "ax1.set_xlabel('Seed Index', fontsize=12)\n",
        "ax1.set_ylabel('NDCG@10', fontsize=12)\n",
        "ax1.set_title('NDCG@10 Across Random Seeds', fontsize=14)\n",
        "ax1.legend(loc='best')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xticks(seeds_idx)\n",
        "\n",
        "# Plot 2: HR@10 across seeds\n",
        "ax2 = axes[1]\n",
        "ax2.plot(seeds_idx, bce_scores['hit@10'], 'o-', label='BCE', color='blue', markersize=8)\n",
        "ax2.plot(seeds_idx, focal_scores['hit@10'], 's-', label='Focal Loss', color='red', markersize=8)\n",
        "ax2.plot(seeds_idx, alpha_bce_scores['hit@10'], '^-', label='Alpha-BCE', color='green', markersize=8)\n",
        "ax2.set_xlabel('Seed Index', fontsize=12)\n",
        "ax2.set_ylabel('HR@10', fontsize=12)\n",
        "ax2.set_title('HR@10 Across Random Seeds', fontsize=14)\n",
        "ax2.legend(loc='best')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xticks(seeds_idx)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('wilcoxon_paired_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFigure saved as 'wilcoxon_paired_results.png'\")"
      ],
      "metadata": {
        "id": "s2YaXa74vg5C"
      },
      "id": "s2YaXa74vg5C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BOX PLOT COMPARISON\n",
        "# ============================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# NDCG@10 box plot\n",
        "ax1 = axes[0]\n",
        "data_ndcg = [bce_scores['ndcg@10'], alpha_bce_scores['ndcg@10'], focal_scores['ndcg@10']]\n",
        "bp1 = ax1.boxplot(data_ndcg, labels=['BCE', 'Alpha-BCE', 'Focal Loss'], patch_artist=True)\n",
        "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
        "for patch, color in zip(bp1['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.7)\n",
        "ax1.set_ylabel('NDCG@10', fontsize=12)\n",
        "ax1.set_title('NDCG@10 Distribution (10 seeds)', fontsize=14)\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# HR@10 box plot\n",
        "ax2 = axes[1]\n",
        "data_hr = [bce_scores['hit@10'], alpha_bce_scores['hit@10'], focal_scores['hit@10']]\n",
        "bp2 = ax2.boxplot(data_hr, labels=['BCE', 'Alpha-BCE', 'Focal Loss'], patch_artist=True)\n",
        "for patch, color in zip(bp2['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.7)\n",
        "ax2.set_ylabel('HR@10', fontsize=12)\n",
        "ax2.set_title('HR@10 Distribution (10 seeds)', fontsize=14)\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('wilcoxon_boxplot.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFigure saved as 'wilcoxon_boxplot.png'\")"
      ],
      "metadata": {
        "id": "ru2x_n0Lvi-p"
      },
      "id": "ru2x_n0Lvi-p",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}