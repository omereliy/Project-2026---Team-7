{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NCF with Focal Loss - ML-1M Improved Experimental Design\n",
    "\n",
    "**Paper**: \"Addressing Class Imbalance in NCF with Focal Loss\" (AAMAS 2025)\n",
    "\n",
    "## Hypotheses\n",
    "\n",
    "This notebook tests three key hypotheses:\n",
    "\n",
    "**H1: Focal Loss improves NeuMF performance**  \n",
    "Focal Loss will outperform standard BCE loss on NeuMF for implicit feedback recommendation.\n",
    "\n",
    "**H2: Focal Loss is robust to negative sampling ratio**  \n",
    "Focal Loss will maintain consistent performance across different negative sampling ratios (1:4, 1:10, 1:50), while BCE may degrade with higher imbalance.\n",
    "\n",
    "**H3: Focusing effect is necessary beyond class weighting**  \n",
    "The focusing parameter (gamma) provides benefits beyond class balancing (alpha). We test this by comparing Focal Loss to Alpha-Balanced BCE (gamma=0 control).\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "- **Dataset**: MovieLens 1M (larger, more realistic than ML-100K)\n",
    "- **Model**: NeuMF (Neural Collaborative Filtering)\n",
    "- **Evaluation**: Leave-one-out, full ranking, HR@10 and NDCG@10\n",
    "- **Primary sampling ratio**: 1:10 (moderate imbalance, commonly used in literature)\n",
    "- **Robustness test**: 1:4 (low), 1:10 (moderate), 1:50 (high imbalance)\n",
    "\n",
    "## Methodology Improvements from Review\n",
    "\n",
    "1. **Alpha-Balanced BCE control** - Isolates focusing effect (gamma) from class weighting (alpha)\n",
    "2. **Multiple sampling ratios** - Tests robustness hypothesis (H2)\n",
    "3. **1:10 as primary** - More realistic imbalance than 1:4\n",
    "4. **Training dynamics tracking** - Validates mechanism (loss contribution by confidence bin)\n",
    "5. **Alpha-sampling interaction analysis** - Documents effective class weight ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Cell 1: Suppress Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Filter out FutureWarnings from pandas\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='pandas')\n",
    "\n",
    "# Filter out FutureWarnings from torch (like the GradScaler one)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='torch')\n",
    "\n",
    "# Filter out warnings from RecBole\n",
    "warnings.filterwarnings('ignore', message='^command line args.*will not be used in RecBole', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', message='^All the same value in \\[label\\].*', category=UserWarning)\n",
    "\n",
    "print(\"Warnings suppressed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Cell 2: Install Dependencies\n",
    "\n",
    "Run this cell, then **RESTART the runtime** before continuing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Install Dependencies\n",
    "# ============================================\n",
    "# Run this cell, then RESTART the runtime before continuing!\n",
    "\n",
    "!pip install ray\n",
    "!pip uninstall numpy -y\n",
    "!pip install numpy==1.26.4\n",
    "!pip install recbole==1.2.0\n",
    "!pip install kmeans-pytorch\n",
    "\n",
    "# Verify numpy version\n",
    "import numpy as np\n",
    "print(f\"\\nNumPy version: {np.__version__}\")\n",
    "if np.__version__.startswith(\"2.\"):\n",
    "    print(\"ERROR: NumPy 2.x detected! Please RESTART the runtime now.\")\n",
    "    print(\"Go to: Runtime -> Restart session\")\n",
    "else:\n",
    "    print(\"NumPy version OK. You can continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Cell 3: Imports\n",
    "\n",
    "Import utilities from `focal_loss_utils.py` and standard libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Fix for PyTorch 2.6+ (only patch once)\n",
    "if not hasattr(torch, '_load_patched'):\n",
    "    _original_torch_load = torch.load\n",
    "    def _patched_torch_load(*args, **kwargs):\n",
    "        if 'weights_only' not in kwargs:\n",
    "            kwargs['weights_only'] = False\n",
    "        return _original_torch_load(*args, **kwargs)\n",
    "    torch.load = _patched_torch_load\n",
    "    torch._load_patched = True\n",
    "\n",
    "# RecBole imports\n",
    "from recbole.quick_start import run_recbole\n",
    "from recbole.model.general_recommender.neumf import NeuMF\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.utils import init_seed, init_logger\n",
    "\n",
    "# Import from focal_loss_utils.py\n",
    "import sys\n",
    "sys.path.insert(0, '/content' if 'google.colab' in sys.modules else '.')\n",
    "from focal_loss_utils import (\n",
    "    FocalLoss, AlphaBalancedBCE,\n",
    "    get_base_config, get_neumf_config,\n",
    "    train_neumf_focal_loss, train_neumf_alpha_bce,\n",
    "    create_comparison_table, compute_improvement,\n",
    "    validate_focal_loss_implementation, demonstrate_focal_loss_effect,\n",
    "    analyze_alpha_sampling_interaction, compute_effective_class_ratio,\n",
    "    get_balanced_alpha, run_multi_seed_experiment\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"Using: CPU\")\n",
    "\n",
    "print(\"\\nImports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Cell 4: Focal Loss Validation Test\n",
    "\n",
    "Validate that our Focal Loss implementation is correct before running experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Focal Loss Validation\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDATION: Focal Loss Implementation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test 1: Verify implementation correctness\n",
    "validate_focal_loss_implementation()\n",
    "\n",
    "# Test 2: Demonstrate focusing effect\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEMONSTRATION: Focal Loss Effect on Easy vs Hard Examples\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "demo_df = demonstrate_focal_loss_effect()\n",
    "print(\"\\n\" + demo_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Key Insight: Focal Loss down-weights easy examples, allowing the model\")\n",
    "print(\"to focus on hard examples (the focusing effect).\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Cell 5: Experiment Configuration\n",
    "\n",
    "### Negative Sampling Ratios\n",
    "\n",
    "We test three negative sampling ratios to evaluate robustness (H2):\n",
    "\n",
    "- **1:4** - Standard ratio (low imbalance)\n",
    "- **1:10** - Moderate imbalance (PRIMARY experiment)\n",
    "- **1:50** - High imbalance (realistic for large-scale systems)\n",
    "\n",
    "### Alpha-Sampling Interaction\n",
    "\n",
    "The class balancing weight (alpha) interacts with the negative sampling ratio. A fixed alpha=0.5 does NOT provide equal weighting across different sampling ratios.\n",
    "\n",
    "Example: With 1:4 sampling and alpha=0.5:\n",
    "- Effective ratio = ((1-alpha) * neg_ratio) / alpha = ((1-0.5) * 4) / 0.5 = 4:1\n",
    "- Negatives are weighted 4x more than positives!\n",
    "\n",
    "We address this by computing balanced alpha values for each sampling ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Configuration Setup\n",
    "# ============================================\n",
    "SAMPLING_RATIOS = [4, 10, 50]\n",
    "DATASET = 'ml-1m'\n",
    "\n",
    "# Create configs for each sampling ratio\n",
    "configs = {}\n",
    "for ratio in SAMPLING_RATIOS:\n",
    "    base = get_base_config(DATASET, device, neg_sample_num=ratio)\n",
    "    configs[ratio] = get_neumf_config(base)\n",
    "\n",
    "print(\"Configurations created for negative sampling ratios:\")\n",
    "for ratio in SAMPLING_RATIOS:\n",
    "    print(f\"  1:{ratio} (config key: {ratio})\")\n",
    "\n",
    "print(f\"\\nDataset: {DATASET}\")\n",
    "print(f\"Model: NeuMF\")\n",
    "print(f\"Primary experiment: 1:10 sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Cell 6: Alpha-Sampling Interaction Analysis\n",
    "\n",
    "This analysis reveals a critical issue: **alpha=0.5 does NOT provide balanced weighting** when combined with negative sampling.\n",
    "\n",
    "The effective class weight ratio is:\n",
    "\n",
    "$$\\text{Effective Ratio} = \\frac{(1-\\alpha) \\times \\text{neg\\_ratio}}{\\alpha}$$\n",
    "\n",
    "For balanced weighting (ratio = 1:1), we need:\n",
    "\n",
    "$$\\alpha = \\frac{\\text{neg\\_ratio}}{\\text{neg\\_ratio} + 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Alpha-Sampling Interaction Analysis\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"ANALYSIS: Alpha-Sampling Interaction\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "interaction_df = analyze_alpha_sampling_interaction(\n",
    "    neg_ratios=SAMPLING_RATIOS,\n",
    "    alphas=[0.25, 0.5, 0.75]\n",
    ")\n",
    "\n",
    "print(\"\\nEffective class weight ratios (neg:pos) for different alpha values:\")\n",
    "print(interaction_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Balanced alpha values (for effective ratio = 1:1):\")\n",
    "print(\"=\"*70)\n",
    "for ratio in SAMPLING_RATIOS:\n",
    "    balanced = get_balanced_alpha(ratio)\n",
    "    print(f\"  1:{ratio} sampling -> balanced alpha = {balanced:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Key Insight: Alpha values must be tuned for each sampling ratio.\")\n",
    "print(\"Using alpha=0.25 (from object detection) may not be optimal for NCF.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Experiment 1: NeuMF-BCE Baseline (1:10 Sampling)\n",
    "\n",
    "Train standard NeuMF with Binary Cross-Entropy loss.\n",
    "\n",
    "This serves as our baseline for testing H1 (Focal Loss improves NeuMF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Train NeuMF-BCE (1:10 sampling - PRIMARY)\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 1: NeuMF-BCE Baseline (1:10 Sampling)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use 1:10 as primary experiment\n",
    "PRIMARY_RATIO = 10\n",
    "bce_config = configs[PRIMARY_RATIO].copy()\n",
    "bce_config['loss_type'] = 'BCE'\n",
    "\n",
    "result_bce = run_recbole(\n",
    "    model='NeuMF',\n",
    "    dataset=DATASET,\n",
    "    config_dict=bce_config\n",
    ")\n",
    "\n",
    "# Store results\n",
    "bce_results = {\n",
    "    'model': 'NeuMF-BCE',\n",
    "    'best_valid_score': result_bce['best_valid_score'],\n",
    "    'test_result': result_bce['test_result']\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NeuMF-BCE Results (1:10 sampling):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Validation NDCG@10: {result_bce['best_valid_score']:.4f}\")\n",
    "print(f\"\\nTest Results:\")\n",
    "for metric, value in result_bce['test_result'].items():\n",
    "    print(f\"  {metric.upper():12s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Experiment 2: NeuMF with Alpha-Balanced BCE (NEW Control)\n",
    "\n",
    "### Purpose: Isolate Focusing Effect from Class Weighting (H3)\n",
    "\n",
    "Alpha-Balanced BCE is Focal Loss with gamma=0. This control allows us to determine whether improvements come from:\n",
    "- **Class weighting (alpha)** - balancing positive/negative loss contributions\n",
    "- **Focusing effect (gamma)** - down-weighting easy examples\n",
    "\n",
    "If Focal Loss outperforms Alpha-BCE, we can attribute the gain to the focusing effect.\n",
    "\n",
    "### Tuned Alpha Values\n",
    "\n",
    "We use tuned alpha values rather than alpha=0.25 from object detection:\n",
    "- For 1:10 sampling, we test alpha in [0.5, 0.7, 0.9] to find the optimal class balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Train NeuMF-AlphaBCE (1:10 sampling)\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 2: NeuMF with Alpha-Balanced BCE (gamma=0 Control)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test multiple alpha values to find optimal class balancing\n",
    "ALPHA_VALUES = [0.5, 0.7, 0.9]\n",
    "alpha_bce_results = {}\n",
    "\n",
    "for alpha in ALPHA_VALUES:\n",
    "    print(f\"\\nTraining with alpha={alpha}...\")\n",
    "    result_alpha = train_neumf_alpha_bce(\n",
    "        config_dict=configs[PRIMARY_RATIO],\n",
    "        dataset=DATASET,\n",
    "        alpha=alpha,\n",
    "        seed=42,\n",
    "        track_dynamics=False\n",
    "    )\n",
    "    \n",
    "    alpha_bce_results[alpha] = result_alpha\n",
    "    print(f\"  Validation NDCG@10: {result_alpha['best_valid_score']:.4f}\")\n",
    "    print(f\"  Test NDCG@10: {result_alpha['test_result']['ndcg@10']:.4f}\")\n",
    "\n",
    "# Select best alpha\n",
    "best_alpha = max(ALPHA_VALUES, key=lambda a: alpha_bce_results[a]['best_valid_score'])\n",
    "best_alpha_bce_result = alpha_bce_results[best_alpha]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Best Alpha-Balanced BCE: alpha={best_alpha}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Validation NDCG@10: {best_alpha_bce_result['best_valid_score']:.4f}\")\n",
    "print(f\"Test NDCG@10: {best_alpha_bce_result['test_result']['ndcg@10']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Experiment 3: NeuMF with Focal Loss\n",
    "\n",
    "Train NeuMF with Focal Loss (gamma=2.0, alpha=0.25).\n",
    "\n",
    "This tests H1: Focal Loss improves NeuMF performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Train NeuMF-FocalLoss (1:10 sampling)\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 3: NeuMF with Focal Loss\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "GAMMA = 2.0\n",
    "ALPHA_FL = 0.25\n",
    "\n",
    "result_fl = train_neumf_focal_loss(\n",
    "    config_dict=configs[PRIMARY_RATIO],\n",
    "    dataset=DATASET,\n",
    "    gamma=GAMMA,\n",
    "    alpha=ALPHA_FL,\n",
    "    seed=42,\n",
    "    track_dynamics=False\n",
    ")\n",
    "\n",
    "fl_results = {\n",
    "    'model': f'NeuMF-FL(g={GAMMA},a={ALPHA_FL})',\n",
    "    'best_valid_score': result_fl['best_valid_score'],\n",
    "    'test_result': result_fl['test_result']\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"NeuMF-FocalLoss Results (gamma={GAMMA}, alpha={ALPHA_FL}):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Validation NDCG@10: {result_fl['best_valid_score']:.4f}\")\n",
    "print(f\"\\nTest Results:\")\n",
    "for metric, value in result_fl['test_result'].items():\n",
    "    print(f\"  {metric.upper():12s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Hypothesis Testing: Primary Comparison (1:10 Sampling)\n",
    "\n",
    "Compare the three approaches:\n",
    "1. **NeuMF-BCE** - Standard baseline\n",
    "2. **NeuMF-AlphaBCE** - Class weighting only (gamma=0)\n",
    "3. **NeuMF-FocalLoss** - Class weighting + focusing effect (gamma=2)\n",
    "\n",
    "This comparison tests:\n",
    "- **H1**: Does Focal Loss improve over BCE?\n",
    "- **H3**: Does the focusing effect (gamma) provide benefits beyond class weighting (alpha)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Comparison Table\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS TESTING: Primary Comparison (1:10 Sampling)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = create_comparison_table(\n",
    "    [bce_results, best_alpha_bce_result, fl_results],\n",
    "    ['BCE', f'Alpha-BCE(a={best_alpha})', f'Focal(g={GAMMA},a={ALPHA_FL})']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Compute improvements\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPROVEMENTS OVER BASELINE (BCE):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Alpha-BCE vs BCE\n",
    "alpha_improvement = compute_improvement(bce_results, best_alpha_bce_result)\n",
    "print(f\"\\nAlpha-BCE (alpha={best_alpha}) vs BCE:\")\n",
    "print(f\"  NDCG@10: {alpha_improvement['ndcg@10']['comparison']:.4f} vs {alpha_improvement['ndcg@10']['baseline']:.4f}\")\n",
    "print(f\"  Change: {alpha_improvement['ndcg@10']['pct_change']:+.2f}%\")\n",
    "print(f\"  HR@10:   {alpha_improvement['hit@10']['comparison']:.4f} vs {alpha_improvement['hit@10']['baseline']:.4f}\")\n",
    "print(f\"  Change: {alpha_improvement['hit@10']['pct_change']:+.2f}%\")\n",
    "\n",
    "# Focal Loss vs BCE\n",
    "fl_improvement = compute_improvement(bce_results, fl_results)\n",
    "print(f\"\\nFocal Loss (gamma={GAMMA}, alpha={ALPHA_FL}) vs BCE:\")\n",
    "print(f\"  NDCG@10: {fl_improvement['ndcg@10']['comparison']:.4f} vs {fl_improvement['ndcg@10']['baseline']:.4f}\")\n",
    "print(f\"  Change: {fl_improvement['ndcg@10']['pct_change']:+.2f}%\")\n",
    "print(f\"  HR@10:   {fl_improvement['hit@10']['comparison']:.4f} vs {fl_improvement['hit@10']['baseline']:.4f}\")\n",
    "print(f\"  Change: {fl_improvement['hit@10']['pct_change']:+.2f}%\")\n",
    "\n",
    "# Focal Loss vs Alpha-BCE (isolating focusing effect)\n",
    "fl_vs_alpha_improvement = compute_improvement(best_alpha_bce_result, fl_results)\n",
    "print(f\"\\nFocal Loss vs Alpha-BCE (focusing effect only):\")\n",
    "print(f\"  NDCG@10: {fl_vs_alpha_improvement['ndcg@10']['comparison']:.4f} vs {fl_vs_alpha_improvement['ndcg@10']['baseline']:.4f}\")\n",
    "print(f\"  Change: {fl_vs_alpha_improvement['ndcg@10']['pct_change']:+.2f}%\")\n",
    "print(f\"  HR@10:   {fl_vs_alpha_improvement['hit@10']['comparison']:.4f} vs {fl_vs_alpha_improvement['hit@10']['baseline']:.4f}\")\n",
    "print(f\"  Change: {fl_vs_alpha_improvement['hit@10']['pct_change']:+.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPOTHESIS TEST RESULTS (1:10 Sampling):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"H1 (FL improves NeuMF): {'SUPPORTED' if fl_improvement['ndcg@10']['pct_change'] > 0 else 'NOT SUPPORTED'}\")\n",
    "print(f\"H3 (Focusing beyond weighting): {'SUPPORTED' if fl_vs_alpha_improvement['ndcg@10']['pct_change'] > 0 else 'NOT SUPPORTED'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Robustness Study: Multiple Sampling Ratios (H2)\n",
    "\n",
    "Test whether Focal Loss maintains consistent performance across different negative sampling ratios:\n",
    "- **1:4** (low imbalance)\n",
    "- **1:10** (moderate imbalance) - already tested above\n",
    "- **1:50** (high imbalance)\n",
    "\n",
    "**H2 Prediction**: Focal Loss will maintain more consistent performance than BCE as imbalance increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Robustness Study - Multiple Sampling Ratios\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"ROBUSTNESS STUDY: Testing H2 (Multiple Sampling Ratios)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store results for each ratio\n",
    "robustness_results = {\n",
    "    'bce': {},\n",
    "    'focal': {}\n",
    "}\n",
    "\n",
    "# We already have results for 1:10, so test 1:4 and 1:50\n",
    "test_ratios = [4, 50]\n",
    "\n",
    "for ratio in test_ratios:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing with 1:{ratio} sampling\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Train BCE\n",
    "    print(f\"\\n[1:{ratio}] Training NeuMF-BCE...\")\n",
    "    bce_config_ratio = configs[ratio].copy()\n",
    "    bce_config_ratio['loss_type'] = 'BCE'\n",
    "    result_bce_ratio = run_recbole(\n",
    "        model='NeuMF',\n",
    "        dataset=DATASET,\n",
    "        config_dict=bce_config_ratio\n",
    "    )\n",
    "    robustness_results['bce'][ratio] = result_bce_ratio\n",
    "    print(f\"  Test NDCG@10: {result_bce_ratio['test_result']['ndcg@10']:.4f}\")\n",
    "    \n",
    "    # Train Focal Loss\n",
    "    print(f\"\\n[1:{ratio}] Training NeuMF-FocalLoss...\")\n",
    "    result_fl_ratio = train_neumf_focal_loss(\n",
    "        config_dict=configs[ratio],\n",
    "        dataset=DATASET,\n",
    "        gamma=GAMMA,\n",
    "        alpha=ALPHA_FL,\n",
    "        seed=42,\n",
    "        track_dynamics=False\n",
    "    )\n",
    "    robustness_results['focal'][ratio] = result_fl_ratio\n",
    "    print(f\"  Test NDCG@10: {result_fl_ratio['test_result']['ndcg@10']:.4f}\")\n",
    "\n",
    "# Add 1:10 results (already computed)\n",
    "robustness_results['bce'][PRIMARY_RATIO] = result_bce\n",
    "robustness_results['focal'][PRIMARY_RATIO] = result_fl\n",
    "\n",
    "# Create robustness comparison table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROBUSTNESS COMPARISON: BCE vs Focal Loss Across Sampling Ratios\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "robustness_data = []\n",
    "for ratio in SAMPLING_RATIOS:\n",
    "    bce_ndcg = robustness_results['bce'][ratio]['test_result']['ndcg@10']\n",
    "    bce_hr = robustness_results['bce'][ratio]['test_result']['hit@10']\n",
    "    fl_ndcg = robustness_results['focal'][ratio]['test_result']['ndcg@10']\n",
    "    fl_hr = robustness_results['focal'][ratio]['test_result']['hit@10']\n",
    "    \n",
    "    improvement_ndcg = (fl_ndcg - bce_ndcg) / bce_ndcg * 100\n",
    "    improvement_hr = (fl_hr - bce_hr) / bce_hr * 100\n",
    "    \n",
    "    robustness_data.append({\n",
    "        'Sampling Ratio': f'1:{ratio}',\n",
    "        'BCE NDCG@10': f'{bce_ndcg:.4f}',\n",
    "        'FL NDCG@10': f'{fl_ndcg:.4f}',\n",
    "        'NDCG Improve': f'{improvement_ndcg:+.2f}%',\n",
    "        'BCE HR@10': f'{bce_hr:.4f}',\n",
    "        'FL HR@10': f'{fl_hr:.4f}',\n",
    "        'HR Improve': f'{improvement_hr:+.2f}%'\n",
    "    })\n",
    "\n",
    "robustness_df = pd.DataFrame(robustness_data)\n",
    "print(\"\\n\" + robustness_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"H2 TEST: Robustness to Sampling Ratio\")\n",
    "print(\"=\"*70)\n",
    "print(\"Focal Loss should maintain more consistent performance than BCE\")\n",
    "print(\"as class imbalance increases (1:4 -> 1:10 -> 1:50).\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Training Dynamics Analysis (NEW)\n",
    "\n",
    "### Validation of Focal Loss Mechanism\n",
    "\n",
    "We track training dynamics to validate that Focal Loss actually focuses on hard examples:\n",
    "\n",
    "1. **Loss by confidence bin** - Show that well-classified examples (high confidence) contribute less to the total loss\n",
    "2. **Easy vs Hard examples** - Demonstrate that hard examples dominate gradient updates\n",
    "\n",
    "This provides mechanism validation beyond just performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Training Dynamics Analysis\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"MECHANISM VALIDATION: Training Dynamics with Focal Loss\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRetraining with dynamics tracking enabled...\")\n",
    "\n",
    "# Retrain with dynamics tracking\n",
    "result_fl_dynamics = train_neumf_focal_loss(\n",
    "    config_dict=configs[PRIMARY_RATIO],\n",
    "    dataset=DATASET,\n",
    "    gamma=GAMMA,\n",
    "    alpha=ALPHA_FL,\n",
    "    seed=42,\n",
    "    track_dynamics=True\n",
    ")\n",
    "\n",
    "if 'dynamics' in result_fl_dynamics:\n",
    "    dynamics_df = result_fl_dynamics['dynamics']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training Dynamics Summary (First 5 Epochs):\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Show first few epochs\n",
    "    if len(dynamics_df) > 0:\n",
    "        print(dynamics_df.head().to_string(index=False))\n",
    "        \n",
    "        # Analyze loss distribution across confidence bins\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Loss Contribution by Confidence Bin (Final Epoch):\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        final_epoch = dynamics_df.iloc[-1]\n",
    "        print(\"\\nLower confidence = harder examples\")\n",
    "        print(\"Higher confidence = easier examples\")\n",
    "        print(\"\\nFocal Loss should concentrate loss on low-confidence bins.\")\n",
    "        \n",
    "        # Extract loss by bin columns\n",
    "        loss_cols = [col for col in dynamics_df.columns if col.startswith('loss_')]\n",
    "        if loss_cols:\n",
    "            print(\"\\nLoss by confidence bin:\")\n",
    "            for col in sorted(loss_cols):\n",
    "                bin_name = col.replace('loss_', '')\n",
    "                loss_val = final_epoch.get(col, 0)\n",
    "                count_col = f'count_{bin_name}'\n",
    "                count_val = final_epoch.get(count_col, 0)\n",
    "                print(f\"  {bin_name}: loss={loss_val:.4f}, count={int(count_val)}\")\n",
    "else:\n",
    "    print(\"\\nDynamics tracking not available in this training run.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Key Insight: Focal Loss should show higher loss contribution from\")\n",
    "print(\"low-confidence bins (hard examples) compared to high-confidence bins.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Comprehensive summary of all experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Full Comparison and Validation\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT SUMMARY: NCF with Focal Loss on ML-1M\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PRIMARY RESULTS (1:10 Sampling):\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROBUSTNESS RESULTS (Multiple Sampling Ratios):\")\n",
    "print(\"=\"*70)\n",
    "print(robustness_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPOTHESIS TEST RESULTS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nH1: Focal Loss improves NeuMF performance\")\n",
    "print(f\"  Status: {'SUPPORTED' if fl_improvement['ndcg@10']['pct_change'] > 0 else 'NOT SUPPORTED'}\")\n",
    "print(f\"  Evidence: FL shows {fl_improvement['ndcg@10']['pct_change']:+.2f}% improvement in NDCG@10\")\n",
    "\n",
    "print(\"\\nH2: Focal Loss is robust to negative sampling ratio\")\n",
    "print(f\"  Status: Review robustness table above\")\n",
    "print(f\"  Evidence: Compare FL performance consistency across 1:4, 1:10, 1:50\")\n",
    "\n",
    "print(\"\\nH3: Focusing effect is necessary beyond class weighting\")\n",
    "print(f\"  Status: {'SUPPORTED' if fl_vs_alpha_improvement['ndcg@10']['pct_change'] > 0 else 'NOT SUPPORTED'}\")\n",
    "print(f\"  Evidence: FL vs Alpha-BCE shows {fl_vs_alpha_improvement['ndcg@10']['pct_change']:+.2f}% improvement\")\n",
    "print(f\"  This isolates the gamma (focusing) effect from alpha (weighting)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"METHODOLOGY IMPROVEMENTS IMPLEMENTED:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Alpha-Balanced BCE control (isolates focusing effect)\")\n",
    "print(\"2. Multiple sampling ratios (tests robustness)\")\n",
    "print(\"3. 1:10 as primary experiment (more realistic than 1:4)\")\n",
    "print(\"4. Training dynamics tracking (mechanism validation)\")\n",
    "print(\"5. Alpha-sampling interaction analysis (documents effective ratios)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Optional: Hyperparameter Grid Search\n",
    "\n",
    "Uncomment and run for full hyperparameter exploration.\n",
    "\n",
    "### Extended Grid Search\n",
    "\n",
    "This searches over:\n",
    "- **Gamma**: [0.5, 1.0, 2.0, 3.0]\n",
    "- **Alpha**: [0.25, 0.5, 0.75, 0.9]\n",
    "- **Sampling ratios**: [4, 10, 50]\n",
    "\n",
    "With statistical testing (10 seeds per configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Grid Search (Optional - Uncomment to run)\n",
    "# ============================================\n",
    "\"\"\"\n",
    "GAMMA_VALUES = [0.5, 1.0, 2.0, 3.0]\n",
    "ALPHA_VALUES = [0.25, 0.5, 0.75, 0.9]\n",
    "SAMPLING_RATIOS_GRID = [4, 10, 50]\n",
    "SEEDS = list(range(10))  # 10 random seeds for statistical testing\n",
    "\n",
    "grid_search_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER GRID SEARCH\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Configurations to test: {len(GAMMA_VALUES) * len(ALPHA_VALUES) * len(SAMPLING_RATIOS_GRID)}\")\n",
    "print(f\"Seeds per configuration: {len(SEEDS)}\")\n",
    "print(f\"Total training runs: {len(GAMMA_VALUES) * len(ALPHA_VALUES) * len(SAMPLING_RATIOS_GRID) * len(SEEDS)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for ratio in SAMPLING_RATIOS_GRID:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing with 1:{ratio} sampling\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for gamma in GAMMA_VALUES:\n",
    "        for alpha in ALPHA_VALUES:\n",
    "            print(f\"\\n[1:{ratio}] gamma={gamma}, alpha={alpha}\")\n",
    "            \n",
    "            seed_results = []\n",
    "            for seed in SEEDS:\n",
    "                result = train_neumf_focal_loss(\n",
    "                    config_dict=configs[ratio],\n",
    "                    dataset=DATASET,\n",
    "                    gamma=gamma,\n",
    "                    alpha=alpha,\n",
    "                    seed=seed,\n",
    "                    track_dynamics=False\n",
    "                )\n",
    "                seed_results.append(result['test_result'])\n",
    "            \n",
    "            # Aggregate results\n",
    "            avg_ndcg10 = np.mean([r.get('ndcg@10', 0) for r in seed_results])\n",
    "            std_ndcg10 = np.std([r.get('ndcg@10', 0) for r in seed_results])\n",
    "            avg_hr10 = np.mean([r.get('hit@10', 0) for r in seed_results])\n",
    "            std_hr10 = np.std([r.get('hit@10', 0) for r in seed_results])\n",
    "            \n",
    "            grid_search_results.append({\n",
    "                'ratio': ratio,\n",
    "                'gamma': gamma,\n",
    "                'alpha': alpha,\n",
    "                'ndcg@10_mean': avg_ndcg10,\n",
    "                'ndcg@10_std': std_ndcg10,\n",
    "                'hr@10_mean': avg_hr10,\n",
    "                'hr@10_std': std_hr10\n",
    "            })\n",
    "            \n",
    "            print(f\"  NDCG@10: {avg_ndcg10:.4f} +/- {std_ndcg10:.4f}\")\n",
    "            print(f\"  HR@10:   {avg_hr10:.4f} +/- {std_hr10:.4f}\")\n",
    "\n",
    "# Display grid search results\n",
    "grid_df = pd.DataFrame(grid_search_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GRID SEARCH RESULTS:\")\n",
    "print(\"=\"*70)\n",
    "print(grid_df.to_string(index=False))\n",
    "\n",
    "# Find best configuration for each sampling ratio\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST CONFIGURATIONS BY SAMPLING RATIO:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for ratio in SAMPLING_RATIOS_GRID:\n",
    "    ratio_df = grid_df[grid_df['ratio'] == ratio]\n",
    "    best_idx = ratio_df['ndcg@10_mean'].idxmax()\n",
    "    best_config = grid_df.loc[best_idx]\n",
    "    \n",
    "    print(f\"\\n1:{ratio} sampling:\")\n",
    "    print(f\"  Best gamma: {best_config['gamma']}\")\n",
    "    print(f\"  Best alpha: {best_config['alpha']}\")\n",
    "    print(f\"  NDCG@10: {best_config['ndcg@10_mean']:.4f} +/- {best_config['ndcg@10_std']:.4f}\")\n",
    "    print(f\"  HR@10:   {best_config['hr@10_mean']:.4f} +/- {best_config['hr@10_std']:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Grid search code available but commented out.\")\n",
    "print(\"Uncomment the code above to run full hyperparameter search.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
