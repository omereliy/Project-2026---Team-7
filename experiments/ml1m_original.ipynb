{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_OdLDDNiJCp"
   },
   "source": [
    "\n",
    "# NCF with Focal Loss - ML-100K Validation\n",
    "This notebook validates the implementation of Neural Collaborative Filtering (NeuMF) with Focal Loss.\n",
    "\n",
    "**Paper**: \"Addressing Class Imbalance in NCF with Focal Loss\" (AAMAS 2025)\n",
    "\n",
    "**Objective**: Compare NeuMF trained with BCE vs Focal Loss on MovieLens 100K dataset.\n",
    "\n",
    "**Success Criteria**:\n",
    "1. Both models train without errors\n",
    "2. HR@10 > 0.5 (reasonable performance)\n",
    "3. Focal Loss performs >= BCE\n",
    "4. Proper convergence curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT6TAyGciJCq"
   },
   "source": "## Cell 1: Install Dependencies\n\n**Instructions:**\n1. Run the install cell below\n2. **RESTART** the runtime (Runtime -> Restart session)\n3. Run the verification cell\n4. Continue with remaining cells"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "810ac82a"
   },
   "source": "# ============================================\n# Install Dependencies (Part 1)\n# ============================================\n# After running this cell, RESTART the runtime, then run the next cell\n\n%pip install -q ray\n%pip install -q recbole==1.2.0\n%pip install -q kmeans-pytorch\n\n# Force numpy 1.x (required for RecBole compatibility)\n%pip uninstall -y numpy\n%pip install -q \"numpy<2\"\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"RESTART REQUIRED\")\nprint(\"=\"*60)\nprint(\"Go to: Runtime -> Restart session\")\nprint(\"Then run the NEXT cell to verify installation.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# Verify Installation (Part 2) - Run AFTER restart\n# ============================================\nimport numpy as np\nprint(f\"NumPy version: {np.__version__}\")\n\nif np.__version__.startswith(\"2.\"):\n    print(\"\\nERROR: NumPy 2.x still detected!\")\n    print(\"Try: Runtime -> Restart session -> Run this cell again\")\nelse:\n    print(\"SUCCESS: NumPy 1.x installed. Continue to next cell.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zWvVVbMSiJCr"
   },
   "source": "# ============================================\n# CELL 2: Imports & Environment Setup\n# ============================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nimport os\nimport logging\nimport sys\n\n# ============================================\n# Environment Setup (Colab / Local)\n# ============================================\nif 'google.colab' in sys.modules:\n    # Clone repo if not already present\n    if not os.path.exists('/content/Project-2026---Team-7'):\n        !git clone https://github.com/omereliy/Project-2026---Team-7.git /content/Project-2026---Team-7\n    # Add experiments folder to path and change directory\n    sys.path.insert(0, '/content/Project-2026---Team-7/experiments')\n    %cd /content/Project-2026---Team-7/experiments\n    print(\"Running on Google Colab - repo cloned\")\nelse:\n    # Local: assume running from experiments folder\n    sys.path.insert(0, '.')\n    print(\"Running locally\")\n\n# ============================================\n# RecBole Fix: Patch torch.distributed.barrier for single-GPU\n# See: https://github.com/RUCAIBox/RecBole/issues/1989\n# ============================================\nif not hasattr(dist, '_barrier_patched'):\n    _original_barrier = dist.barrier\n    def _patched_barrier(*args, **kwargs):\n        if dist.is_available() and dist.is_initialized():\n            return _original_barrier(*args, **kwargs)\n        # Skip barrier if not in distributed mode\n    dist.barrier = _patched_barrier\n    dist._barrier_patched = True\n    print(\"Applied RecBole distributed fix\")\n\n# PyTorch 2.6+ compatibility patch\nif not hasattr(torch, '_load_patched'):\n    _original_torch_load = torch.load\n    def _patched_torch_load(*args, **kwargs):\n        if 'weights_only' not in kwargs:\n            kwargs['weights_only'] = False\n        return _original_torch_load(*args, **kwargs)\n    torch.load = _patched_torch_load\n    torch._load_patched = True\n\n# RecBole imports\nfrom recbole.quick_start import run_recbole\nfrom recbole.model.general_recommender.neumf import NeuMF\nfrom recbole.config import Config\nfrom recbole.data import create_dataset, data_preparation\nfrom recbole.trainer import Trainer\nfrom recbole.utils import init_seed, init_logger\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.is_available():\n    print(f\"Using: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(f\"Using: CPU\")\n\nprint(\"\\nImports successful!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZbxc_kTiJCs"
   },
   "source": [
    "## Cell 2: Custom Focal Loss Implementation\n",
    "\n",
    "Focal Loss formula: $FL(p_t) = -\\alpha_t (1-p_t)^\\gamma \\log(p_t)$\n",
    "\n",
    "Where:\n",
    "- $p_t$ = model's estimated probability for the ground-truth class\n",
    "- $\\gamma$ = focusing parameter (default: 2.0)\n",
    "- $\\alpha$ = class balancing weight (default: 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QfNr69C0iJCt"
   },
   "source": [
    "# ============================================\n",
    "# CELL 3: Focal Loss Implementation\n",
    "# ============================================\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for addressing class imbalance in recommendation systems.\n",
    "\n",
    "    Reference: Lin et al., \"Focal Loss for Dense Object Detection\", ICCV 2017\n",
    "\n",
    "    Args:\n",
    "        gamma (float): Focusing parameter. Higher values down-weight easy examples more.\n",
    "                      gamma=0 reduces to standard BCE. Default: 2.0\n",
    "        alpha (float): Class balancing weight for positive class. Default: 0.25\n",
    "        reduction (str): 'mean', 'sum', or 'none'. Default: 'mean'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Predicted probabilities (after sigmoid), shape [batch_size]\n",
    "            targets: Ground truth labels (0 or 1), shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "            Focal loss value\n",
    "        \"\"\"\n",
    "        # Clamp for numerical stability\n",
    "        inputs = torch.clamp(inputs, min=1e-7, max=1-1e-7)\n",
    "\n",
    "        # Calculate p_t (probability of true class)\n",
    "        # p_t = p if y=1, else 1-p\n",
    "        p_t = targets * inputs + (1 - targets) * (1 - inputs)\n",
    "\n",
    "        # Calculate alpha_t (class weight)\n",
    "        # alpha_t = alpha if y=1, else 1-alpha\n",
    "        alpha_t = targets * self.alpha + (1 - targets) * (1 - self.alpha)\n",
    "\n",
    "        # Focal loss: -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "        focal_weight = alpha_t * torch.pow(1 - p_t, self.gamma)\n",
    "        focal_loss = -focal_weight * torch.log(p_t)\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "# Verify Focal Loss implementation\n",
    "def test_focal_loss():\n",
    "    \"\"\"Test that Focal Loss with gamma=0 behaves correctly\"\"\"\n",
    "    bce_loss = nn.BCELoss()\n",
    "\n",
    "    # Test inputs\n",
    "    preds = torch.tensor([0.9, 0.1, 0.5, 0.8])\n",
    "    targets = torch.tensor([1.0, 0.0, 1.0, 0.0])\n",
    "\n",
    "    bce = bce_loss(preds, targets)\n",
    "\n",
    "    # With gamma=0 and alpha=0.5, FL = 0.5 * BCE (both classes weighted by 0.5)\n",
    "    focal_loss_gamma0 = FocalLoss(gamma=0.0, alpha=0.5)\n",
    "    fl_alpha05 = focal_loss_gamma0(preds, targets)\n",
    "\n",
    "    print(f\"BCE Loss: {bce.item():.4f}\")\n",
    "    print(f\"Focal Loss (gamma=0, alpha=0.5): {fl_alpha05.item():.4f}\")\n",
    "    print(f\"Expected (0.5 * BCE): {0.5 * bce.item():.4f}\")\n",
    "\n",
    "    # With alpha=0.5, FL should be exactly half of BCE\n",
    "    assert abs(fl_alpha05.item() - 0.5 * bce.item()) < 0.01, \"FL(gamma=0, alpha=0.5) should equal 0.5*BCE\"\n",
    "    print(\"Test 1 PASSED: FL(gamma=0, alpha=0.5) = 0.5 * BCE\")\n",
    "\n",
    "    # Test gamma effect: higher gamma should reduce loss for well-classified examples\n",
    "    focal_loss_gamma2 = FocalLoss(gamma=2.0, alpha=0.5)\n",
    "    fl_gamma2 = focal_loss_gamma2(preds, targets)\n",
    "\n",
    "    print(f\"\\nFocal Loss (gamma=2, alpha=0.5): {fl_gamma2.item():.4f}\")\n",
    "    assert fl_gamma2.item() < fl_alpha05.item(), \"Higher gamma should reduce loss\"\n",
    "    print(\"Test 2 PASSED: FL(gamma=2) < FL(gamma=0)\")\n",
    "\n",
    "    print(\"\\nFocal Loss implementation PASSED!\")\n",
    "\n",
    "test_focal_loss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kYZbTUOmiJCv"
   },
   "source": [
    "# ============================================\n",
    "# CELL 4: Demonstrate Focal Loss Effect\n",
    "# ============================================\n",
    "def demonstrate_focal_loss_effect():\n",
    "    \"\"\"Show how Focal Loss down-weights easy examples\"\"\"\n",
    "    bce_loss = nn.BCELoss(reduction='none')\n",
    "    focal_loss = FocalLoss(gamma=2.0, alpha=0.25, reduction='none')\n",
    "\n",
    "    # Scenarios from the paper's toy example\n",
    "    scenarios = [\n",
    "        (\"Easy negative (model predicts 0.05 for y=0)\", torch.tensor([0.05]), torch.tensor([0.0])),\n",
    "        (\"Hard positive (model predicts 0.3 for y=1)\", torch.tensor([0.30]), torch.tensor([1.0])),\n",
    "        (\"Hard negative (model predicts 0.7 for y=0)\", torch.tensor([0.70]), torch.tensor([0.0])),\n",
    "        (\"Easy positive (model predicts 0.95 for y=1)\", torch.tensor([0.95]), torch.tensor([1.0])),\n",
    "    ]\n",
    "\n",
    "    print(\"Comparing BCE vs Focal Loss (gamma=2, alpha=0.25):\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for desc, pred, target in scenarios:\n",
    "        bce = bce_loss(pred, target).item()\n",
    "        fl = focal_loss(pred, target).item()\n",
    "        ratio = bce / fl if fl > 0 else float('inf')\n",
    "\n",
    "        print(f\"\\n{desc}\")\n",
    "        print(f\"  BCE Loss:   {bce:.4f}\")\n",
    "        print(f\"  Focal Loss: {fl:.4f}\")\n",
    "        print(f\"  BCE/FL ratio: {ratio:.1f}x (Focal Loss reduces by {ratio:.0f}x)\")\n",
    "\n",
    "demonstrate_focal_loss_effect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_AAnBYHiJCw"
   },
   "source": [
    "## Cell 3: Data Configuration (ML-100K)\n",
    "\n",
    "Using RecBole's built-in MovieLens 100K dataset with:\n",
    "- Binarization: ratings >= 4 â†’ positive\n",
    "- Leave-one-out evaluation\n",
    "- 4 negatives per positive (training)\n",
    "- 99 negatives (evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rYzUtqWziJCy"
   },
   "source": [
    "# ============================================\n",
    "# CELL 5: Base Configuration (ML-100K)\n",
    "# ============================================\n",
    "base_config = {\n",
    "    # Dataset\n",
    "    'dataset': 'ml-100k',\n",
    "    'data_path': './dataset/',\n",
    "\n",
    "    # Data preprocessing (from methodology)\n",
    "    'load_col': {'inter': ['user_id', 'item_id', 'rating', 'timestamp']},\n",
    "    'threshold': {'rating': 4},  # Binarize: ratings >= 4 are positive\n",
    "    'val_interval': {'rating': '[4,inf)'},  # Only consider ratings >= 4 as positive\n",
    "\n",
    "    # Evaluation settings (from methodology)\n",
    "    'eval_args': {\n",
    "        'split': {'LS': 'valid_and_test'},  # Leave-one-out\n",
    "        'group_by': 'user',\n",
    "        'order': 'TO',  # Temporal order (most recent for test)\n",
    "        'mode': 'full',  # Full ranking evaluation\n",
    "    },\n",
    "\n",
    "    # Training negative sampling\n",
    "    'train_neg_sample_args': {\n",
    "        'distribution': 'uniform',\n",
    "        'sample_num': 4,  # 4 negatives per positive\n",
    "        'dynamic': False,\n",
    "    },\n",
    "\n",
    "    # Evaluation settings\n",
    "    'metrics': ['Hit', 'NDCG'],\n",
    "    'topk': [5, 10, 20],\n",
    "    'valid_metric': 'NDCG@10',\n",
    "\n",
    "    # Training settings\n",
    "    'epochs': 100,\n",
    "    'stopping_step': 10,  # Early stopping patience\n",
    "    'train_batch_size': 256,\n",
    "    'eval_batch_size': 4096,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    # Reproducibility\n",
    "    'seed': 42,\n",
    "\n",
    "    # Device\n",
    "    'device': device,\n",
    "\n",
    "    # Logging\n",
    "    'show_progress': True,\n",
    "}\n",
    "\n",
    "print(\"Base configuration loaded.\")\n",
    "print(f\"Dataset: {base_config['dataset']}\")\n",
    "print(f\"Binarization threshold: rating >= {base_config['threshold']['rating']}\")\n",
    "print(f\"Training negatives per positive: {base_config['train_neg_sample_args']['sample_num']}\")\n",
    "print(f\"Early stopping patience: {base_config['stopping_step']} epochs\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk64xWJjiJC0"
   },
   "source": [
    "## Cell 4: NeuMF with BCE (Baseline)\n",
    "\n",
    "Standard NeuMF architecture with Binary Cross-Entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1oT3CKfFiJC0"
   },
   "source": [
    "# ============================================\n",
    "# CELL 6: NeuMF-BCE Configuration\n",
    "# ============================================\n",
    "neumf_bce_config = base_config.copy()\n",
    "neumf_bce_config.update({\n",
    "    'model': 'NeuMF',\n",
    "\n",
    "    # NeuMF architecture (from methodology)\n",
    "    'mf_embedding_size': 64,\n",
    "    'mlp_embedding_size': 64,\n",
    "    'mlp_hidden_size': [128, 64, 32],\n",
    "    'dropout_prob': 0.0,\n",
    "\n",
    "    # Use default BCE loss\n",
    "    'loss_type': 'BCE',\n",
    "})\n",
    "\n",
    "print(\"NeuMF-BCE Configuration:\")\n",
    "print(f\"  MF Embedding Size: {neumf_bce_config['mf_embedding_size']}\")\n",
    "print(f\"  MLP Embedding Size: {neumf_bce_config['mlp_embedding_size']}\")\n",
    "print(f\"  MLP Hidden Layers: {neumf_bce_config['mlp_hidden_size']}\")\n",
    "print(f\"  Loss: BCE\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cEj3KIibiJC1"
   },
   "source": [
    "# ============================================\n",
    "# CELL 7: Train NeuMF with BCE\n",
    "# ============================================\n",
    "print(\"=\"*60)\n",
    "print(\"Training NeuMF with BCE Loss\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result_bce = run_recbole(\n",
    "    model='NeuMF',\n",
    "    dataset='ml-100k',\n",
    "    config_dict=neumf_bce_config\n",
    ")\n",
    "\n",
    "# Store results\n",
    "bce_results = {\n",
    "    'model': 'NeuMF-BCE',\n",
    "    'best_valid_score': result_bce['best_valid_score'],\n",
    "    'test_result': result_bce['test_result']\n",
    "}\n",
    "\n",
    "print(\"\\nNeuMF-BCE Results:\")\n",
    "print(f\"  Best Validation NDCG@10: {result_bce['best_valid_score']:.4f}\")\n",
    "print(f\"  Test Results: {result_bce['test_result']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-ikpeDiJC1"
   },
   "source": [
    "## Cell 5: NeuMF with Focal Loss\n",
    "\n",
    "Custom NeuMF with Focal Loss (gamma=2.0, alpha=0.25).\n",
    "\n",
    "We need to create a custom model class that extends RecBole's NeuMF."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Va-O63BsiJC2"
   },
   "source": [
    "# ============================================\n",
    "# CELL 8: NeuMF with Focal Loss Class\n",
    "# ============================================\n",
    "# NeuMF is already imported above as direct import\n",
    "\n",
    "class NeuMF_FocalLoss(NeuMF):\n",
    "    \"\"\"\n",
    "    NeuMF model with Focal Loss instead of BCE.\n",
    "\n",
    "    This extends RecBole's NeuMF and replaces the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, dataset, gamma=2.0, alpha=0.25):\n",
    "        super(NeuMF_FocalLoss, self).__init__(config, dataset)\n",
    "\n",
    "        # Replace BCE loss with Focal Loss\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.focal_loss = FocalLoss(gamma=gamma, alpha=alpha, reduction='mean')\n",
    "\n",
    "        print(f\"Initialized NeuMF with Focal Loss (gamma={gamma}, alpha={alpha})\")\n",
    "\n",
    "    def calculate_loss(self, interaction):\n",
    "        \"\"\"\n",
    "        Calculate Focal Loss for the given interaction.\n",
    "\n",
    "        This overrides the parent class's calculate_loss method.\n",
    "        \"\"\"\n",
    "        user = interaction[self.USER_ID]\n",
    "        item = interaction[self.ITEM_ID]\n",
    "        label = interaction[self.LABEL]\n",
    "\n",
    "        # Forward pass to get predictions\n",
    "        output = self.forward(user, item)\n",
    "\n",
    "        # Apply Focal Loss\n",
    "        loss = self.focal_loss(output, label)\n",
    "\n",
    "        return loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0bo9IoowiJC2"
   },
   "source": [
    "# ============================================\n",
    "# CELL 9: Training Function for Focal Loss\n",
    "# ============================================\n",
    "def train_neumf_focal_loss(config_dict, gamma=2.0, alpha=0.25, seed=42):\n",
    "    \"\"\"\n",
    "    Train NeuMF with Focal Loss using RecBole's infrastructure.\n",
    "\n",
    "    Args:\n",
    "        config_dict: Configuration dictionary\n",
    "        gamma: Focal Loss focusing parameter\n",
    "        alpha: Focal Loss class balancing weight\n",
    "        seed: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with training results\n",
    "    \"\"\"\n",
    "    # Set seed\n",
    "    init_seed(seed, reproducibility=True)\n",
    "\n",
    "    # Create config\n",
    "    config = Config(model='NeuMF', dataset='ml-100k', config_dict=config_dict)\n",
    "\n",
    "    # Initialize logger\n",
    "    init_logger(config)\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    # Create dataset and dataloaders\n",
    "    dataset = create_dataset(config)\n",
    "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "    # Create model with Focal Loss\n",
    "    model = NeuMF_FocalLoss(config, dataset, gamma=gamma, alpha=alpha).to(config['device'])\n",
    "    logger.info(model)\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    # Train\n",
    "    best_valid_score, best_valid_result = trainer.fit(train_data, valid_data)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_result = trainer.evaluate(test_data)\n",
    "\n",
    "    return {\n",
    "        'best_valid_score': best_valid_score,\n",
    "        'best_valid_result': best_valid_result,\n",
    "        'test_result': test_result,\n",
    "        'model': model,\n",
    "        'trainer': trainer\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ych8PoOmiJC3"
   },
   "source": [
    "# ============================================\n",
    "# CELL 10: NeuMF-FocalLoss Configuration\n",
    "# ============================================\n",
    "neumf_fl_config = base_config.copy()\n",
    "neumf_fl_config.update({\n",
    "    'model': 'NeuMF',\n",
    "\n",
    "    # NeuMF architecture (same as BCE for fair comparison)\n",
    "    'mf_embedding_size': 64,\n",
    "    'mlp_embedding_size': 64,\n",
    "    'mlp_hidden_size': [128, 64, 32],\n",
    "    'dropout_prob': 0.0,\n",
    "})\n",
    "\n",
    "# Focal Loss hyperparameters (from methodology)\n",
    "GAMMA = 2.0  # Focusing parameter\n",
    "ALPHA = 0.25  # Class balancing weight\n",
    "\n",
    "print(\"NeuMF-FocalLoss Configuration:\")\n",
    "print(f\"  MF Embedding Size: {neumf_fl_config['mf_embedding_size']}\")\n",
    "print(f\"  MLP Embedding Size: {neumf_fl_config['mlp_embedding_size']}\")\n",
    "print(f\"  MLP Hidden Layers: {neumf_fl_config['mlp_hidden_size']}\")\n",
    "print(f\"  Focal Loss gamma: {GAMMA}\")\n",
    "print(f\"  Focal Loss alpha: {ALPHA}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xDu-Rp77iJC4"
   },
   "source": [
    "# ============================================\n",
    "# CELL 11: Train NeuMF with Focal Loss\n",
    "# ============================================\n",
    "print(\"=\"*60)\n",
    "print(f\"Training NeuMF with Focal Loss (gamma={GAMMA}, alpha={ALPHA})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result_fl = train_neumf_focal_loss(\n",
    "    config_dict=neumf_fl_config,\n",
    "    gamma=GAMMA,\n",
    "    alpha=ALPHA,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Store results\n",
    "fl_results = {\n",
    "    'model': f'NeuMF-FL(g={GAMMA},a={ALPHA})',\n",
    "    'best_valid_score': result_fl['best_valid_score'],\n",
    "    'test_result': result_fl['test_result']\n",
    "}\n",
    "\n",
    "print(f\"\\nNeuMF-FocalLoss Results:\")\n",
    "print(f\"  Best Validation NDCG@10: {result_fl['best_valid_score']:.4f}\")\n",
    "print(f\"  Test Results: {result_fl['test_result']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dSjWTeMiJC4"
   },
   "source": [
    "## Cell 6: Evaluation & Comparison\n",
    "\n",
    "Compare BCE vs Focal Loss results:\n",
    "- HR@5, HR@10, HR@20\n",
    "- NDCG@5, NDCG@10, NDCG@20"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bcNa6bMCiJC4"
   },
   "source": [
    "# ============================================\n",
    "# CELL 12: Comparison Table\n",
    "# ============================================\n",
    "def create_comparison_table(bce_results, fl_results):\n",
    "    \"\"\"Create a side-by-side comparison table of BCE vs Focal Loss results\"\"\"\n",
    "\n",
    "    metrics = ['hit@5', 'hit@10', 'hit@20', 'ndcg@5', 'ndcg@10', 'ndcg@20']\n",
    "\n",
    "    data = []\n",
    "    for metric in metrics:\n",
    "        bce_val = bce_results['test_result'].get(metric, 0)\n",
    "        fl_val = fl_results['test_result'].get(metric, 0)\n",
    "        diff = fl_val - bce_val\n",
    "        pct_change = (diff / bce_val * 100) if bce_val > 0 else 0\n",
    "\n",
    "        data.append({\n",
    "            'Metric': metric.upper(),\n",
    "            'NeuMF-BCE': f'{bce_val:.4f}',\n",
    "            'NeuMF-FL': f'{fl_val:.4f}',\n",
    "            'Difference': f'{diff:+.4f}',\n",
    "            '% Change': f'{pct_change:+.2f}%'\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Display comparison\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON: NeuMF-BCE vs NeuMF-FocalLoss on ML-100K\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_df = create_comparison_table(bce_results, fl_results)\n",
    "print(comparison_df.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZyyPkQY1iJC5"
   },
   "source": [
    "# ============================================\n",
    "# CELL 13: Validation Checks\n",
    "# ============================================\n",
    "def validate_results(bce_results, fl_results):\n",
    "    \"\"\"Check if results meet success criteria\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"VALIDATION CHECKS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Check 1: Both models trained without errors\n",
    "    print(\"\\n[CHECK 1] Both models trained successfully\")\n",
    "    if bce_results['test_result'] and fl_results['test_result']:\n",
    "        print(\"  PASSED: Both models have test results\")\n",
    "    else:\n",
    "        print(\"  FAILED: One or both models failed to produce results\")\n",
    "\n",
    "    # Check 2: HR@10 > 0.5 (reasonable performance)\n",
    "    print(\"\\n[CHECK 2] Reasonable performance (HR@10 > 0.5)\")\n",
    "    bce_hr10 = bce_results['test_result'].get('hit@10', 0)\n",
    "    fl_hr10 = fl_results['test_result'].get('hit@10', 0)\n",
    "    print(f\"  BCE HR@10: {bce_hr10:.4f} {'PASSED' if bce_hr10 > 0.5 else 'BELOW THRESHOLD'}\")\n",
    "    print(f\"  FL HR@10:  {fl_hr10:.4f} {'PASSED' if fl_hr10 > 0.5 else 'BELOW THRESHOLD'}\")\n",
    "\n",
    "    # Check 3: Focal Loss >= BCE\n",
    "    print(\"\\n[CHECK 3] Focal Loss performance >= BCE\")\n",
    "    bce_ndcg10 = bce_results['test_result'].get('ndcg@10', 0)\n",
    "    fl_ndcg10 = fl_results['test_result'].get('ndcg@10', 0)\n",
    "    if fl_ndcg10 >= bce_ndcg10:\n",
    "        print(f\"  PASSED: FL NDCG@10 ({fl_ndcg10:.4f}) >= BCE NDCG@10 ({bce_ndcg10:.4f})\")\n",
    "        improvement = (fl_ndcg10 - bce_ndcg10) / bce_ndcg10 * 100\n",
    "        print(f\"  Improvement: {improvement:+.2f}%\")\n",
    "    else:\n",
    "        print(f\"  NOTE: FL NDCG@10 ({fl_ndcg10:.4f}) < BCE NDCG@10 ({bce_ndcg10:.4f})\")\n",
    "        print(\"  This may indicate need for hyperparameter tuning\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Validation complete. Review results above.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "validate_results(bce_results, fl_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I4Z6bVViJC6"
   },
   "source": [
    "## Cell 7: Additional Validation - Gamma=0 Test\n",
    "\n",
    "Verify that Focal Loss with gamma=0 produces similar results to BCE."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9C5D3ww8iJC6"
   },
   "source": [
    "# ============================================\n",
    "# CELL 14: Gamma=0 Validation Test\n",
    "# ============================================\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION: Focal Loss with gamma=0 should approximate BCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result_fl_gamma0 = train_neumf_focal_loss(\n",
    "    config_dict=neumf_fl_config,\n",
    "    gamma=0.0,  # gamma=0 reduces to weighted BCE\n",
    "    alpha=0.5,  # alpha=0.5 for equal weighting (standard BCE)\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  BCE NDCG@10:           {bce_results['test_result'].get('ndcg@10', 0):.4f}\")\n",
    "print(f\"  FL(gamma=0) NDCG@10:   {result_fl_gamma0['test_result'].get('ndcg@10', 0):.4f}\")\n",
    "print(f\"  FL(gamma=2) NDCG@10:   {fl_results['test_result'].get('ndcg@10', 0):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FngqLmzoiJC7"
   },
   "source": [
    "## Cell 8: Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8luYhTSciJC8"
   },
   "source": [
    "# ============================================\n",
    "# CELL 15: Summary & Next Steps\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT SUMMARY: NCF with Focal Loss on ML-100K\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nDataset: MovieLens 100K\")\n",
    "print(\"Model: NeuMF (GMF + MLP hybrid)\")\n",
    "print(\"\\nResults:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"-\"*70)\n",
    "print(\"1. If validation passed: Proceed to full hyperparameter grid search\")\n",
    "print(\"2. Run same experiment on ML-1M dataset\")\n",
    "print(\"3. Add BPR loss comparison\")\n",
    "print(\"4. Run ablation studies (varying gamma and alpha)\")\n",
    "print(\"5. Add negative sampling ratio experiments (1:4, 1:10, 1:50)\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhOJ07MdiJC9"
   },
   "source": [
    "---\n",
    "\n",
    "## Optional: Grid Search (Run after validation passes)\n",
    "\n",
    "Uncomment and run the cells below for full hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MPuGf62ziJC9"
   },
   "source": [
    "# ============================================\n",
    "# CELL 16: Grid Search (Optional - Uncomment to run)\n",
    "# ============================================\n",
    "GAMMA_VALUES = [0.5, 1.0, 2.0, 3.0]\n",
    "ALPHA_VALUES = [0.25, 0.5, 0.75]\n",
    "SEEDS = list(range(10))  # 10 random seeds for statistical testing\n",
    "\n",
    "grid_search_results = []\n",
    "\n",
    "for gamma in GAMMA_VALUES:\n",
    "    for alpha in ALPHA_VALUES:\n",
    "        print(f\"\\nRunning: gamma={gamma}, alpha={alpha}\")\n",
    "\n",
    "        seed_results = []\n",
    "        for seed in SEEDS:\n",
    "            result = train_neumf_focal_loss(\n",
    "                config_dict=neumf_fl_config,\n",
    "                gamma=gamma,\n",
    "                alpha=alpha,\n",
    "                seed=seed\n",
    "            )\n",
    "            seed_results.append(result['test_result'])\n",
    "\n",
    "        # Aggregate results\n",
    "        avg_ndcg10 = np.mean([r.get('ndcg@10', 0) for r in seed_results])\n",
    "        std_ndcg10 = np.std([r.get('ndcg@10', 0) for r in seed_results])\n",
    "\n",
    "        grid_search_results.append({\n",
    "            'gamma': gamma,\n",
    "            'alpha': alpha,\n",
    "            'ndcg@10_mean': avg_ndcg10,\n",
    "            'ndcg@10_std': std_ndcg10\n",
    "        })\n",
    "\n",
    "        print(f\"  NDCG@10: {avg_ndcg10:.4f} +/- {std_ndcg10:.4f}\")\n",
    "\n",
    "# Display grid search results\n",
    "grid_df = pd.DataFrame(grid_search_results)\n",
    "print(\"\\nGrid Search Results:\")\n",
    "print(grid_df.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}