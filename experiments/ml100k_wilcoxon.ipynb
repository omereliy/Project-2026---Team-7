{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Per-User Wilcoxon Signed-Rank Test: BCE vs Focal Loss\n",
        "\n",
        "This notebook runs a proper Wilcoxon signed-rank test using per-user metrics (n=943 users).\n",
        "\n",
        "**Purpose:** Achieve statistical significance that wasn't possible with only n=3 aggregate observations.\n",
        "\n",
        "**Estimated Runtime:** ~15-30 minutes (model training + evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Install Dependencies\n",
        "\n",
        "Run this cell, then **RESTART** the runtime (Runtime -> Restart session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q recbole==1.2.0\n",
        "%pip install -q kmeans-pytorch\n",
        "%pip uninstall -y numpy\n",
        "%pip install -q \"numpy<2\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESTART REQUIRED\")\n",
        "print(\"=\"*60)\n",
        "print(\"Go to: Runtime -> Restart session\")\n",
        "print(\"Then run Cell 2 to continue.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "\n",
        "# RecBole imports\n",
        "from recbole.quick_start import run_recbole\n",
        "from recbole.model.general_recommender import NeuMF\n",
        "from recbole.config import Config\n",
        "from recbole.data import create_dataset, data_preparation\n",
        "from recbole.trainer import Trainer\n",
        "from recbole.utils import init_seed, init_logger\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Focal Loss Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss for binary classification.\"\"\"\n",
        "    def __init__(self, gamma=2.0, alpha=0.25, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(\n",
        "            inputs, targets, reduction='none'\n",
        "        )\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
        "        focal_loss = alpha_t * ((1 - pt) ** self.gamma) * BCE_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        return focal_loss\n",
        "\n",
        "\n",
        "class NeuMF_FocalLoss(NeuMF):\n",
        "    \"\"\"NeuMF with Focal Loss.\"\"\"\n",
        "    def __init__(self, config, dataset, gamma=2.0, alpha=0.25):\n",
        "        super().__init__(config, dataset)\n",
        "        self.focal_loss = FocalLoss(gamma=gamma, alpha=alpha)\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def calculate_loss(self, interaction):\n",
        "        user = interaction[self.USER_ID]\n",
        "        item = interaction[self.ITEM_ID]\n",
        "        label = interaction[self.LABEL]\n",
        "        output = self.forward(user, item)\n",
        "        return self.focal_loss(output, label)\n",
        "\n",
        "\n",
        "print(\"Focal Loss model defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Per-User Metric Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_per_user_metrics(model, test_data, config, k_list=[5, 10, 20]):\n",
        "    \"\"\"\n",
        "    Extract per-user NDCG and Hit Rate metrics.\n",
        "    \n",
        "    Returns dict: {user_id: {'ndcg@k': value, 'hit@k': value, ...}}\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    user_metrics = {}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batched_data in enumerate(test_data):\n",
        "            interaction, history_index, positive_u, positive_i = batched_data\n",
        "            \n",
        "            # Get model scores for all items\n",
        "            scores = model.full_sort_predict(interaction)\n",
        "            scores = scores.view(-1, test_data.dataset.item_num)\n",
        "            \n",
        "            # Mask out items in history\n",
        "            scores[history_index] = -np.inf\n",
        "            \n",
        "            # Get user IDs\n",
        "            user_ids = interaction[model.USER_ID].cpu().numpy()\n",
        "            \n",
        "            # Convert positive items to sets per user in batch\n",
        "            pos_u_np = positive_u.cpu().numpy()\n",
        "            pos_i_np = positive_i.cpu().numpy()\n",
        "            \n",
        "            for idx, user_id in enumerate(user_ids):\n",
        "                if user_id in user_metrics:\n",
        "                    continue\n",
        "                \n",
        "                # Get this user's scores\n",
        "                user_scores = scores[idx].cpu().numpy()\n",
        "                \n",
        "                # Get positive items for this user (within batch)\n",
        "                pos_mask = pos_u_np == idx\n",
        "                pos_items = pos_i_np[pos_mask]\n",
        "                \n",
        "                if len(pos_items) == 0:\n",
        "                    continue\n",
        "                \n",
        "                # Rank items\n",
        "                ranked_items = np.argsort(-user_scores)\n",
        "                \n",
        "                # Compute metrics for each k\n",
        "                metrics = {}\n",
        "                for k in k_list:\n",
        "                    top_k = ranked_items[:k]\n",
        "                    \n",
        "                    # Hit@k\n",
        "                    hits = np.isin(top_k, pos_items)\n",
        "                    hit_rate = 1.0 if hits.any() else 0.0\n",
        "                    \n",
        "                    # NDCG@k\n",
        "                    dcg = 0.0\n",
        "                    for rank, item in enumerate(top_k):\n",
        "                        if item in pos_items:\n",
        "                            dcg += 1.0 / np.log2(rank + 2)\n",
        "                    \n",
        "                    n_pos = min(len(pos_items), k)\n",
        "                    idcg = sum(1.0 / np.log2(i + 2) for i in range(n_pos))\n",
        "                    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "                    \n",
        "                    metrics[f'hit@{k}'] = hit_rate\n",
        "                    metrics[f'ndcg@{k}'] = ndcg\n",
        "                \n",
        "                user_metrics[user_id] = metrics\n",
        "    \n",
        "    return user_metrics\n",
        "\n",
        "\n",
        "print(\"Per-user metric extraction function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Wilcoxon Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_wilcoxon_test(bce_metrics, fl_metrics, metric='ndcg@10'):\n",
        "    \"\"\"Run Wilcoxon signed-rank test on per-user metrics.\"\"\"\n",
        "    common_users = set(bce_metrics.keys()) & set(fl_metrics.keys())\n",
        "    n_users = len(common_users)\n",
        "    \n",
        "    bce_scores = np.array([bce_metrics[u][metric] for u in sorted(common_users)])\n",
        "    fl_scores = np.array([fl_metrics[u][metric] for u in sorted(common_users)])\n",
        "    differences = fl_scores - bce_scores\n",
        "    \n",
        "    # Statistics\n",
        "    mean_diff = np.mean(differences)\n",
        "    std_diff = np.std(differences)\n",
        "    fl_wins = np.sum(differences > 0)\n",
        "    bce_wins = np.sum(differences < 0)\n",
        "    ties = np.sum(differences == 0)\n",
        "    \n",
        "    # Wilcoxon tests\n",
        "    stat, p_two = stats.wilcoxon(differences, alternative='two-sided')\n",
        "    _, p_greater = stats.wilcoxon(differences, alternative='greater')\n",
        "    \n",
        "    # Effect sizes\n",
        "    n_nonzero = np.sum(differences != 0)\n",
        "    r_effect = 1 - (2 * stat) / (n_nonzero * (n_nonzero + 1) / 2) if n_nonzero > 0 else 0\n",
        "    cohens_d = mean_diff / std_diff if std_diff > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        'n_users': n_users,\n",
        "        'mean_bce': np.mean(bce_scores),\n",
        "        'mean_fl': np.mean(fl_scores),\n",
        "        'mean_diff': mean_diff,\n",
        "        'std_diff': std_diff,\n",
        "        'fl_wins': fl_wins,\n",
        "        'bce_wins': bce_wins,\n",
        "        'ties': ties,\n",
        "        'wilcoxon_stat': stat,\n",
        "        'p_two_sided': p_two,\n",
        "        'p_one_sided': p_greater,\n",
        "        'rank_biserial_r': r_effect,\n",
        "        'cohens_d': cohens_d,\n",
        "    }\n",
        "\n",
        "\n",
        "def print_results(results, metric, alpha=0.05):\n",
        "    \"\"\"Print formatted Wilcoxon test results.\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"WILCOXON SIGNED-RANK TEST: {metric.upper()}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    print(f\"\\nSample Size: n = {results['n_users']} users\")\n",
        "    \n",
        "    print(f\"\\nDescriptive Statistics:\")\n",
        "    print(f\"  BCE mean:  {results['mean_bce']:.4f}\")\n",
        "    print(f\"  FL mean:   {results['mean_fl']:.4f}\")\n",
        "    pct = results['mean_diff'] / results['mean_bce'] * 100 if results['mean_bce'] > 0 else 0\n",
        "    print(f\"  Difference: {results['mean_diff']:+.4f} ({pct:+.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nWin/Loss/Tie:\")\n",
        "    print(f\"  FL wins:  {results['fl_wins']} ({results['fl_wins']/results['n_users']*100:.1f}%)\")\n",
        "    print(f\"  BCE wins: {results['bce_wins']} ({results['bce_wins']/results['n_users']*100:.1f}%)\")\n",
        "    print(f\"  Ties:     {results['ties']}\")\n",
        "    \n",
        "    print(f\"\\nStatistical Test:\")\n",
        "    print(f\"  Wilcoxon W: {results['wilcoxon_stat']:.2f}\")\n",
        "    print(f\"  p-value (two-sided): {results['p_two_sided']:.2e}\")\n",
        "    print(f\"  p-value (one-sided): {results['p_one_sided']:.2e}\")\n",
        "    \n",
        "    sig = \"YES ✓\" if results['p_one_sided'] < alpha else \"NO\"\n",
        "    print(f\"  Significant at α={alpha}: {sig}\")\n",
        "    \n",
        "    print(f\"\\nEffect Sizes:\")\n",
        "    d = abs(results['cohens_d'])\n",
        "    d_interp = \"negligible\" if d < 0.2 else \"small\" if d < 0.5 else \"medium\" if d < 0.8 else \"large\"\n",
        "    print(f\"  Cohen's d: {results['cohens_d']:.3f} ({d_interp})\")\n",
        "    \n",
        "    r = abs(results['rank_biserial_r'])\n",
        "    r_interp = \"negligible\" if r < 0.1 else \"small\" if r < 0.3 else \"medium\" if r < 0.5 else \"large\"\n",
        "    print(f\"  Rank-biserial r: {results['rank_biserial_r']:.3f} ({r_interp})\")\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "print(\"Wilcoxon test functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment configuration\n",
        "SEED = 42\n",
        "DATASET = 'ml-100k'\n",
        "SAMPLING_RATIO = 10  # Change to 4 or 50 for other ratios\n",
        "\n",
        "# Focal Loss parameters\n",
        "FL_GAMMA = 2.0\n",
        "FL_ALPHA = 0.25\n",
        "\n",
        "# Base configuration\n",
        "config_dict = {\n",
        "    'seed': SEED,\n",
        "    'reproducibility': True,\n",
        "    'data_path': 'dataset/',\n",
        "    'USER_ID_FIELD': 'user_id',\n",
        "    'ITEM_ID_FIELD': 'item_id',\n",
        "    'RATING_FIELD': 'rating',\n",
        "    'TIME_FIELD': 'timestamp',\n",
        "    'load_col': {'inter': ['user_id', 'item_id', 'rating', 'timestamp']},\n",
        "    'val_interval': {'rating': '[3,inf)'},\n",
        "    'threshold': {'rating': 3},\n",
        "    \n",
        "    # Training\n",
        "    'epochs': 100,\n",
        "    'train_batch_size': 256,\n",
        "    'eval_batch_size': 4096,\n",
        "    'learning_rate': 0.001,\n",
        "    'train_neg_sample_args': {\n",
        "        'distribution': 'uniform',\n",
        "        'sample_num': SAMPLING_RATIO,\n",
        "        'dynamic': False,\n",
        "    },\n",
        "    \n",
        "    # Model\n",
        "    'embedding_size': 64,\n",
        "    'mlp_hidden_size': [128, 64, 32],\n",
        "    'dropout_prob': 0.1,\n",
        "    \n",
        "    # Evaluation\n",
        "    'eval_args': {\n",
        "        'split': {'LS': 'valid_and_test'},\n",
        "        'group_by': 'user',\n",
        "        'order': 'TO',\n",
        "        'mode': 'full',\n",
        "    },\n",
        "    'metrics': ['Hit', 'NDCG'],\n",
        "    'topk': [5, 10, 20],\n",
        "    'valid_metric': 'NDCG@10',\n",
        "    \n",
        "    # Misc\n",
        "    'stopping_step': 10,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'show_progress': True,\n",
        "}\n",
        "\n",
        "print(f\"Configuration set:\")\n",
        "print(f\"  Dataset: {DATASET}\")\n",
        "print(f\"  Sampling ratio: 1:{SAMPLING_RATIO}\")\n",
        "print(f\"  Focal Loss: gamma={FL_GAMMA}, alpha={FL_ALPHA}\")\n",
        "print(f\"  Device: {config_dict['device']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize\n",
        "init_seed(SEED, reproducibility=True)\n",
        "config = Config(model='NeuMF', dataset=DATASET, config_dict=config_dict)\n",
        "init_logger(config)\n",
        "\n",
        "# Create dataset\n",
        "dataset = create_dataset(config)\n",
        "train_data, valid_data, test_data = data_preparation(config, dataset)\n",
        "\n",
        "print(f\"\\nDataset statistics:\")\n",
        "print(f\"  Users: {dataset.user_num}\")\n",
        "print(f\"  Items: {dataset.item_num}\")\n",
        "print(f\"  Interactions: {dataset.inter_num}\")\n",
        "print(f\"  Sparsity: {1 - dataset.inter_num / (dataset.user_num * dataset.item_num):.4%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Train BCE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"Training BCE Baseline\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train BCE model\n",
        "model_bce = NeuMF(config, dataset).to(config['device'])\n",
        "trainer_bce = Trainer(config, model_bce)\n",
        "best_valid_score_bce, best_valid_result_bce = trainer_bce.fit(train_data, valid_data)\n",
        "\n",
        "print(f\"\\nBCE Best validation NDCG@10: {best_valid_score_bce:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Train Focal Loss Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(f\"Training Focal Loss (gamma={FL_GAMMA}, alpha={FL_ALPHA})\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Reset seed for fair comparison\n",
        "init_seed(SEED, reproducibility=True)\n",
        "\n",
        "# Train FL model\n",
        "model_fl = NeuMF_FocalLoss(config, dataset, gamma=FL_GAMMA, alpha=FL_ALPHA).to(config['device'])\n",
        "trainer_fl = Trainer(config, model_fl)\n",
        "best_valid_score_fl, best_valid_result_fl = trainer_fl.fit(train_data, valid_data)\n",
        "\n",
        "print(f\"\\nFL Best validation NDCG@10: {best_valid_score_fl:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Extract Per-User Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Extracting per-user metrics...\")\n",
        "print(\"\\nBCE model:\")\n",
        "bce_user_metrics = get_per_user_metrics(model_bce, test_data, config)\n",
        "print(f\"  Extracted metrics for {len(bce_user_metrics)} users\")\n",
        "\n",
        "print(\"\\nFocal Loss model:\")\n",
        "fl_user_metrics = get_per_user_metrics(model_fl, test_data, config)\n",
        "print(f\"  Extracted metrics for {len(fl_user_metrics)} users\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11: Run Wilcoxon Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test NDCG@10\n",
        "results_ndcg = run_wilcoxon_test(bce_user_metrics, fl_user_metrics, 'ndcg@10')\n",
        "print_results(results_ndcg, 'ndcg@10')\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Test Hit@10\n",
        "results_hit = run_wilcoxon_test(bce_user_metrics, fl_user_metrics, 'hit@10')\n",
        "print_results(results_hit, 'hit@10')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 12: Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary dataframe\n",
        "summary_data = []\n",
        "for metric, results in [('NDCG@10', results_ndcg), ('Hit@10', results_hit)]:\n",
        "    pct = results['mean_diff'] / results['mean_bce'] * 100 if results['mean_bce'] > 0 else 0\n",
        "    summary_data.append({\n",
        "        'Metric': metric,\n",
        "        'BCE': f\"{results['mean_bce']:.4f}\",\n",
        "        'FL': f\"{results['mean_fl']:.4f}\",\n",
        "        'Δ': f\"{results['mean_diff']:+.4f}\",\n",
        "        '% Δ': f\"{pct:+.1f}%\",\n",
        "        'FL Wins': f\"{results['fl_wins']}/{results['n_users']} ({results['fl_wins']/results['n_users']*100:.1f}%)\",\n",
        "        'p-value': f\"{results['p_one_sided']:.2e}\",\n",
        "        \"Cohen's d\": f\"{results['cohens_d']:.3f}\",\n",
        "        'Significant': '✓' if results['p_one_sided'] < 0.05 else '✗',\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"SUMMARY: Per-User Wilcoxon Test (1:{SAMPLING_RATIO} sampling, n={results_ndcg['n_users']} users)\")\n",
        "print(\"=\"*70)\n",
        "print(df_summary.to_string(index=False))\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 13: Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to file\n",
        "results_dict = {\n",
        "    'sampling_ratio': SAMPLING_RATIO,\n",
        "    'n_users': results_ndcg['n_users'],\n",
        "    'fl_gamma': FL_GAMMA,\n",
        "    'fl_alpha': FL_ALPHA,\n",
        "    'ndcg@10': results_ndcg,\n",
        "    'hit@10': results_hit,\n",
        "}\n",
        "\n",
        "# Print for copy-paste to saved_results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS FOR PAPER / saved_results_ml100k.py\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\"\"\n",
        "WILCOXON_RESULTS_{SAMPLING_RATIO} = {{\n",
        "    'sampling_ratio': {SAMPLING_RATIO},\n",
        "    'n_users': {results_ndcg['n_users']},\n",
        "    'fl_params': {{'gamma': {FL_GAMMA}, 'alpha': {FL_ALPHA}}},\n",
        "    'ndcg@10': {{\n",
        "        'bce_mean': {results_ndcg['mean_bce']:.4f},\n",
        "        'fl_mean': {results_ndcg['mean_fl']:.4f},\n",
        "        'mean_diff': {results_ndcg['mean_diff']:.4f},\n",
        "        'fl_wins': {results_ndcg['fl_wins']},\n",
        "        'p_value': {results_ndcg['p_one_sided']:.6f},\n",
        "        'cohens_d': {results_ndcg['cohens_d']:.4f},\n",
        "        'significant': {results_ndcg['p_one_sided'] < 0.05},\n",
        "    }},\n",
        "    'hit@10': {{\n",
        "        'bce_mean': {results_hit['mean_bce']:.4f},\n",
        "        'fl_mean': {results_hit['mean_fl']:.4f},\n",
        "        'mean_diff': {results_hit['mean_diff']:.4f},\n",
        "        'fl_wins': {results_hit['fl_wins']},\n",
        "        'p_value': {results_hit['p_one_sided']:.6f},\n",
        "        'cohens_d': {results_hit['cohens_d']:.4f},\n",
        "        'significant': {results_hit['p_one_sided'] < 0.05},\n",
        "    }},\n",
        "}}\n",
        "\"\"\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 14 (Optional): Run for Multiple Sampling Ratios\n",
        "\n",
        "Uncomment and run to test all three sampling ratios (takes longer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Uncomment to run for all sampling ratios\n",
        "# all_results = {}\n",
        "# \n",
        "# for ratio in [4, 10, 50]:\n",
        "#     print(f\"\\n{'#'*70}\")\n",
        "#     print(f\"# SAMPLING RATIO 1:{ratio}\")\n",
        "#     print(f\"{'#'*70}\\n\")\n",
        "#     \n",
        "#     # Update config\n",
        "#     config_dict['train_neg_sample_args']['sample_num'] = ratio\n",
        "#     config = Config(model='NeuMF', dataset=DATASET, config_dict=config_dict)\n",
        "#     dataset = create_dataset(config)\n",
        "#     train_data, valid_data, test_data = data_preparation(config, dataset)\n",
        "#     \n",
        "#     # Train BCE\n",
        "#     init_seed(SEED, reproducibility=True)\n",
        "#     model_bce = NeuMF(config, dataset).to(config['device'])\n",
        "#     trainer_bce = Trainer(config, model_bce)\n",
        "#     trainer_bce.fit(train_data, valid_data)\n",
        "#     \n",
        "#     # Train FL\n",
        "#     init_seed(SEED, reproducibility=True)\n",
        "#     model_fl = NeuMF_FocalLoss(config, dataset, gamma=FL_GAMMA, alpha=FL_ALPHA).to(config['device'])\n",
        "#     trainer_fl = Trainer(config, model_fl)\n",
        "#     trainer_fl.fit(train_data, valid_data)\n",
        "#     \n",
        "#     # Extract metrics\n",
        "#     bce_metrics = get_per_user_metrics(model_bce, test_data, config)\n",
        "#     fl_metrics = get_per_user_metrics(model_fl, test_data, config)\n",
        "#     \n",
        "#     # Run test\n",
        "#     results = run_wilcoxon_test(bce_metrics, fl_metrics, 'ndcg@10')\n",
        "#     print_results(results, f'ndcg@10 (1:{ratio})')\n",
        "#     all_results[ratio] = results\n",
        "# \n",
        "# # Summary across ratios\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"SUMMARY ACROSS ALL SAMPLING RATIOS\")\n",
        "# print(\"=\"*70)\n",
        "# for ratio, res in all_results.items():\n",
        "#     sig = \"✓\" if res['p_one_sided'] < 0.05 else \"✗\"\n",
        "#     print(f\"1:{ratio}: FL {'+' if res['mean_diff'] > 0 else ''}{res['mean_diff']/res['mean_bce']*100:.1f}%, p={res['p_one_sided']:.2e}, d={res['cohens_d']:.3f} {sig}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
